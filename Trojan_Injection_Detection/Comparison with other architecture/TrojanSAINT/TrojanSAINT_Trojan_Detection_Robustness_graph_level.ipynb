{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7dc29d7-5b96-44ea-a9a7-260997a08eff",
   "metadata": {},
   "source": [
    "#### Training Evaluation using TrojanSAINT taken from https://github.com/DfX-NYUAD/TrojanSAINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ae98cf3-e6db-4a7e-880c-9166ab2b75a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch_cluster/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "/home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch_spline_conv/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(\n",
      "/home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circuits split -> train:64 val:14 test:14\n",
      "Pretraining will use nodes from 78 circuits (train+val) and exclude 14 test circuits.\n",
      "Node pretraining sizes (labeled): train=130706 val=28008\n",
      "Pretrain Epoch 001 | Loss 0.9511 | Val 0.2940\n",
      "Pretrain Epoch 005 | Loss 0.7267 | Val 0.6488\n",
      "Pretrain Epoch 010 | Loss 0.5281 | Val 0.9220\n",
      "Pretrain Epoch 015 | Loss 0.3861 | Val 0.9845\n",
      "Pretrain Epoch 020 | Loss 0.2874 | Val 0.9946\n",
      "Pretrain Epoch 025 | Loss 0.2167 | Val 0.9971\n",
      "Pretrain Epoch 030 | Loss 0.1668 | Val 0.9984\n",
      "Pretrain Epoch 035 | Loss 0.1302 | Val 0.9992\n",
      "Pretrain Epoch 040 | Loss 0.1067 | Val 0.9993\n",
      "Pretrain Epoch 045 | Loss 0.0867 | Val 0.9993\n",
      "Pretrain Epoch 050 | Loss 0.0707 | Val 0.9986\n",
      "Pretrain Epoch 055 | Loss 0.0628 | Val 0.9969\n",
      "Pretrain Epoch 060 | Loss 0.0535 | Val 0.9969\n",
      "Pretrain Epoch 065 | Loss 0.0459 | Val 0.9970\n",
      "Pretrain Epoch 070 | Loss 0.0410 | Val 0.9998\n",
      "Pretrain Epoch 075 | Loss 0.0364 | Val 0.9998\n",
      "Pretrain Epoch 080 | Loss 0.0320 | Val 0.9999\n",
      "Pretrain Epoch 085 | Loss 0.0288 | Val 0.9999\n",
      "Pretrain Epoch 090 | Loss 0.0258 | Val 0.9999\n",
      "Pretrain Epoch 095 | Loss 0.0230 | Val 0.9999\n",
      "Pretrain Epoch 100 | Loss 0.0211 | Val 0.9999\n",
      "Saved node-level model checkpoint: node_gcn_pretrained.pth\n",
      "\n",
      "Building graph-level Data objects for fine-tuning (train/val/test circuits) ...\n",
      "Built 92 graphs.\n",
      "Graph counts -> train: 64, val: 14, test: 14\n",
      "Transferred 14 parameter tensors from node encoder to graph encoder.\n",
      "\n",
      "Stage 1: freeze encoder and train classifier head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76081/4107286665.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  node_state = torch.load(PRETRAIN_CHECKPOINT, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune (frozen) Epoch 001 | Val Acc 0.5000 | AvgLoss 0.9971\n",
      "Fine-tune (frozen) Epoch 005 | Val Acc 0.6429 | AvgLoss 0.9000\n",
      "Fine-tune (frozen) Epoch 010 | Val Acc 0.8571 | AvgLoss 0.5870\n",
      "Fine-tune (frozen) Epoch 015 | Val Acc 0.9286 | AvgLoss 0.7016\n",
      "Fine-tune (frozen) Epoch 020 | Val Acc 1.0000 | AvgLoss 0.5106\n",
      "Fine-tune (frozen) Epoch 025 | Val Acc 1.0000 | AvgLoss 0.2519\n",
      "Fine-tune (frozen) Epoch 030 | Val Acc 1.0000 | AvgLoss 0.1758\n",
      "Fine-tune (frozen) Epoch 035 | Val Acc 1.0000 | AvgLoss 0.1928\n",
      "Fine-tune (frozen) Epoch 040 | Val Acc 1.0000 | AvgLoss 0.1375\n",
      "Fine-tune (frozen) Epoch 045 | Val Acc 1.0000 | AvgLoss 0.1183\n",
      "Early stopping fine-tune stage.\n",
      "After head training -> Test Acc: 1.0000\n",
      "\n",
      "Stage 2: unfreeze encoder and fine-tune entire model\n",
      "Fine-tune (all) Epoch 001 | Val Acc 0.9286 | AvgLoss 0.4813\n",
      "Fine-tune (all) Epoch 005 | Val Acc 1.0000 | AvgLoss 0.1793\n",
      "Fine-tune (all) Epoch 010 | Val Acc 1.0000 | AvgLoss 0.2301\n",
      "Fine-tune (all) Epoch 015 | Val Acc 1.0000 | AvgLoss 0.1879\n",
      "Fine-tune (all) Epoch 020 | Val Acc 1.0000 | AvgLoss 0.0555\n",
      "Fine-tune (all) Epoch 025 | Val Acc 1.0000 | AvgLoss 0.0524\n",
      "Fine-tune (all) Epoch 030 | Val Acc 1.0000 | AvgLoss 0.0223\n",
      "Early stopping fine-tune stage.\n",
      "After full fine-tune -> Test Acc: 1.0000\n",
      "\n",
      "Final Evaluation (Graph-level after transfer)\n",
      "=============================================\n",
      "Test Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000         4\n",
      "           1     1.0000    1.0000    1.0000        10\n",
      "\n",
      "    accuracy                         1.0000        14\n",
      "   macro avg     1.0000    1.0000    1.0000        14\n",
      "weighted avg     1.0000    1.0000    1.0000        14\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 4  0]\n",
      " [ 0 10]]\n"
     ]
    }
   ],
   "source": [
    "# train_graphlevel_trojansaint.py\n",
    "import os, random, math\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool  # TrojanSAINT-style GCN backbone\n",
    "\n",
    "# ----------------- Config -----------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NODE_CSV = \"GNNDatasets/node.csv\"\n",
    "NODE_EDGE_CSV = \"GNNDatasets/node_edges.csv\"\n",
    "GRAPH_CSV = \"GNNDatasets/graph.csv\"\n",
    "GRAPH_EDGE_CSV = \"GNNDatasets/graph_edges.csv\"\n",
    "\n",
    "PRETRAIN_CHECKPOINT = \"node_gcn_pretrained.pth\"  # keep name unchanged\n",
    "\n",
    "# Hyperparams (tweakable)\n",
    "HID_DIM = 64\n",
    "PRETRAIN_LR = 1e-3\n",
    "PRETRAIN_EPOCHS = 100\n",
    "PRETRAIN_BATCH = None   # full-graph pretraining uses adjacency; we do whole-graph training (no batch)\n",
    "\n",
    "FT_HEAD_LR = 1e-3\n",
    "FT_FINETUNE_LR = 5e-4\n",
    "FT_EPOCHS_HEAD = 60\n",
    "FT_EPOCHS_FINETUNE = 120\n",
    "BATCH_SIZE = 16\n",
    "EARLY_STOPPING = 30\n",
    "DROPOUT = 0.35\n",
    "\n",
    "# ----------------- Load graph-level labels and split circuits -----------------\n",
    "graph_df = pd.read_csv(GRAPH_CSV)\n",
    "# find graph label column\n",
    "graph_label_col = None\n",
    "for cand in [\"label_graph\", \"label\", \"is_trojan\", \"trojan\"]:\n",
    "    if cand in graph_df.columns:\n",
    "        graph_label_col = cand; break\n",
    "if graph_label_col is None:\n",
    "    graph_df[\"label_graph\"] = graph_df[\"circuit_name\"].astype(str).str.contains(\"__trojan_\").astype(int)\n",
    "    graph_label_col = \"label_graph\"\n",
    "\n",
    "circuits = graph_df[\"circuit_name\"].tolist()\n",
    "graph_labels = [int(x) for x in graph_df[graph_label_col].tolist()]\n",
    "\n",
    "# stratified split of circuits for final graph-level evaluation\n",
    "train_circuits, temp_circuits, y_train_c, y_temp_c = train_test_split(\n",
    "    circuits, graph_labels, test_size=0.30, random_state=SEED, stratify=graph_labels\n",
    ")\n",
    "val_circuits, test_circuits, y_val_c, y_test_c = train_test_split(\n",
    "    temp_circuits, y_temp_c, test_size=0.50, random_state=SEED, stratify=y_temp_c\n",
    ")\n",
    "\n",
    "print(f\"Circuits split -> train:{len(train_circuits)} val:{len(val_circuits)} test:{len(test_circuits)}\")\n",
    "\n",
    "# ----------------- Prepare node data for pretraining (exclude test circuits!) -----------------\n",
    "nodes_df = pd.read_csv(NODE_CSV)\n",
    "edges_df = pd.read_csv(NODE_EDGE_CSV)\n",
    "\n",
    "# identify node label column\n",
    "node_label_col = None\n",
    "for cand in [\"label\", \"is_trojan\", \"trojan\", \"target\"]:\n",
    "    if cand in nodes_df.columns:\n",
    "        node_label_col = cand; break\n",
    "if node_label_col is None:\n",
    "    nodes_df[\"label\"] = nodes_df[\"circuit_name\"].astype(str).str.contains(\"__trojan_\").astype(int)\n",
    "    node_label_col = \"label\"\n",
    "\n",
    "nodes_df[\"uid\"] = nodes_df[\"circuit_name\"].astype(str) + \"::\" + nodes_df[\"node\"].astype(str)\n",
    "\n",
    "# Pretrain set circuits: combine train + val circuits (we exclude graph test circuits)\n",
    "pretrain_circuits = set(train_circuits + val_circuits)\n",
    "print(f\"Pretraining will use nodes from {len(pretrain_circuits)} circuits (train+val) and exclude {len(test_circuits)} test circuits.\")\n",
    "\n",
    "# Filter nodes/edges for pretraining graph\n",
    "nodes_pretrain_df = nodes_df[nodes_df[\"circuit_name\"].isin(pretrain_circuits)].reset_index(drop=True)\n",
    "edges_pretrain_df = edges_df[edges_df[\"circuit_name\"].isin(pretrain_circuits)].reset_index(drop=True)\n",
    "\n",
    "# feature columns (numeric)  exclude metadata\n",
    "feat_df = nodes_pretrain_df.copy()\n",
    "if \"gate_type\" in feat_df.columns:\n",
    "    # one-hot encode gate_type\n",
    "    gate_oh = pd.get_dummies(feat_df[\"gate_type\"], prefix=\"gt\")\n",
    "    feat_df = pd.concat([feat_df.drop(columns=[\"gate_type\"]), gate_oh], axis=1)\n",
    "\n",
    "exclude = {\"uid\",\"node\",\"circuit_name\", node_label_col}\n",
    "feature_cols = [c for c in feat_df.columns if c not in exclude and pd.api.types.is_numeric_dtype(feat_df[c])]\n",
    "if len(feature_cols) == 0:\n",
    "    raise SystemExit(\"No numeric feature columns found in node CSV. Please include features.\")\n",
    "\n",
    "# Build X_all for pretraining nodes and mapping\n",
    "X_pre = feat_df[feature_cols].fillna(0.0).to_numpy(dtype=np.float32)\n",
    "y_pre = nodes_pretrain_df[node_label_col].to_numpy(dtype=np.int64)\n",
    "uids_pre = nodes_pretrain_df[\"uid\"].tolist()\n",
    "uid_to_idx_pre = {u: i for i,u in enumerate(uids_pre)}\n",
    "\n",
    "# Some edges may include nodes not present in nodes_pretrain_df (rare)  filter edges\n",
    "def map_uid(signal, circuit):\n",
    "    return f\"{circuit}::{signal}\"\n",
    "\n",
    "edge_src_uids = edges_pretrain_df.apply(lambda r: map_uid(r[\"src\"], r[\"circuit_name\"]), axis=1)\n",
    "edge_dst_uids = edges_pretrain_df.apply(lambda r: map_uid(r[\"dst\"], r[\"circuit_name\"]), axis=1)\n",
    "\n",
    "edge_src_idx = edge_src_uids.map(uid_to_idx_pre).dropna().astype(int).values\n",
    "edge_dst_idx = edge_dst_uids.map(uid_to_idx_pre).dropna().astype(int).values\n",
    "\n",
    "if len(edge_src_idx) == 0:\n",
    "    raise SystemExit(\"No edges left after filtering to pretrain circuits; check your GNNDatasets files.\")\n",
    "\n",
    "edge_index_pre = np.stack([np.concatenate([edge_src_idx, edge_dst_idx]),\n",
    "                           np.concatenate([edge_dst_idx, edge_src_idx])], axis=0)  # undirected\n",
    "\n",
    "# Scale features using labeled nodes only (within pretrain set)\n",
    "# ensure labels are available\n",
    "labeled_mask_pre = (y_pre >= 0)\n",
    "scaler = StandardScaler()\n",
    "X_pre_scaled = X_pre.copy()\n",
    "if labeled_mask_pre.sum() == 0:\n",
    "    raise SystemExit(\"No labeled nodes in pretraining set.\")\n",
    "X_pre_scaled[labeled_mask_pre] = scaler.fit_transform(X_pre_scaled[labeled_mask_pre])\n",
    "X_pre_scaled[~labeled_mask_pre] = (X_pre_scaled[~labeled_mask_pre] - scaler.mean_) / np.sqrt(scaler.var_ + 1e-8)\n",
    "\n",
    "# Convert to torch\n",
    "X_pre_t = torch.from_numpy(X_pre_scaled).to(DEVICE)\n",
    "y_pre_t = torch.from_numpy(y_pre).to(DEVICE)\n",
    "edge_index_pre_t = torch.from_numpy(edge_index_pre).long().to(DEVICE)\n",
    "\n",
    "# Create train/val split (node-level) for early stopping on pretraining\n",
    "idx_nodes = np.where(labeled_mask_pre)[0]\n",
    "y_nodes = y_pre[labeled_mask_pre]\n",
    "n_train_nodes, n_tmp_nodes = train_test_split(idx_nodes, test_size=0.30, random_state=SEED, stratify=y_nodes)\n",
    "n_val_nodes, n_test_nodes = train_test_split(n_tmp_nodes, test_size=0.50, random_state=SEED,\n",
    "                                             stratify=y_pre[n_tmp_nodes])\n",
    "\n",
    "train_mask_nodes = torch.zeros(len(y_pre), dtype=torch.bool, device=DEVICE); train_mask_nodes[n_train_nodes] = True\n",
    "val_mask_nodes   = torch.zeros(len(y_pre), dtype=torch.bool, device=DEVICE);   val_mask_nodes[n_val_nodes] = True\n",
    "# note: we won't use node_test_nodes later\n",
    "\n",
    "# ----------------- Node TrojanSAINT-style pretraining (GCN + BN + Dropout) -----------------\n",
    "class NodeTrojanSAINT(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=HID_DIM, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hid_dim, cached=False, add_self_loops=True, normalize=True)\n",
    "        self.bn1 = nn.BatchNorm1d(hid_dim)\n",
    "        self.conv2 = GCNConv(hid_dim, hid_dim, cached=False, add_self_loops=True, normalize=True)\n",
    "        self.bn2 = nn.BatchNorm1d(hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.head = nn.Linear(hid_dim, 2)\n",
    "        nn.init.xavier_uniform_(self.head.weight)\n",
    "        if self.head.bias is not None:\n",
    "            nn.init.zeros_(self.head.bias)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = self.bn1(h)\n",
    "        h = F.relu(h, inplace=True)\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = self.bn2(h)\n",
    "        h = F.relu(h, inplace=True)\n",
    "        h = self.dropout(h)\n",
    "        return self.head(h)  # per-node logits\n",
    "\n",
    "node_model = NodeTrojanSAINT(in_dim=X_pre_t.shape[1], hid_dim=HID_DIM, dropout=DROPOUT).to(DEVICE)\n",
    "\n",
    "# class weights on node-level training labels\n",
    "train_labels_nodes = y_pre_t[train_mask_nodes]\n",
    "num_pos = int((train_labels_nodes==1).sum().item()) if (train_labels_nodes==1).any() else 1\n",
    "num_neg = int((train_labels_nodes==0).sum().item()) if (train_labels_nodes==0).any() else 1\n",
    "w_pos = (num_neg + num_pos) / (2.0 * num_pos)\n",
    "w_neg = (num_neg + num_pos) / (2.0 * num_neg)\n",
    "class_weights_nodes = torch.tensor([w_neg, w_pos], dtype=torch.float32, device=DEVICE)\n",
    "crit_node = nn.CrossEntropyLoss(weight=class_weights_nodes)\n",
    "opt_node = torch.optim.Adam(node_model.parameters(), lr=PRETRAIN_LR, weight_decay=5e-4)\n",
    "\n",
    "# Pretraining loop\n",
    "best_val = -1.0; best_state = None; patience_cnt = 0\n",
    "print(\"Node pretraining sizes (labeled): train=%d val=%d\" % (train_mask_nodes.sum().item(), val_mask_nodes.sum().item()))\n",
    "for epoch in range(1, PRETRAIN_EPOCHS+1):\n",
    "    node_model.train()\n",
    "    opt_node.zero_grad()\n",
    "    logits_nodes = node_model(X_pre_t, edge_index_pre_t)\n",
    "    loss = crit_node(logits_nodes[train_mask_nodes], y_pre_t[train_mask_nodes])\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(node_model.parameters(), 2.0)  # stable like TrojanSAINT configs\n",
    "    opt_node.step()\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        node_model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits_val = node_model(X_pre_t, edge_index_pre_t)\n",
    "            preds_val = logits_val.argmax(dim=1)\n",
    "            if val_mask_nodes.sum() > 0:\n",
    "                val_acc = (preds_val[val_mask_nodes] == y_pre_t[val_mask_nodes]).float().mean().item()\n",
    "            else:\n",
    "                val_acc = 0.0\n",
    "        print(f\"Pretrain Epoch {epoch:03d} | Loss {loss.item():.4f} | Val {val_acc:.4f}\")\n",
    "        if val_acc > best_val + 1e-5:\n",
    "            best_val = val_acc\n",
    "            best_state = {k:v.detach().cpu().clone() for k,v in node_model.state_dict().items()}\n",
    "            patience_cnt = 0\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            if patience_cnt >= EARLY_STOPPING:\n",
    "                print(\"Early stopping node pretrain.\")\n",
    "                break\n",
    "\n",
    "if best_state is not None:\n",
    "    node_model.load_state_dict(best_state)\n",
    "torch.save(node_model.state_dict(), PRETRAIN_CHECKPOINT)\n",
    "print(\"Saved node-level model checkpoint:\", PRETRAIN_CHECKPOINT)\n",
    "\n",
    "# ----------------- Build graph-level dataset (per-circuit Data objects) -----------------\n",
    "print(\"\\nBuilding graph-level Data objects for fine-tuning (train/val/test circuits) ...\")\n",
    "# load full nodes/edges for graphs (use nodes_df, edges_df from earlier)\n",
    "nodes_full_df = pd.read_csv(NODE_CSV)\n",
    "edges_full_df = pd.read_csv(GRAPH_EDGE_CSV)\n",
    "\n",
    "# Prepare feature scaler: use the scaler fitted during pretraining (we already have StandardScaler scaler)\n",
    "# For nodes not seen in pretrain set, we'll apply same scaler transform using scaler.mean_/var_\n",
    "def node_uid(circuit, node):\n",
    "    return f\"{circuit}::{node}\"\n",
    "\n",
    "# build uid->feature map for all nodes (apply scaler to full dataset)\n",
    "feat_full_df = nodes_full_df.copy()\n",
    "if \"gate_type\" in feat_full_df.columns:\n",
    "    gate_oh = pd.get_dummies(feat_full_df[\"gate_type\"], prefix=\"gt\")\n",
    "    feat_full_df = pd.concat([feat_full_df.drop(columns=[\"gate_type\"]), gate_oh], axis=1)\n",
    "# ensure feature columns compatible: if full has extra gate dummies or missing ones compared to pretrain, align\n",
    "for col in feature_cols:\n",
    "    if col not in feat_full_df.columns:\n",
    "        feat_full_df[col] = 0.0\n",
    "feat_full_df = feat_full_df[[\"circuit_name\",\"node\"] + feature_cols]\n",
    "\n",
    "# scale using pretrain scaler (note: scaler was fitted on pretrain labeled nodes' features)\n",
    "full_X = feat_full_df[feature_cols].fillna(0.0).to_numpy(dtype=np.float32)\n",
    "full_X_scaled = (full_X - scaler.mean_) / np.sqrt(scaler.var_ + 1e-8)  # consistent transform\n",
    "feat_full_df[\"scaled_feat\"] = list(full_X_scaled.tolist())\n",
    "\n",
    "# build edges by circuit\n",
    "edges_full_df[\"src_uid\"] = edges_full_df[\"circuit_name\"].astype(str) + \"::\" + edges_full_df[\"src\"].astype(str)\n",
    "edges_full_df[\"dst_uid\"] = edges_full_df[\"circuit_name\"].astype(str) + \"::\" + edges_full_df[\"dst\"].astype(str)\n",
    "edges_by_circuit = defaultdict(list)\n",
    "for _, r in edges_full_df.iterrows():\n",
    "    edges_by_circuit[r[\"circuit_name\"]].append((r[\"src\"], r[\"dst\"]))\n",
    "\n",
    "# build per-circuit Data (only circuits present in graph_labels)\n",
    "graph_data_list = []\n",
    "graph_names = []\n",
    "graph_target = []\n",
    "for _, row in graph_df.iterrows():\n",
    "    ckt = row[\"circuit_name\"]\n",
    "    lbl = int(row[graph_label_col])\n",
    "    # nodes of this circuit\n",
    "    sub_nodes = feat_full_df[feat_full_df[\"circuit_name\"]==ckt]\n",
    "    if sub_nodes.shape[0] == 0: \n",
    "        continue\n",
    "    node_names = sub_nodes[\"node\"].tolist()\n",
    "    uid_map = {n:i for i,n in enumerate(node_names)}\n",
    "    X_nodes = np.vstack(sub_nodes[\"scaled_feat\"].values).astype(np.float32)\n",
    "    # build edge_index\n",
    "    srcs, dsts = [], []\n",
    "    if ckt in edges_by_circuit:\n",
    "        for u,v in edges_by_circuit[ckt]:\n",
    "            if u in uid_map and v in uid_map:\n",
    "                srcs.extend([uid_map[u], uid_map[v]])\n",
    "                dsts.extend([uid_map[v], uid_map[u]])\n",
    "    if len(srcs) == 0:\n",
    "        # skip graphs without edges (unlikely)\n",
    "        continue\n",
    "    edge_index = torch.tensor([srcs, dsts], dtype=torch.long)\n",
    "    data = Data(x=torch.tensor(X_nodes, dtype=torch.float), edge_index=edge_index, y=torch.tensor([lbl], dtype=torch.long))\n",
    "    data.circuit_name = ckt\n",
    "    graph_data_list.append(data)\n",
    "    graph_names.append(ckt)\n",
    "    graph_target.append(lbl)\n",
    "\n",
    "print(f\"Built {len(graph_data_list)} graphs.\")\n",
    "\n",
    "# Create train/val/test lists by circuit split we made earlier\n",
    "def filter_by_circuit(list_data, circuits_set):\n",
    "    idxs = [i for i,d in enumerate(list_data) if d.circuit_name in circuits_set]\n",
    "    return [list_data[i] for i in idxs]\n",
    "\n",
    "train_graphs = filter_by_circuit(graph_data_list, set(train_circuits))\n",
    "val_graphs   = filter_by_circuit(graph_data_list, set(val_circuits))\n",
    "test_graphs  = filter_by_circuit(graph_data_list, set(test_circuits))\n",
    "\n",
    "print(f\"Graph counts -> train: {len(train_graphs)}, val: {len(val_graphs)}, test: {len(test_graphs)}\")\n",
    "\n",
    "# ----------------- Graph classifier reusing encoder from node_model (TrojanSAINT-style GCN + BN) -----------------\n",
    "class GraphTrojanSAINT(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=HID_DIM, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hid_dim, cached=False, add_self_loops=True, normalize=True)\n",
    "        self.bn1 = nn.BatchNorm1d(hid_dim)\n",
    "        self.conv2 = GCNConv(hid_dim, hid_dim, cached=False, add_self_loops=True, normalize=True)\n",
    "        self.bn2 = nn.BatchNorm1d(hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hid_dim, 2)\n",
    "        nn.init.xavier_uniform_(self.classifier.weight)\n",
    "        if self.classifier.bias is not None:\n",
    "            nn.init.zeros_(self.classifier.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        h = self.conv1(x, edge_index); h = self.bn1(h); h = F.relu(h, inplace=True); h = self.dropout(h)\n",
    "        h = self.conv2(h, edge_index); h = self.bn2(h); h = F.relu(h, inplace=True); h = self.dropout(h)\n",
    "        g = global_mean_pool(h, batch)\n",
    "        return self.classifier(g)\n",
    "\n",
    "graph_model = GraphTrojanSAINT(in_dim=X_pre_t.shape[1], hid_dim=HID_DIM, dropout=DROPOUT).to(DEVICE)\n",
    "\n",
    "# Transfer encoder weights from node_model -> graph_model where shapes match\n",
    "node_state = torch.load(PRETRAIN_CHECKPOINT, map_location=\"cpu\")\n",
    "gstate = graph_model.state_dict()\n",
    "copied = 0\n",
    "for k in gstate.keys():\n",
    "    if k in node_state and gstate[k].shape == node_state[k].shape:\n",
    "        gstate[k] = node_state[k]\n",
    "        copied += 1\n",
    "graph_model.load_state_dict(gstate)\n",
    "print(f\"Transferred {copied} parameter tensors from node encoder to graph encoder.\")\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# --- training/eval (unchanged flow) ---\n",
    "def train_graphs_fn(model, train_set, val_set, test_set, freeze_encoder=True):\n",
    "    # dataloaders\n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    if freeze_encoder:\n",
    "        for p in model.conv1.parameters(): p.requires_grad = False\n",
    "        for p in model.bn1.parameters(): p.requires_grad = False\n",
    "        for p in model.conv2.parameters(): p.requires_grad = False\n",
    "        for p in model.bn2.parameters(): p.requires_grad = False\n",
    "    else:\n",
    "        for p in model.parameters(): p.requires_grad = True\n",
    "\n",
    "    # class weights\n",
    "    ytrain = np.array([int(d.y.item()) for d in train_set])\n",
    "    if len(np.unique(ytrain)) == 2:\n",
    "        counts = np.bincount(ytrain); w = torch.tensor([ (counts.sum()/counts[0]), (counts.sum()/counts[1]) ], dtype=torch.float32).to(DEVICE)\n",
    "        criterion = nn.CrossEntropyLoss(weight=w)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                 lr=FT_HEAD_LR if freeze_encoder else FT_FINETUNE_LR, weight_decay=5e-4)\n",
    "    best_val = -1.0; best_state = None; pcount = 0\n",
    "\n",
    "    for epoch in range(1, FT_EPOCHS_HEAD + 1 if freeze_encoder else FT_EPOCHS_FINETUNE + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch.x, batch.edge_index, batch.batch)\n",
    "            loss = criterion(logits, batch.y.view(-1))\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        ys, ps = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(DEVICE)\n",
    "                out = model(batch.x, batch.edge_index, batch.batch)\n",
    "                preds = out.argmax(dim=1)\n",
    "                ys.extend(batch.y.cpu().numpy()); ps.extend(preds.cpu().numpy())\n",
    "        val_acc = accuracy_score(ys, ps) if len(ys)>0 else 0.0\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(f\"Fine-tune ({'frozen' if freeze_encoder else 'all'}) Epoch {epoch:03d} | Val Acc {val_acc:.4f} | AvgLoss {total_loss / max(1,len(train_set)):.4f}\")\n",
    "        if val_acc > best_val + 1e-4:\n",
    "            best_val = val_acc\n",
    "            best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "            pcount = 0\n",
    "        else:\n",
    "            pcount += 1\n",
    "            if pcount >= EARLY_STOPPING:\n",
    "                print(\"Early stopping fine-tune stage.\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # test eval\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            out = model(batch.x, batch.edge_index, batch.batch)\n",
    "            preds = out.argmax(dim=1)\n",
    "            ys.extend(batch.y.cpu().numpy()); ps.extend(preds.cpu().numpy())\n",
    "    test_acc = accuracy_score(ys, ps) if len(ys)>0 else 0.0\n",
    "    return test_acc, ys, ps\n",
    "\n",
    "# Stage 1: freeze encoder, train head\n",
    "print(\"\\nStage 1: freeze encoder and train classifier head\")\n",
    "for p in graph_model.conv1.parameters(): p.requires_grad = False\n",
    "for p in graph_model.bn1.parameters(): p.requires_grad = False\n",
    "for p in graph_model.conv2.parameters(): p.requires_grad = False\n",
    "for p in graph_model.bn2.parameters(): p.requires_grad = False\n",
    "# Stage 1 call:\n",
    "test_acc_1, ys1, ps1 = train_graphs_fn(graph_model, train_graphs, val_graphs, test_graphs, freeze_encoder=True)\n",
    "print(\"After head training -> Test Acc: {:.4f}\".format(test_acc_1))\n",
    "\n",
    "# Stage 2: unfreeze and fine-tune all (lower lr)\n",
    "print(\"\\nStage 2: unfreeze encoder and fine-tune entire model\")\n",
    "for p in graph_model.parameters(): p.requires_grad = True\n",
    "# update global lr for finetune\n",
    "FT_HEAD_LR = FT_FINETUNE_LR\n",
    "# Stage 2 call:\n",
    "test_acc_2, ys2, ps2 = train_graphs_fn(graph_model, train_graphs, val_graphs, test_graphs, freeze_encoder=False)\n",
    "print(\"After full fine-tune -> Test Acc: {:.4f}\".format(test_acc_2))\n",
    "\n",
    "# Final report\n",
    "final_ys, final_ps = (ys2, ps2) if len(ys2)>0 else (ys1, ps1)\n",
    "print(\"\\nFinal Evaluation (Graph-level after transfer)\")\n",
    "print(\"=============================================\")\n",
    "print(f\"Test Accuracy: {accuracy_score(final_ys, final_ps):.4f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(final_ys, final_ps, digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(final_ys, final_ps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd800ce9-2f2c-4a77-9366-867c114588d8",
   "metadata": {},
   "source": [
    "#### All in One, same perturbation across all metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "988bf3ea-f923-40b6-bc4c-0f674ee5fe95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Robustness Metrics (Graph-Level, Before vs After Perturbation)\n",
      "================================================================\n",
      "Jacobian Sensitivity      orig=0.0000 pert=0.0002 rel.err=2.2547\n",
      "Local Lipschitz           orig=0.0000 pert=0.0002 rel.err=2.2547\n",
      "Hessian Curvature         orig=0.0000 pert=0.0003 rel.err=2.6463\n",
      "Prediction Margin         orig=0.6324 pert=0.4551 rel.err=0.6368\n",
      "Adv Robustness Radius     orig=0.0000 pert=0.0000 rel.err=0.0000\n",
      "Stability under Noise     orig=0.0037 pert=0.0047 rel.err=1.1367\n",
      "\n",
      "Perturbation Success Evaluation\n",
      "================================\n",
      "Perturbation Success Rate: 92.86%\n",
      "\n",
      "Performance on Perturbed Samples Only\n",
      "-------------------------------------\n",
      "Accuracy: 0.07142857142857142\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000         4\n",
      "           1     0.2000    0.1000    0.1333        10\n",
      "\n",
      "    accuracy                         0.0714        14\n",
      "   macro avg     0.1000    0.0500    0.0667        14\n",
      "weighted avg     0.1429    0.0714    0.0952        14\n",
      "\n",
      "Confusion Matrix:\n",
      "[[0 4]\n",
      " [9 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# ----------------- Robustness Metric Functions -----------------\n",
    "def jacobian_sensitivity(model, batch):\n",
    "    x = batch.x.clone().detach().to(DEVICE).requires_grad_(True)\n",
    "    out = model(x, batch.edge_index, batch.batch)\n",
    "    pred_class = out.argmax(dim=1)\n",
    "    loss = F.nll_loss(F.log_softmax(out, dim=1), pred_class)\n",
    "    grads = torch.autograd.grad(loss, x, retain_graph=False, create_graph=False)[0]\n",
    "    return grads.norm(p=\"fro\").item() / x.size(0)\n",
    "\n",
    "def local_lipschitz(model, batch):\n",
    "    x = batch.x.clone().detach().to(DEVICE).requires_grad_(True)\n",
    "    out = model(x, batch.edge_index, batch.batch)\n",
    "    pred_class = out.argmax(dim=1)\n",
    "    loss = F.nll_loss(F.log_softmax(out, dim=1), pred_class)\n",
    "    grads = torch.autograd.grad(loss, x, retain_graph=False, create_graph=False)[0]\n",
    "    return grads.norm(2).item() / x.size(0)\n",
    "\n",
    "def hessian_curvature(model, batch):\n",
    "    x = batch.x.clone().detach().to(DEVICE).requires_grad_(True)\n",
    "    out = model(x, batch.edge_index, batch.batch)\n",
    "    pred_class = out.argmax(dim=1)\n",
    "    loss = F.nll_loss(F.log_softmax(out, dim=1), pred_class)\n",
    "    grad = torch.autograd.grad(loss, x, create_graph=True)[0]\n",
    "    hvp = torch.autograd.grad(grad.sum(), x, retain_graph=False)[0]\n",
    "    return hvp.norm(2).item() / x.size(0)\n",
    "\n",
    "def prediction_margin(model, batch):\n",
    "    out = model(batch.x.to(DEVICE), batch.edge_index, batch.batch)\n",
    "    probs = F.softmax(out, dim=1)\n",
    "    top2 = torch.topk(probs, 2, dim=1).values\n",
    "    return (top2[:,0] - top2[:,1]).mean().item()\n",
    "\n",
    "def adv_robustness_radius(model, batch, eps=1e-3):\n",
    "    x = batch.x.clone().detach().to(DEVICE)\n",
    "    noise = torch.randn_like(x) * eps\n",
    "    out1 = model(x, batch.edge_index, batch.batch).argmax(dim=1)\n",
    "    out2 = model(x + noise, batch.edge_index, batch.batch).argmax(dim=1)\n",
    "    return (out1 != out2).float().mean().item()\n",
    "\n",
    "def stability_under_noise(model, batch, sigma=0.05, trials=5):\n",
    "    diffs = []\n",
    "    x = batch.x.clone().detach().to(DEVICE)\n",
    "    out_orig = model(x, batch.edge_index, batch.batch)\n",
    "    for _ in range(trials):\n",
    "        noise = torch.randn_like(x) * sigma\n",
    "        out_noisy = model(x+noise, batch.edge_index, batch.batch)\n",
    "        diffs.append((out_noisy - out_orig).norm().item())\n",
    "    return np.mean(diffs)\n",
    "\n",
    "# ----------------- Perturbation Function -----------------\n",
    "EPS = 0.3\n",
    "ALPHA = 0.05\n",
    "STEPS = 15\n",
    "NOISE_STD = 0.1\n",
    "\n",
    "def perturb_graph(batch, pgd=True, gaussian=True):\n",
    "    x = batch.x.clone().detach().to(DEVICE)\n",
    "    x.requires_grad_(True)\n",
    "\n",
    "    delta = torch.zeros_like(x).to(DEVICE)\n",
    "    if pgd:\n",
    "        for _ in range(STEPS):\n",
    "            out = graph_model(x + delta, batch.edge_index, batch.batch)\n",
    "            loss = F.cross_entropy(out, batch.y.view(-1))\n",
    "            loss.backward()\n",
    "            grad = x.grad.detach()\n",
    "            delta = (delta + ALPHA * grad.sign()).clamp(-EPS, EPS)\n",
    "            x.grad.zero_()\n",
    "    x_pert = (x + delta).detach()\n",
    "\n",
    "    if gaussian:\n",
    "        noise = torch.randn_like(x_pert) * NOISE_STD\n",
    "        x_pert = x_pert + noise\n",
    "\n",
    "    return x_pert.detach()\n",
    "\n",
    "# ----------------- Metric Collection -----------------\n",
    "def collect_metrics(batch, perturbed=False):\n",
    "    if perturbed:\n",
    "        x_used = perturb_graph(batch, pgd=True, gaussian=True)\n",
    "    else:\n",
    "        x_used = batch.x.clone().detach().to(DEVICE)\n",
    "    b = Data(x=x_used, edge_index=batch.edge_index, y=batch.y, batch=batch.batch).to(DEVICE)\n",
    "    return {\n",
    "        \"jac\": jacobian_sensitivity(graph_model, b),\n",
    "        \"lip\": local_lipschitz(graph_model, b),\n",
    "        \"hess\": hessian_curvature(graph_model, b),\n",
    "        \"marg\": prediction_margin(graph_model, b),\n",
    "        \"rad\": adv_robustness_radius(graph_model, b),\n",
    "        \"stab\": stability_under_noise(graph_model, b)\n",
    "    }\n",
    "\n",
    "orig_metrics, pert_metrics = [], []\n",
    "ys_true, ys_pred, ys_pred_pert = [], [], []\n",
    "\n",
    "for batch in test_graphs:\n",
    "    batch = batch.to(DEVICE)\n",
    "\n",
    "    # metrics before/after perturbation\n",
    "    orig_m = collect_metrics(batch, perturbed=False)\n",
    "    pert_m = collect_metrics(batch, perturbed=True)\n",
    "    orig_metrics.append(orig_m); pert_metrics.append(pert_m)\n",
    "\n",
    "    # predictions\n",
    "    with torch.no_grad():\n",
    "        out = graph_model(batch.x, batch.edge_index, batch.batch)\n",
    "        ys_true.append(batch.y.item())\n",
    "        ys_pred.append(out.argmax(dim=1).item())\n",
    "    x_pert = perturb_graph(batch, pgd=True, gaussian=True)\n",
    "    with torch.no_grad():\n",
    "        out_p = graph_model(x_pert, batch.edge_index, batch.batch)\n",
    "        ys_pred_pert.append(out_p.argmax(dim=1).item())\n",
    "\n",
    "# ----------------- Summarize Results -----------------\n",
    "def summarize_with_relerr(name, key):\n",
    "    orig_vals = [m[key] for m in orig_metrics]\n",
    "    pert_vals = [m[key] for m in pert_metrics]\n",
    "    rel_errs = [abs(p-o)/(abs(o)+1e-8) for o,p in zip(orig_vals, pert_vals)]\n",
    "    print(f\"{name:<25} orig={np.mean(orig_vals):.4f} pert={np.mean(pert_vals):.4f} \"\n",
    "          f\"rel.err={np.mean(rel_errs):.4f}\")\n",
    "\n",
    "print(\"\\nRobustness Metrics (Graph-Level, Before vs After Perturbation)\")\n",
    "print(\"================================================================\")\n",
    "summarize_with_relerr(\"Jacobian Sensitivity\", \"jac\")\n",
    "summarize_with_relerr(\"Local Lipschitz\", \"lip\")\n",
    "summarize_with_relerr(\"Hessian Curvature\", \"hess\")\n",
    "summarize_with_relerr(\"Prediction Margin\", \"marg\")\n",
    "summarize_with_relerr(\"Adv Robustness Radius\", \"rad\")\n",
    "summarize_with_relerr(\"Stability under Noise\", \"stab\")\n",
    "\n",
    "# Perturbation success\n",
    "ys_true = np.array(ys_true); ys_pred = np.array(ys_pred); ys_pred_pert = np.array(ys_pred_pert)\n",
    "success_rate = np.mean(ys_pred != ys_pred_pert) * 100\n",
    "print(\"\\nPerturbation Success Evaluation\")\n",
    "print(\"================================\")\n",
    "print(f\"Perturbation Success Rate: {success_rate:.2f}%\")\n",
    "\n",
    "print(\"\\nPerformance on Perturbed Samples Only\")\n",
    "print(\"-------------------------------------\")\n",
    "print(\"Accuracy:\", accuracy_score(ys_true, ys_pred_pert))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(ys_true, ys_pred_pert, digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(ys_true, ys_pred_pert))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54ec0a70-c165-4bb8-8772-bd5944c946a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Robustness Metrics (Graph-Level, Before vs After Perturbation)\n",
      "================================================================\n",
      "Jacobian Sensitivity     \n",
      "  Clean : avg=0.0000 ± 0.0001, avg_relerr=2.2547e+00 ± 3.2201e+00\n",
      "  Pert. : avg=0.0002 ± 0.0005, avg_relerr=2.2547e+00 ± 3.2201e+00\n",
      "\n",
      "Local Lipschitz          \n",
      "  Clean : avg=0.0000 ± 0.0001, avg_relerr=2.2547e+00 ± 3.2201e+00\n",
      "  Pert. : avg=0.0002 ± 0.0005, avg_relerr=2.2547e+00 ± 3.2201e+00\n",
      "\n",
      "Hessian Curvature        \n",
      "  Clean : avg=0.0000 ± 0.0001, avg_relerr=2.6463e+00 ± 3.8600e+00\n",
      "  Pert. : avg=0.0003 ± 0.0008, avg_relerr=2.6463e+00 ± 3.8600e+00\n",
      "\n",
      "Prediction Margin        \n",
      "  Clean : avg=0.6324 ± 0.2019, avg_relerr=6.3677e-01 ± 1.0521e+00\n",
      "  Pert. : avg=0.4551 ± 0.2025, avg_relerr=6.3677e-01 ± 1.0521e+00\n",
      "\n",
      "Adv Robustness Radius    \n",
      "  Clean : avg=0.0000 ± 0.0000, avg_relerr=0.0000e+00 ± 0.0000e+00\n",
      "  Pert. : avg=0.0000 ± 0.0000, avg_relerr=0.0000e+00 ± 0.0000e+00\n",
      "\n",
      "Stability under Noise    \n",
      "  Clean : avg=0.0037 ± 0.0045, avg_relerr=1.1367e+00 ± 1.3012e+00\n",
      "  Pert. : avg=0.0047 ± 0.0042, avg_relerr=1.1367e+00 ± 1.3012e+00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Summarize Results (with mean ± std) -----------------\n",
    "def summarize_with_relerr(name, key):\n",
    "    orig_vals = np.array([m[key] for m in orig_metrics])\n",
    "    pert_vals = np.array([m[key] for m in pert_metrics])\n",
    "    rel_errs = np.abs(pert_vals - orig_vals) / (np.abs(orig_vals) + 1e-8)\n",
    "\n",
    "    mean_orig, std_orig = orig_vals.mean(), orig_vals.std()\n",
    "    mean_pert, std_pert = pert_vals.mean(), pert_vals.std()\n",
    "    mean_rel, std_rel = rel_errs.mean(), rel_errs.std()\n",
    "\n",
    "    print(f\"{name:<25}\")\n",
    "    print(f\"  Clean : avg={mean_orig:.4f} ± {std_orig:.4f}, \"\n",
    "          f\"avg_relerr={mean_rel:.4e} ± {std_rel:.4e}\")\n",
    "    print(f\"  Pert. : avg={mean_pert:.4f} ± {std_pert:.4f}, \"\n",
    "          f\"avg_relerr={mean_rel:.4e} ± {std_rel:.4e}\\n\")\n",
    "\n",
    "print(\"\\nRobustness Metrics (Graph-Level, Before vs After Perturbation)\")\n",
    "print(\"================================================================\")\n",
    "summarize_with_relerr(\"Jacobian Sensitivity\", \"jac\")\n",
    "summarize_with_relerr(\"Local Lipschitz\", \"lip\")\n",
    "summarize_with_relerr(\"Hessian Curvature\", \"hess\")\n",
    "summarize_with_relerr(\"Prediction Margin\", \"marg\")\n",
    "summarize_with_relerr(\"Adv Robustness Radius\", \"rad\")\n",
    "summarize_with_relerr(\"Stability under Noise\", \"stab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694e0ab0-3b2e-4558-972b-b1b5ae751e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
