{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa6ce9e-9a74-4424-a211-23c117d27959",
   "metadata": {},
   "source": [
    "#### Training Evaluation with GraphSAINT taken from https://github.com/DfX-NYUAD/TrojanSAINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79de5491-1d0a-4061-a85c-bb3bdf26c81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss 0.2149 | Val 0.2494 | Test 0.2495\n",
      "Epoch 010 | Loss 0.0014 | Val 0.2497 | Test 0.2500\n",
      "Epoch 020 | Loss 0.0007 | Val 0.3833 | Test 0.3831\n",
      "Epoch 030 | Loss 0.0006 | Val 0.3490 | Test 0.3488\n",
      "Epoch 040 | Loss 0.0006 | Val 0.2675 | Test 0.2687\n",
      "Epoch 050 | Loss 0.0025 | Val 0.8438 | Test 0.8493\n",
      "Epoch 060 | Loss 0.0005 | Val 0.9770 | Test 0.9776\n",
      "Epoch 070 | Loss 0.0005 | Val 0.9625 | Test 0.9623\n",
      "Epoch 080 | Loss 0.0015 | Val 0.7825 | Test 0.7851\n",
      "Epoch 090 | Loss 0.0004 | Val 0.3912 | Test 0.3925\n",
      "Epoch 100 | Loss 0.0004 | Val 0.2504 | Test 0.2509\n",
      "Epoch 110 | Loss 0.0004 | Val 0.2494 | Test 0.2495\n",
      "Epoch 120 | Loss 0.0004 | Val 0.2783 | Test 0.2783\n",
      "Epoch 130 | Loss 0.0004 | Val 0.2757 | Test 0.2754\n",
      "Epoch 140 | Loss 0.0019 | Val 0.2494 | Test 0.2495\n",
      "Epoch 150 | Loss 0.0020 | Val 0.2494 | Test 0.2495\n",
      "Epoch 160 | Loss 0.0019 | Val 0.2494 | Test 0.2495\n",
      "Epoch 170 | Loss 0.0004 | Val 0.2494 | Test 0.2495\n",
      "Epoch 180 | Loss 0.0004 | Val 0.2494 | Test 0.2495\n",
      "Epoch 190 | Loss 0.0004 | Val 0.2494 | Test 0.2495\n",
      "Epoch 200 | Loss 0.0004 | Val 0.2494 | Test 0.2495\n",
      "Epoch 210 | Loss 0.0004 | Val 0.2494 | Test 0.2495\n",
      "Epoch 220 | Loss 0.0004 | Val 0.2494 | Test 0.2495\n",
      "Epoch 230 | Loss 0.0004 | Val 0.2494 | Test 0.2495\n",
      "Epoch 240 | Loss 0.0004 | Val 0.2494 | Test 0.2495\n",
      "Epoch 250 | Loss 0.0004 | Val 0.2494 | Test 0.2495\n",
      "Epoch 260 | Loss 0.0004 | Val 0.2494 | Test 0.2495\n",
      "Early stopping.\n",
      "\n",
      "Final Evaluation (Node-Level, TrojanSAINT/GraphSAINT-style)\n",
      "============================================================\n",
      "Test Accuracy: 0.9776\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     0.9176    1.0000    0.9570      9159\n",
      "      trojan     1.0000    0.9701    0.9848     27556\n",
      "\n",
      "    accuracy                         0.9776     36715\n",
      "   macro avg     0.9588    0.9851    0.9709     36715\n",
      "weighted avg     0.9794    0.9776    0.9779     36715\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 9159     0]\n",
      " [  823 26733]]\n"
     ]
    }
   ],
   "source": [
    "# train_trojansaint_node_fixed.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "NODE_CSV  = \"GNNDatasets/node.csv\"\n",
    "EDGE_CSV  = \"GNNDatasets/node_edges.csv\"\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# ----------------------------- Load nodes -----------------------------\n",
    "nodes_df = pd.read_csv(NODE_CSV)\n",
    "\n",
    "label_col = None\n",
    "for cand in [\"label\", \"is_trojan\", \"trojan\", \"target\"]:\n",
    "    if cand in nodes_df.columns:\n",
    "        label_col = cand; break\n",
    "if label_col is None:\n",
    "    nodes_df[\"label\"] = nodes_df[\"circuit_name\"].astype(str).str.contains(\"__trojan_\").astype(int)\n",
    "    label_col = \"label\"\n",
    "\n",
    "nodes_df[\"uid\"] = nodes_df[\"circuit_name\"].astype(str) + \"::\" + nodes_df[\"node\"].astype(str)\n",
    "\n",
    "feat_df = nodes_df.copy()\n",
    "if \"gate_type\" in feat_df.columns:\n",
    "    gate_oh = pd.get_dummies(feat_df[\"gate_type\"], prefix=\"gt\")\n",
    "    feat_df = pd.concat([feat_df.drop(columns=[\"gate_type\"]), gate_oh], axis=1)\n",
    "\n",
    "exclude = {\"uid\",\"node\",\"circuit_name\",label_col}\n",
    "num_cols = [c for c in feat_df.columns if c not in exclude and pd.api.types.is_numeric_dtype(feat_df[c])]\n",
    "X = feat_df[num_cols].fillna(0.0).values.astype(np.float32)\n",
    "y = nodes_df[label_col].values.astype(np.int64)\n",
    "\n",
    "# ----------------------------- Load edges; add missing nodes -----------------------------\n",
    "edges_df = pd.read_csv(EDGE_CSV)\n",
    "edges_df[\"src_uid\"] = edges_df[\"circuit_name\"].astype(str) + \"::\" + edges_df[\"src\"].astype(str)\n",
    "edges_df[\"dst_uid\"] = edges_df[\"circuit_name\"].astype(str) + \"::\" + edges_df[\"dst\"].astype(str)\n",
    "\n",
    "known_uids = set(nodes_df[\"uid\"])\n",
    "edge_uids = set(edges_df[\"src_uid\"]).union(set(edges_df[\"dst_uid\"]))\n",
    "missing = list(edge_uids - known_uids)\n",
    "\n",
    "if missing:\n",
    "    zero_row = np.zeros((1, X.shape[1]), dtype=np.float32)\n",
    "    addX = np.repeat(zero_row, len(missing), axis=0)\n",
    "    addY = -1*np.ones(len(missing), dtype=np.int64)\n",
    "    add_df = pd.DataFrame({\n",
    "        \"uid\": missing,\n",
    "        \"circuit_name\": [u.split(\"::\",1)[0] for u in missing],\n",
    "        \"node\": [u.split(\"::\",1)[1] for u in missing],\n",
    "        label_col: addY\n",
    "    })\n",
    "    X = np.vstack([X, addX])\n",
    "    y = np.concatenate([y, addY])\n",
    "    nodes_df = pd.concat([nodes_df, add_df], ignore_index=True)\n",
    "\n",
    "uid_to_idx = {u:i for i,u in enumerate(nodes_df[\"uid\"].tolist())}\n",
    "src_idx = edges_df[\"src_uid\"].map(uid_to_idx).dropna().astype(int).values\n",
    "dst_idx = edges_df[\"dst_uid\"].map(uid_to_idx).dropna().astype(int).values\n",
    "edge_index = np.stack([np.concatenate([src_idx, dst_idx]),\n",
    "                       np.concatenate([dst_idx, src_idx])], axis=0)\n",
    "\n",
    "num_nodes = X.shape(0) if callable(getattr(X, \"shape\", None)) else X.shape[0]\n",
    "\n",
    "# ----------------------------- Scale features -----------------------------\n",
    "labeled_mask_np = (y >= 0)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "X_scaled[labeled_mask_np] = scaler.fit_transform(X_scaled[labeled_mask_np])\n",
    "if (~labeled_mask_np).any():\n",
    "    X_scaled[~labeled_mask_np] = (X_scaled[~labeled_mask_np] - scaler.mean_) / np.sqrt(scaler.var_ + 1e-8)\n",
    "\n",
    "# ----------------------------- Splits -----------------------------\n",
    "idx_all = np.where(labeled_mask_np)[0]\n",
    "y_all = y[labeled_mask_np]\n",
    "\n",
    "idx_train, idx_tmp, y_train, y_tmp = train_test_split(\n",
    "    idx_all, y_all, test_size=0.30, random_state=SEED, stratify=y_all\n",
    ")\n",
    "idx_val, idx_test, y_val, y_test = train_test_split(\n",
    "    idx_tmp, y_tmp, test_size=0.50, random_state=SEED, stratify=y_tmp\n",
    ")\n",
    "\n",
    "# ----------------------------- Torch tensors -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_t = torch.from_numpy(X_scaled).to(device)\n",
    "y_t = torch.from_numpy(y).to(device)\n",
    "edge_index_t = torch.from_numpy(edge_index).long().to(device)\n",
    "\n",
    "train_mask_t = torch.zeros(len(y), dtype=torch.bool, device=device); train_mask_t[idx_train] = True\n",
    "val_mask_t   = torch.zeros(len(y), dtype=torch.bool, device=device); val_mask_t[idx_val]   = True\n",
    "test_mask_t  = torch.zeros(len(y), dtype=torch.bool, device=device); test_mask_t[idx_test] = True\n",
    "labeled_mask_t = torch.from_numpy(labeled_mask_np).to(device)\n",
    "\n",
    "# ----------------------------- Utility: degree, neighbors -----------------------------\n",
    "def degrees(num_nodes, ei):\n",
    "    deg = torch.bincount(ei[0], minlength=num_nodes).float()\n",
    "    return deg\n",
    "\n",
    "def coalesce_edge_index(ei):\n",
    "    # simple coalesce: unique columns of ei (2 x E)\n",
    "    u = ei[0]* (ei[0].max()+1) + ei[1]\n",
    "    uniq, idx = torch.unique(u, return_inverse=True)\n",
    "    ei0 = uniq // (ei[0].max()+1)\n",
    "    ei1 = uniq %  (ei[0].max()+1)\n",
    "    return torch.stack([ei0, ei1], dim=0)\n",
    "\n",
    "# ----------------------------- GraphSAINT-style sampler -----------------------------\n",
    "class GraphSAINTSampler:\n",
    "    def __init__(self, num_nodes, edge_index, train_mask, batch_size=2000, num_steps=1,\n",
    "                 bias_by_degree=True, device='cpu'):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.edge_index = edge_index\n",
    "        self.train_mask = train_mask\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.device = device\n",
    "\n",
    "        deg = torch.bincount(edge_index[0], minlength=num_nodes).float()\n",
    "        deg = deg + 1e-6\n",
    "        base_prob = deg / deg.sum() if bias_by_degree else torch.full(\n",
    "            (num_nodes,), 1.0/num_nodes, device=edge_index.device\n",
    "        )\n",
    "\n",
    "        self.sample_space = torch.where(train_mask)[0]\n",
    "        p_space = base_prob[self.sample_space]\n",
    "        self.p_space = p_space / p_space.sum()\n",
    "        self.base_prob = base_prob\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def next_seeds(self):\n",
    "        replace = self.sample_space.numel() < self.batch_size\n",
    "        idx = torch.multinomial(self.p_space, self.batch_size, replacement=replace)\n",
    "        return self.sample_space[idx]\n",
    "\n",
    "    def _induce_subgraph(self, seeds):\n",
    "        ei = self.edge_index\n",
    "        mask = (ei[0].unsqueeze(1) == seeds.unsqueeze(0)).any(dim=1) | \\\n",
    "               (ei[1].unsqueeze(1) == seeds.unsqueeze(0)).any(dim=1)\n",
    "        sub_ei = ei[:, mask]\n",
    "        nodes = torch.unique(torch.cat([sub_ei[0], sub_ei[1], seeds], dim=0))\n",
    "\n",
    "        nid_map = -torch.ones(self.num_nodes, dtype=torch.long, device=ei.device)\n",
    "        nid_map[nodes] = torch.arange(nodes.numel(), device=ei.device)\n",
    "        sub_ei = nid_map[sub_ei]\n",
    "\n",
    "        if sub_ei.numel() == 0:\n",
    "            sub_ei = torch.stack([torch.tensor([0], device=ei.device), torch.tensor([0], device=ei.device)], dim=0)\n",
    "            nodes = nodes[:1]\n",
    "\n",
    "        u = sub_ei[0] * nodes.numel() + sub_ei[1]\n",
    "        uniq = torch.unique(u)\n",
    "        sub_ei0 = uniq // nodes.numel()\n",
    "        sub_ei1 = uniq %  nodes.numel()\n",
    "        sub_ei = torch.stack([sub_ei0, sub_ei1], dim=0)\n",
    "\n",
    "        self_loops = torch.arange(nodes.numel(), device=ei.device)\n",
    "        sub_ei = torch.cat([sub_ei, torch.stack([self_loops, self_loops])], dim=1)\n",
    "\n",
    "        deg = torch.bincount(sub_ei[0], minlength=nodes.numel()).float()\n",
    "        deg_inv_sqrt = deg.clamp(min=1).pow(-0.5)\n",
    "        w = deg_inv_sqrt[sub_ei[0]] * deg_inv_sqrt[sub_ei[1]]\n",
    "        A = torch.sparse_coo_tensor(sub_ei, w, (nodes.numel(), nodes.numel())).coalesce()\n",
    "        return nodes, A\n",
    "\n",
    "    def __next__(self):\n",
    "        seeds = self.next_seeds()\n",
    "        nodes, A = self._induce_subgraph(seeds)\n",
    "        p = torch.clamp(self.base_prob[nodes], min=1e-8)\n",
    "        norm_loss = (1.0 / p) / (1.0 / p).mean()\n",
    "        return nodes, A, norm_loss\n",
    "\n",
    "\n",
    "# ----------------------------- TrojanSAINT (GraphSAINT-style) model -----------------------------\n",
    "class SAINTGCNLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, dropout=0.0, bias=True):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(in_dim, out_dim, bias=bias)\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        nn.init.xavier_uniform_(self.lin.weight)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # GCN propagation on sampled subgraph\n",
    "        x = torch.sparse.mm(adj, x)\n",
    "        x = self.lin(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TrojanSAINT(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=96, out_dim=2, dropout=0.35):\n",
    "        super().__init__()\n",
    "        self.g1 = SAINTGCNLayer(in_dim, hid_dim, dropout=dropout)\n",
    "        self.g2 = nn.Linear(hid_dim, out_dim, bias=True)\n",
    "        nn.init.xavier_uniform_(self.g2.weight)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.g1(x, adj)\n",
    "        x = torch.sparse.mm(adj, x)  # last propagation (linear head after propagation)\n",
    "        x = self.g2(x)\n",
    "        return x\n",
    "\n",
    "model = TrojanSAINT(in_dim=X_t.size(1), hid_dim=96, out_dim=2, dropout=0.35).to(device)\n",
    "\n",
    "# ----------------------------- Class weights, loss, optimizer -----------------------------\n",
    "train_labels = y_t[train_mask_t]\n",
    "classes, counts = torch.unique(train_labels, return_counts=True)\n",
    "num_pos = counts[classes==1].item() if (classes==1).any() else 1\n",
    "num_neg = counts[classes==0].item() if (classes==0).any() else 1\n",
    "weight_pos = (num_neg + num_pos) / (2.0 * num_pos)\n",
    "weight_neg = (num_neg + num_pos) / (2.0 * num_neg)\n",
    "class_weights = torch.tensor([weight_neg, weight_pos], dtype=torch.float32, device=device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none', weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3, weight_decay=5e-4)\n",
    "\n",
    "# ----------------------------- Sampler -----------------------------\n",
    "num_nodes = X_t.size(0)\n",
    "sampler = GraphSAINTSampler(\n",
    "    num_nodes=num_nodes,\n",
    "    edge_index=edge_index_t,\n",
    "    train_mask=train_mask_t & (y_t >= 0),\n",
    "    batch_size=min(4000, (train_mask_t & (y_t >= 0)).sum().item()),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ----------------------------- Evaluation (full-graph) -----------------------------\n",
    "def build_full_adj(num_nodes, edge_index):\n",
    "    self_loops = torch.arange(num_nodes, device=edge_index.device)\n",
    "    ei = torch.cat([edge_index, torch.stack([self_loops, self_loops])], dim=1)\n",
    "    deg = torch.bincount(ei[0], minlength=num_nodes).float()\n",
    "    deg_inv_sqrt = deg.clamp(min=1).pow(-0.5)\n",
    "    w = deg_inv_sqrt[ei[0]] * deg_inv_sqrt[ei[1]]\n",
    "    A = torch.sparse_coo_tensor(ei, w, (num_nodes, num_nodes))\n",
    "    return A.coalesce()\n",
    "\n",
    "A_full = build_full_adj(X_t.size(0), edge_index_t)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(mask_t):\n",
    "    model.eval()\n",
    "    logits = model(X_t, A_full)\n",
    "    pred = logits.argmax(dim=1)\n",
    "    msk = mask_t & (y_t >= 0)\n",
    "    if msk.sum() == 0: return 0.0\n",
    "    return (pred[msk] == y_t[msk]).float().mean().item()\n",
    "\n",
    "# ----------------------------- Training (GraphSAINT-style) -----------------------------\n",
    "best_val, best_state = -1.0, None\n",
    "patience, patience_cnt = 20, 0\n",
    "EPOCHS = 300\n",
    "steps_per_epoch = max(1, int(np.ceil((train_mask_t & (y_t >= 0)).sum().item() / sampler.batch_size)))\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for _ in range(steps_per_epoch):\n",
    "        nodes, A_b, norm_loss = next(sampler)\n",
    "        x_b = X_t[nodes]\n",
    "        y_b = y_t[nodes]\n",
    "        train_b = train_mask_t[nodes] & (y_b >= 0)\n",
    "\n",
    "        if train_b.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        logits_b = model(x_b, A_b)\n",
    "        loss_raw = criterion(logits_b[train_b], y_b[train_b])\n",
    "        # GraphSAINT normalization proxy: weight per-node loss by norm_loss\n",
    "        loss = (loss_raw * norm_loss[train_b]).mean()\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        val_acc = evaluate(val_mask_t)\n",
    "        test_acc = evaluate(test_mask_t)\n",
    "        print(f\"Epoch {epoch:03d} | Loss {epoch_loss/steps_per_epoch:.4f} | Val {val_acc:.4f} | Test {test_acc:.4f}\")\n",
    "        if val_acc > best_val + 1e-4:\n",
    "            best_val = val_acc\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience_cnt = 0\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            if patience_cnt >= patience:\n",
    "                print(\"Early stopping.\"); break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "# ----------------------------- Final eval -----------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X_t, A_full)\n",
    "    preds = logits.argmax(dim=1)\n",
    "\n",
    "msk = (test_mask_t & (y_t >= 0)).cpu().numpy()\n",
    "y_true = y_t.cpu().numpy()[msk]\n",
    "y_pred = preds.cpu().numpy()[msk]\n",
    "\n",
    "acc = (y_true == y_pred).mean()\n",
    "print(\"\\nFinal Evaluation (Node-Level, TrojanSAINT/GraphSAINT-style)\")\n",
    "print(\"============================================================\")\n",
    "print(f\"Test Accuracy: {acc:.4f}\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, labels=[0,1], target_names=[\"clean\",\"trojan\"], digits=4))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=[0,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231ef087-787a-4619-9821-37653ff3fb50",
   "metadata": {},
   "source": [
    "#### All in One, same perturbation across all metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13b98dad-6684-4cda-bed4-566920850a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 50 clean and 50 trojan nodes from test set.\n",
      "Perturbation success: 45/100 (45.00%)\n",
      "\n",
      "Performance on Perturbed Selected Nodes\n",
      "---------------------------------------\n",
      "Accuracy: 0.5000, Precision: 0.2500, Recall: 0.5000, F1: 0.3333\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     0.5000    1.0000    0.6667        50\n",
      "      trojan     0.0000    0.0000    0.0000        50\n",
      "\n",
      "    accuracy                         0.5000       100\n",
      "   macro avg     0.2500    0.5000    0.3333       100\n",
      "weighted avg     0.2500    0.5000    0.3333       100\n",
      "\n",
      "Confusion Matrix:\n",
      "[[50  0]\n",
      " [50  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jacobian Frobenius Norm (sampled nodes):\n",
      " Node 14263: 1.8702\n",
      " Node 23954: 1.8375\n",
      " Node 35912: 1.3144\n",
      " Node 9652: 1.8866\n",
      " Node 31500: 1.0735\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Robustness PGD + Jacobian (Node-level, TrojanSAINT)\n",
    "# =========================\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "PER_CLASS     = 50\n",
    "EPSILON_PGD   = 1.5\n",
    "ALPHA_PGD     = 0.3\n",
    "NUM_ITERS_PGD = 20\n",
    "FD_EPS        = 1e-3\n",
    "SEED          = 42\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "device = X_t.device\n",
    "\n",
    "# -------------------------\n",
    "# Select test nodes\n",
    "# -------------------------\n",
    "test_mask_np = test_mask_t.cpu().numpy() & (y_t.cpu().numpy() >= 0)\n",
    "y_np = y_t.cpu().numpy()\n",
    "idx_test = np.where(test_mask_np)[0]\n",
    "\n",
    "sel_clean  = np.random.choice(idx_test[y_np[idx_test]==0],\n",
    "                              size=min(PER_CLASS, (y_np[idx_test]==0).sum()),\n",
    "                              replace=False)\n",
    "sel_trojan = np.random.choice(idx_test[y_np[idx_test]==1],\n",
    "                              size=min(PER_CLASS, (y_np[idx_test]==1).sum()),\n",
    "                              replace=False)\n",
    "selected = np.concatenate([sel_clean, sel_trojan])\n",
    "print(f\"Selected {len(sel_clean)} clean and {len(sel_trojan)} trojan nodes from test set.\")\n",
    "\n",
    "# -------------------------\n",
    "# PGD perturbation per node\n",
    "# -------------------------\n",
    "perturbed_feats = X_t.clone().detach()\n",
    "orig_preds, adv_preds = [], []\n",
    "\n",
    "for idx in selected:\n",
    "    x_adv = X_t.clone().detach()\n",
    "    x_adv.requires_grad_(True)\n",
    "\n",
    "    for it in range(NUM_ITERS_PGD):\n",
    "        logits = model(x_adv, A_full)\n",
    "        loss = F.cross_entropy(logits[idx:idx+1], y_t[idx:idx+1])\n",
    "        grad = torch.autograd.grad(loss, x_adv, retain_graph=False, create_graph=False)[0]\n",
    "        g_node = grad[idx]\n",
    "        step = ALPHA_PGD * g_node / (g_node.norm() + 1e-12)\n",
    "        x_adv = x_adv.detach()\n",
    "        x_adv[idx] = (x_adv[idx] + step).detach()\n",
    "        # project back into L2 ball\n",
    "        diff = x_adv[idx] - X_t[idx]\n",
    "        if diff.norm() > EPSILON_PGD:\n",
    "            diff = diff * (EPSILON_PGD / (diff.norm() + 1e-12))\n",
    "            x_adv[idx] = X_t[idx] + diff\n",
    "        x_adv.requires_grad_(True)\n",
    "\n",
    "    perturbed_feats[idx] = x_adv[idx].detach()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        orig_preds.append(int(model(X_t, A_full)[idx].argmax().item()))\n",
    "        adv_preds.append(int(model(perturbed_feats, A_full)[idx].argmax().item()))\n",
    "\n",
    "orig_preds, adv_preds = np.array(orig_preds), np.array(adv_preds)\n",
    "num_flips = (orig_preds != adv_preds).sum()\n",
    "print(f\"Perturbation success: {num_flips}/{len(selected)} \"\n",
    "      f\"({100.0*num_flips/len(selected):.2f}%)\")\n",
    "\n",
    "# -------------------------\n",
    "# Evaluate perturbed selected nodes\n",
    "# -------------------------\n",
    "labels_sel = y_np[selected]\n",
    "with torch.no_grad():\n",
    "    logits_adv = model(perturbed_feats, A_full)[selected]\n",
    "    preds_adv = logits_adv.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "acc = (preds_adv == labels_sel).mean()\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(labels_sel, preds_adv, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"\\nPerformance on Perturbed Selected Nodes\")\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(labels_sel, preds_adv, labels=[0,1], target_names=[\"clean\",\"trojan\"], digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(labels_sel, preds_adv, labels=[0,1]))\n",
    "\n",
    "# -------------------------\n",
    "# Jacobian Frobenius norm example\n",
    "# -------------------------\n",
    "jacobian_vals = []\n",
    "for idx in selected[:5]:  # sample a few nodes (Jacobian is expensive)\n",
    "    x_in = perturbed_feats.clone().detach().requires_grad_(True)\n",
    "    def f_node(z):\n",
    "        return model(z, A_full)[idx]\n",
    "    J = torch.autograd.functional.jacobian(f_node, x_in)\n",
    "    # shape: (C, N, F) ? flatten node dimension\n",
    "    if J.ndim == 3:\n",
    "        J = J[:, idx, :]  # only keep Jacobian wrt that nodes features\n",
    "    frob = torch.norm(J, p='fro').item()\n",
    "    jacobian_vals.append((idx, frob))\n",
    "\n",
    "print(\"\\nJacobian Frobenius Norm (sampled nodes):\")\n",
    "for idx, val in jacobian_vals:\n",
    "    print(f\" Node {idx}: {val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15212bcf-78f6-4e1a-8d06-fce03304d5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jacobian Sensitivity (on perturbed selected nodes):\n",
      " Clean:  avg=1.5936 ± 0.3236, avg_relerr=7.5815e-04 ± 2.4729e-03\n",
      " Trojan: avg=4.7731 ± 1.9465, avg_relerr=2.5976e-04 ± 7.8128e-04\n",
      "\n",
      "Local Lipschitz (on perturbed selected nodes):\n",
      " Clean:  avg=1.5932 ± 0.3235, avg_relerr=1.5952e-03 ± 3.6699e-03\n",
      " Trojan: avg=4.7710 ± 1.9470, avg_relerr=6.2384e-04 ± 1.8397e-03\n",
      "\n",
      "Hessian Curvature (on perturbed selected nodes):\n",
      " Clean:  avg=0.0010 ± 0.0023, avg_relerr=1.0013e+00 ± 7.1515e-03\n",
      " Trojan: avg=0.1581 ± 0.5224, avg_relerr=1.0614e+00 ± 1.6674e-01\n",
      "\n",
      "Prediction Margin (on perturbed selected nodes):\n",
      " Clean:  avg=5.9187 ± 1.3137, avg_relerr=3.2050e-04 ± 2.4754e-04\n",
      " Trojan: avg=3.7113 ± 1.4298, avg_relerr=1.4169e-03 ± 1.1791e-03\n",
      "\n",
      "Adversarial Robustness Radius (on perturbed selected nodes):\n",
      " Clean:  avg=6.0185 ± 0.0000, avg_relerr=4.4652e-02 ± 0.0000e+00\n",
      " Trojan: avg=5.5198 ± 1.2059, avg_relerr=1.4842e-01 ± 2.2524e-01\n",
      "\n",
      "Stability under Noise (on perturbed selected nodes):\n",
      " Clean:  avg=0.5819 ± 0.2441, avg_relerr=3.2175e-01 ± 1.9590e-01\n",
      " Trojan: avg=1.8602 ± 0.9110, avg_relerr=3.2100e-01 ± 2.3982e-01\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Robustness Metrics (Node-Level, TrojanSAINT)\n",
    "# ================================\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "# ------------- Parameters -------------\n",
    "PER_CLASS     = 50\n",
    "EPSILON_PGD   = 1.5\n",
    "ALPHA_PGD     = 0.3\n",
    "NUM_ITERS_PGD = 20\n",
    "FD_EPS        = 1e-3\n",
    "ARR_INITIAL_EPS  = 1e-3\n",
    "ARR_GROW1        = 1.25\n",
    "ARR_GROW2        = 1.4\n",
    "ARR_MAX_EPS      = 5.0\n",
    "ARR_BS_ITERS     = 5\n",
    "ARR_TRIALS       = 3\n",
    "STAB_SIGMA       = 0.5\n",
    "STAB_SAMPLES     = 5\n",
    "SEED          = 42\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "device = X_t.device\n",
    "\n",
    "\n",
    "# ------------- Utility: stats -------------\n",
    "def mean_std(a):\n",
    "    return (np.mean(a).item() if len(a) else 0.0,\n",
    "            np.std(a).item() if len(a) else 0.0)\n",
    "\n",
    "def print_stats(name, clean_vals, troj_vals, clean_errs, troj_errs):\n",
    "    c_mean, c_std = mean_std(clean_vals)\n",
    "    t_mean, t_std = mean_std(troj_vals)\n",
    "    ce_mean, ce_std = mean_std(clean_errs)\n",
    "    te_mean, te_std = mean_std(troj_errs)\n",
    "    print(f\"\\n{name} (on perturbed selected nodes):\")\n",
    "    print(f\" Clean:  avg={c_mean:.4f} ± {c_std:.4f}, avg_relerr={ce_mean:.4e} ± {ce_std:.4e}\")\n",
    "    print(f\" Trojan: avg={t_mean:.4f} ± {t_std:.4f}, avg_relerr={te_mean:.4e} ± {te_std:.4e}\")\n",
    "\n",
    "# ------------- Metric 1: Jacobian -------------\n",
    "jac_vals, jac_errs, labels = [], [], []\n",
    "for idx in selected:\n",
    "    x_in = perturbed_feats.clone().detach().requires_grad_(True)\n",
    "    def f_node(z): return model(z, A_full)[idx]\n",
    "    J = torch.autograd.functional.jacobian(f_node, x_in)\n",
    "    if J.ndim == 3:\n",
    "        J = J[:, idx, :]\n",
    "    jac_frob = torch.norm(J, p='fro').item()\n",
    "    delta_fd = FD_EPS * torch.randn_like(x_in[idx])\n",
    "    pred_change = J @ delta_fd\n",
    "    f0 = f_node(x_in).detach()\n",
    "    f1 = f_node(x_in + (delta_fd.unsqueeze(0) * (torch.arange(len(x_in))==idx).float().to(device)[:,None]))\n",
    "    actual_change = (f1 - f0).detach()\n",
    "    rel_err = (torch.norm(pred_change - actual_change)/ (torch.norm(actual_change)+1e-12)).item()\n",
    "    jac_vals.append(jac_frob); jac_errs.append(rel_err); labels.append(y_np[idx])\n",
    "\n",
    "print_stats(\"Jacobian Sensitivity\", \n",
    "            [jac_vals[i] for i in range(len(labels)) if labels[i]==0],\n",
    "            [jac_vals[i] for i in range(len(labels)) if labels[i]==1],\n",
    "            [jac_errs[i] for i in range(len(labels)) if labels[i]==0],\n",
    "            [jac_errs[i] for i in range(len(labels)) if labels[i]==1])\n",
    "\n",
    "# ------------- Metric 2: Local Lipschitz -------------\n",
    "lip_vals, lip_errs, labels = [], [], []\n",
    "for idx in selected:\n",
    "    x_in = perturbed_feats.clone().detach().requires_grad_(True)\n",
    "    def f_node(z): return model(z, A_full)[idx]\n",
    "    J = torch.autograd.functional.jacobian(f_node, x_in)\n",
    "    if J.ndim == 3: J = J[:, idx, :]\n",
    "    U, S, V = torch.linalg.svd(J, full_matrices=False)\n",
    "    sigma_max = S[0].item()\n",
    "    delta_fd = FD_EPS * torch.randn_like(x_in[idx])\n",
    "    pred_change = J @ delta_fd\n",
    "    f0 = f_node(x_in).detach()\n",
    "    f1 = f_node(x_in + (delta_fd.unsqueeze(0) * (torch.arange(len(x_in))==idx).float().to(device)[:,None]))\n",
    "    actual_change = (f1 - f0).detach()\n",
    "    rel_err = (torch.norm(pred_change - actual_change)/ (torch.norm(actual_change)+1e-12)).item()\n",
    "    lip_vals.append(sigma_max); lip_errs.append(rel_err); labels.append(y_np[idx])\n",
    "\n",
    "print_stats(\"Local Lipschitz\", \n",
    "            [lip_vals[i] for i in range(len(labels)) if labels[i]==0],\n",
    "            [lip_vals[i] for i in range(len(labels)) if labels[i]==1],\n",
    "            [lip_errs[i] for i in range(len(labels)) if labels[i]==0],\n",
    "            [lip_errs[i] for i in range(len(labels)) if labels[i]==1])\n",
    "\n",
    "# ------------- Metric 3: Hessian Proxy -------------\n",
    "hess_vals, hess_errs, labels = [], [], []\n",
    "for idx in selected:\n",
    "    x_in = perturbed_feats.clone().detach().requires_grad_(True)\n",
    "    logits = model(x_in, A_full)[idx]\n",
    "    pred_class = logits.argmax().item()\n",
    "    logp = F.log_softmax(logits, dim=0)[pred_class]\n",
    "    g = torch.autograd.grad(logp, x_in, retain_graph=False, create_graph=False)[0][idx]\n",
    "    lambda_proxy = g.norm().item()**2\n",
    "    rels = []\n",
    "    for _ in range(3):\n",
    "        delta = FD_EPS * torch.randn_like(x_in[idx])\n",
    "        gt_delta = (g * delta).sum().item()\n",
    "        logits_p = model(x_in.clone().detach().index_add(0, torch.tensor([idx], device=device), delta.unsqueeze(0)), A_full)[idx]\n",
    "        logp_p = F.log_softmax(logits_p, dim=0)[pred_class]\n",
    "        actual_second = (logp_p - logp).item() - gt_delta\n",
    "        pred_second = 0.5*(gt_delta**2)\n",
    "        rels.append(abs(pred_second-actual_second)/(abs(actual_second)+1e-12))\n",
    "    hess_vals.append(lambda_proxy); hess_errs.append(np.mean(rels)); labels.append(y_np[idx])\n",
    "\n",
    "print_stats(\"Hessian Curvature\", \n",
    "            [hess_vals[i] for i in range(len(labels)) if labels[i]==0],\n",
    "            [hess_vals[i] for i in range(len(labels)) if labels[i]==1],\n",
    "            [hess_errs[i] for i in range(len(labels)) if labels[i]==0],\n",
    "            [hess_errs[i] for i in range(len(labels)) if labels[i]==1])\n",
    "\n",
    "# ------------- Metric 4: Prediction Margin -------------\n",
    "margin_vals, margin_errs, labels = [], [], []\n",
    "for idx in selected:\n",
    "    logits = model(perturbed_feats, A_full)[idx]\n",
    "    pred_class = logits.argmax().item()\n",
    "    margin = logits[pred_class].item() - logits[[j for j in range(len(logits)) if j!=pred_class]].max().item()\n",
    "    delta = FD_EPS * torch.randn_like(perturbed_feats[idx])\n",
    "    logits_p = model(perturbed_feats.clone().detach().index_add(0, torch.tensor([idx], device=device), delta.unsqueeze(0)), A_full)[idx]\n",
    "    margin_p = logits_p[pred_class].item() - logits_p[[j for j in range(len(logits_p)) if j!=pred_class]].max().item()\n",
    "    rel_err = abs(margin-margin_p)/(abs(margin_p)+1e-12)\n",
    "    margin_vals.append(margin); margin_errs.append(rel_err); labels.append(y_np[idx])\n",
    "\n",
    "print_stats(\"Prediction Margin\", \n",
    "            [margin_vals[i] for i in range(len(labels)) if labels[i]==0],\n",
    "            [margin_vals[i] for i in range(len(labels)) if labels[i]==1],\n",
    "            [margin_errs[i] for i in range(len(labels)) if labels[i]==0],\n",
    "            [margin_errs[i] for i in range(len(labels)) if labels[i]==1])\n",
    "\n",
    "# ------------- Metric 5: ARR -------------\n",
    "def adversarial_radius(idx, growth=ARR_GROW1):\n",
    "    x0 = perturbed_feats.clone().detach()\n",
    "    y0 = model(x0, A_full)[idx].argmax().item()\n",
    "    d = torch.randn_like(x0[idx]); d /= d.norm()+1e-12\n",
    "    eps = ARR_INITIAL_EPS\n",
    "    while eps<ARR_MAX_EPS and model(x0.clone().detach().index_add(0, torch.tensor([idx], device=device), (eps*d).unsqueeze(0)), A_full)[idx].argmax().item()==y0:\n",
    "        eps*=growth\n",
    "    return eps\n",
    "\n",
    "arr_vals, arr_errs, labels = [], [], []\n",
    "for idx in selected:\n",
    "    r1 = adversarial_radius(idx, growth=ARR_GROW1)\n",
    "    r2 = adversarial_radius(idx, growth=ARR_GROW2)\n",
    "    rel = abs(r1-r2)/(abs(r2)+1e-12)\n",
    "    arr_vals.append(r1); arr_errs.append(rel); labels.append(y_np[idx])\n",
    "\n",
    "print_stats(\"Adversarial Robustness Radius\", \n",
    "            [arr_vals[i] for i in range(len(labels)) if labels[i]==0],\n",
    "            [arr_vals[i] for i in range(len(labels)) if labels[i]==1],\n",
    "            [arr_errs[i] for i in range(len(labels)) if labels[i]==0],\n",
    "            [arr_errs[i] for i in range(len(labels)) if labels[i]==1])\n",
    "\n",
    "# ------------- Metric 6: Stability under Noise -------------\n",
    "stab_vals, stab_errs, labels = [], [], []\n",
    "for idx in selected:\n",
    "    base = model(perturbed_feats, A_full)[idx]\n",
    "    diffs = []\n",
    "    for _ in range(STAB_SAMPLES):\n",
    "        noise = STAB_SIGMA*torch.randn_like(perturbed_feats[idx])\n",
    "        x_noisy = perturbed_feats.clone().detach()\n",
    "        x_noisy[idx]+=noise\n",
    "        f_noisy = model(x_noisy, A_full)[idx]\n",
    "        diffs.append(torch.norm(f_noisy-base).item())\n",
    "    val = np.mean(diffs)\n",
    "    re_vals = []\n",
    "    for _ in range(3):\n",
    "        diffs2 = []\n",
    "        for _ in range(STAB_SAMPLES):\n",
    "            noise = STAB_SIGMA*torch.randn_like(perturbed_feats[idx])\n",
    "            x_noisy = perturbed_feats.clone().detach()\n",
    "            x_noisy[idx]+=noise\n",
    "            f_noisy = model(x_noisy, A_full)[idx]\n",
    "            diffs2.append(torch.norm(f_noisy-base).item())\n",
    "        re_vals.append(np.mean(diffs2))\n",
    "    ref = np.mean(re_vals)\n",
    "    rel_err = abs(val-ref)/(abs(ref)+1e-12)\n",
    "    stab_vals.append(val); stab_errs.append(rel_err); labels.append(y_np[idx])\n",
    "\n",
    "print_stats(\"Stability under Noise\", \n",
    "            [stab_vals[i] for i in range(len(labels)) if labels[i]==0],\n",
    "            [stab_vals[i] for i in range(len(labels)) if labels[i]==1],\n",
    "            [stab_errs[i] for i in range(len(labels)) if labels[i]==0],\n",
    "            [stab_errs[i] for i in range(len(labels)) if labels[i]==1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa346fa-16b7-4c3a-aeea-8f1a96a9114a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
