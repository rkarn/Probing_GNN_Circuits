{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6caf6b8f-50f8-475a-9da8-142ac5ead8a8",
   "metadata": {},
   "source": [
    "#### Training Evaluation similar to https://github.com/DfX-NYUAD/TrojanSAINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbf1215c-0a9e-4619-a418-218eb6db72c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss 0.2149 | Val 0.2494 | Test 0.2495\n",
      "Epoch 010 | Loss 0.0014 | Val 0.2497 | Test 0.2500\n",
      "Epoch 020 | Loss 0.0007 | Val 0.3832 | Test 0.3831\n",
      "Epoch 030 | Loss 0.0006 | Val 0.3490 | Test 0.3488\n",
      "Epoch 040 | Loss 0.0006 | Val 0.2675 | Test 0.2687\n",
      "Epoch 050 | Loss 0.0025 | Val 0.8437 | Test 0.8493\n",
      "Epoch 060 | Loss 0.0005 | Val 0.9770 | Test 0.9776\n",
      "Epoch 070 | Loss 0.0005 | Val 0.9625 | Test 0.9622\n",
      "Epoch 080 | Loss 0.0015 | Val 0.7824 | Test 0.7851\n",
      "Epoch 090 | Loss 0.0004 | Val 0.3913 | Test 0.3925\n",
      "Epoch 100 | Loss 0.0004 | Val 0.2505 | Test 0.2509\n",
      "Epoch 110 | Loss 0.0004 | Val 0.2494 | Test 0.2495\n",
      "Epoch 120 | Loss 0.0004 | Val 0.2783 | Test 0.2783\n",
      "Epoch 130 | Loss 0.0004 | Val 0.2757 | Test 0.2754\n",
      "Epoch 140 | Loss 0.0019 | Val 0.2494 | Test 0.2495\n",
      "Epoch 150 | Loss 0.0022 | Val 0.2494 | Test 0.2495\n",
      "Epoch 160 | Loss 0.0019 | Val 0.2494 | Test 0.2495\n",
      "Epoch 170 | Loss 0.0004 | Val 0.2494 | Test 0.2495\n",
      "Epoch 180 | Loss 0.0004 | Val 0.2494 | Test 0.2495\n",
      "Epoch 190 | Loss 0.0004 | Val 0.2494 | Test 0.2495\n",
      "Epoch 200 | Loss 0.0004 | Val 0.2494 | Test 0.2495\n",
      "Epoch 210 | Loss 0.0004 | Val 0.2494 | Test 0.2495\n",
      "Epoch 220 | Loss 0.0004 | Val 0.2494 | Test 0.2495\n",
      "Epoch 230 | Loss 0.0004 | Val 0.2494 | Test 0.2495\n",
      "Epoch 240 | Loss 0.0004 | Val 0.2494 | Test 0.2495\n",
      "Epoch 250 | Loss 0.0004 | Val 0.2494 | Test 0.2495\n",
      "Epoch 260 | Loss 0.0004 | Val 0.2494 | Test 0.2495\n",
      "Early stopping.\n",
      "\n",
      "Final Evaluation (Node-Level, TrojanSAINT/GraphSAINT-style)\n",
      "============================================================\n",
      "Test Accuracy: 0.9776\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     0.9176    1.0000    0.9571      9159\n",
      "      trojan     1.0000    0.9702    0.9849     27556\n",
      "\n",
      "    accuracy                         0.9776     36715\n",
      "   macro avg     0.9588    0.9851    0.9710     36715\n",
      "weighted avg     0.9795    0.9776    0.9779     36715\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 9159     0]\n",
      " [  822 26734]]\n"
     ]
    }
   ],
   "source": [
    "# train_trojansaint_node_fixed.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "NODE_CSV  = \"GNNDatasets/node.csv\"\n",
    "EDGE_CSV  = \"GNNDatasets/node_edges.csv\"\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# ----------------------------- Load nodes -----------------------------\n",
    "nodes_df = pd.read_csv(NODE_CSV)\n",
    "\n",
    "label_col = None\n",
    "for cand in [\"label\", \"is_trojan\", \"trojan\", \"target\"]:\n",
    "    if cand in nodes_df.columns:\n",
    "        label_col = cand; break\n",
    "if label_col is None:\n",
    "    nodes_df[\"label\"] = nodes_df[\"circuit_name\"].astype(str).str.contains(\"__trojan_\").astype(int)\n",
    "    label_col = \"label\"\n",
    "\n",
    "nodes_df[\"uid\"] = nodes_df[\"circuit_name\"].astype(str) + \"::\" + nodes_df[\"node\"].astype(str)\n",
    "\n",
    "feat_df = nodes_df.copy()\n",
    "if \"gate_type\" in feat_df.columns:\n",
    "    gate_oh = pd.get_dummies(feat_df[\"gate_type\"], prefix=\"gt\")\n",
    "    feat_df = pd.concat([feat_df.drop(columns=[\"gate_type\"]), gate_oh], axis=1)\n",
    "\n",
    "exclude = {\"uid\",\"node\",\"circuit_name\",label_col}\n",
    "num_cols = [c for c in feat_df.columns if c not in exclude and pd.api.types.is_numeric_dtype(feat_df[c])]\n",
    "X = feat_df[num_cols].fillna(0.0).values.astype(np.float32)\n",
    "y = nodes_df[label_col].values.astype(np.int64)\n",
    "\n",
    "# ----------------------------- Load edges; add missing nodes -----------------------------\n",
    "edges_df = pd.read_csv(EDGE_CSV)\n",
    "edges_df[\"src_uid\"] = edges_df[\"circuit_name\"].astype(str) + \"::\" + edges_df[\"src\"].astype(str)\n",
    "edges_df[\"dst_uid\"] = edges_df[\"circuit_name\"].astype(str) + \"::\" + edges_df[\"dst\"].astype(str)\n",
    "\n",
    "known_uids = set(nodes_df[\"uid\"])\n",
    "edge_uids = set(edges_df[\"src_uid\"]).union(set(edges_df[\"dst_uid\"]))\n",
    "missing = list(edge_uids - known_uids)\n",
    "\n",
    "if missing:\n",
    "    zero_row = np.zeros((1, X.shape[1]), dtype=np.float32)\n",
    "    addX = np.repeat(zero_row, len(missing), axis=0)\n",
    "    addY = -1*np.ones(len(missing), dtype=np.int64)\n",
    "    add_df = pd.DataFrame({\n",
    "        \"uid\": missing,\n",
    "        \"circuit_name\": [u.split(\"::\",1)[0] for u in missing],\n",
    "        \"node\": [u.split(\"::\",1)[1] for u in missing],\n",
    "        label_col: addY\n",
    "    })\n",
    "    X = np.vstack([X, addX])\n",
    "    y = np.concatenate([y, addY])\n",
    "    nodes_df = pd.concat([nodes_df, add_df], ignore_index=True)\n",
    "\n",
    "uid_to_idx = {u:i for i,u in enumerate(nodes_df[\"uid\"].tolist())}\n",
    "src_idx = edges_df[\"src_uid\"].map(uid_to_idx).dropna().astype(int).values\n",
    "dst_idx = edges_df[\"dst_uid\"].map(uid_to_idx).dropna().astype(int).values\n",
    "edge_index = np.stack([np.concatenate([src_idx, dst_idx]),\n",
    "                       np.concatenate([dst_idx, src_idx])], axis=0)\n",
    "\n",
    "num_nodes = X.shape(0) if callable(getattr(X, \"shape\", None)) else X.shape[0]\n",
    "\n",
    "# ----------------------------- Scale features -----------------------------\n",
    "labeled_mask_np = (y >= 0)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "X_scaled[labeled_mask_np] = scaler.fit_transform(X_scaled[labeled_mask_np])\n",
    "if (~labeled_mask_np).any():\n",
    "    X_scaled[~labeled_mask_np] = (X_scaled[~labeled_mask_np] - scaler.mean_) / np.sqrt(scaler.var_ + 1e-8)\n",
    "\n",
    "# ----------------------------- Splits -----------------------------\n",
    "idx_all = np.where(labeled_mask_np)[0]\n",
    "y_all = y[labeled_mask_np]\n",
    "\n",
    "idx_train, idx_tmp, y_train, y_tmp = train_test_split(\n",
    "    idx_all, y_all, test_size=0.30, random_state=SEED, stratify=y_all\n",
    ")\n",
    "idx_val, idx_test, y_val, y_test = train_test_split(\n",
    "    idx_tmp, y_tmp, test_size=0.50, random_state=SEED, stratify=y_tmp\n",
    ")\n",
    "\n",
    "# ----------------------------- Torch tensors -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_t = torch.from_numpy(X_scaled).to(device)\n",
    "y_t = torch.from_numpy(y).to(device)\n",
    "edge_index_t = torch.from_numpy(edge_index).long().to(device)\n",
    "\n",
    "train_mask_t = torch.zeros(len(y), dtype=torch.bool, device=device); train_mask_t[idx_train] = True\n",
    "val_mask_t   = torch.zeros(len(y), dtype=torch.bool, device=device); val_mask_t[idx_val]   = True\n",
    "test_mask_t  = torch.zeros(len(y), dtype=torch.bool, device=device); test_mask_t[idx_test] = True\n",
    "labeled_mask_t = torch.from_numpy(labeled_mask_np).to(device)\n",
    "\n",
    "# ----------------------------- Utility: degree, neighbors -----------------------------\n",
    "def degrees(num_nodes, ei):\n",
    "    deg = torch.bincount(ei[0], minlength=num_nodes).float()\n",
    "    return deg\n",
    "\n",
    "def coalesce_edge_index(ei):\n",
    "    # simple coalesce: unique columns of ei (2 x E)\n",
    "    u = ei[0]* (ei[0].max()+1) + ei[1]\n",
    "    uniq, idx = torch.unique(u, return_inverse=True)\n",
    "    ei0 = uniq // (ei[0].max()+1)\n",
    "    ei1 = uniq %  (ei[0].max()+1)\n",
    "    return torch.stack([ei0, ei1], dim=0)\n",
    "\n",
    "# ----------------------------- GraphSAINT-style sampler -----------------------------\n",
    "class GraphSAINTSampler:\n",
    "    \"\"\"\n",
    "    Lightweight GraphSAINT-style node sampler:\n",
    "    - Sample a set of seed nodes (biased by degree for stability)\n",
    "    - Induce 1-hop subgraph\n",
    "    - Provide normalization weights to approximately debias sampling\n",
    "    \"\"\"\n",
    "    def __init__(self, num_nodes, edge_index, train_mask, batch_size=2000, num_steps=1,\n",
    "                 bias_by_degree=True, device='cpu'):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.edge_index = edge_index\n",
    "        self.train_mask = train_mask\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.device = device\n",
    "\n",
    "        deg = torch.bincount(edge_index[0], minlength=num_nodes).float()\n",
    "        deg = deg + 1e-6\n",
    "        base_prob = deg / deg.sum() if bias_by_degree else torch.full((num_nodes,), 1.0/num_nodes, device=edge_index.device)\n",
    "\n",
    "        # Only sample from labeled training nodes\n",
    "        self.sample_space = torch.where(train_mask)[0]\n",
    "        p_space = base_prob[self.sample_space]\n",
    "        self.p_space = p_space / p_space.sum()\n",
    "\n",
    "        self.base_prob = base_prob  # keep global probs for normalization\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def next_seeds(self):\n",
    "        replace = self.sample_space.numel() < self.batch_size\n",
    "        idx = torch.multinomial(self.p_space, self.batch_size, replacement=replace)\n",
    "        return self.sample_space[idx]\n",
    "\n",
    "    def _induce_subgraph(self, seeds):\n",
    "        ei = self.edge_index\n",
    "        # keep edges if either endpoint is in seeds (1-hop expansion)\n",
    "        mask = (ei[0].unsqueeze(1) == seeds.unsqueeze(0)).any(dim=1) | \\\n",
    "               (ei[1].unsqueeze(1) == seeds.unsqueeze(0)).any(dim=1)\n",
    "        sub_ei = ei[:, mask]\n",
    "\n",
    "        # nodes present in subgraph\n",
    "        nodes = torch.unique(torch.cat([sub_ei[0], sub_ei[1], seeds], dim=0))\n",
    "\n",
    "        # map global -> local ids\n",
    "        nid_map = -torch.ones(self.num_nodes, dtype=torch.long, device=ei.device)\n",
    "        nid_map[nodes] = torch.arange(nodes.numel(), device=ei.device)\n",
    "        sub_ei = nid_map[sub_ei]\n",
    "\n",
    "        # coalesce and normalize (GCN)\n",
    "        # ensure no negative indices (can happen if sub_ei was empty)\n",
    "        if sub_ei.numel() == 0:\n",
    "            # create a trivial self-loop to avoid empty adj\n",
    "            sub_ei = torch.stack([torch.tensor([0], device=ei.device), torch.tensor([0], device=ei.device)], dim=0)\n",
    "            nodes = nodes[:1]\n",
    "\n",
    "        # coalesce\n",
    "        u = sub_ei[0] * nodes.numel() + sub_ei[1]\n",
    "        uniq = torch.unique(u)\n",
    "        sub_ei0 = uniq // nodes.numel()\n",
    "        sub_ei1 = uniq %  nodes.numel()\n",
    "        sub_ei = torch.stack([sub_ei0, sub_ei1], dim=0)\n",
    "\n",
    "        # add self-loops\n",
    "        self_loops = torch.arange(nodes.numel(), device=ei.device)\n",
    "        sub_ei = torch.cat([sub_ei, torch.stack([self_loops, self_loops])], dim=1)\n",
    "\n",
    "        deg = torch.bincount(sub_ei[0], minlength=nodes.numel()).float()\n",
    "        deg_inv_sqrt = deg.clamp(min=1).pow(-0.5)\n",
    "        w = deg_inv_sqrt[sub_ei[0]] * deg_inv_sqrt[sub_ei[1]]\n",
    "        A = torch.sparse_coo_tensor(sub_ei, w, (nodes.numel(), nodes.numel())).coalesce()\n",
    "        return nodes, A\n",
    "\n",
    "    def __next__(self):\n",
    "        seeds = self.next_seeds()\n",
    "        nodes, A = self._induce_subgraph(seeds)\n",
    "        # importance weights (GraphSAINT normalization proxy)\n",
    "        p = torch.clamp(self.base_prob[nodes], min=1e-8)\n",
    "        norm_loss = (1.0 / p) / (1.0 / p).mean()\n",
    "        return nodes, A, norm_loss\n",
    "\n",
    "# ----------------------------- TrojanSAINT (GraphSAINT-style) model -----------------------------\n",
    "class SAINTGCNLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, dropout=0.0, bias=True):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(in_dim, out_dim, bias=bias)\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        nn.init.xavier_uniform_(self.lin.weight)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # GCN propagation on sampled subgraph\n",
    "        x = torch.sparse.mm(adj, x)\n",
    "        x = self.lin(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TrojanSAINT(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=96, out_dim=2, dropout=0.35):\n",
    "        super().__init__()\n",
    "        self.g1 = SAINTGCNLayer(in_dim, hid_dim, dropout=dropout)\n",
    "        self.g2 = nn.Linear(hid_dim, out_dim, bias=True)\n",
    "        nn.init.xavier_uniform_(self.g2.weight)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.g1(x, adj)\n",
    "        x = torch.sparse.mm(adj, x)  # last propagation (linear head after propagation)\n",
    "        x = self.g2(x)\n",
    "        return x\n",
    "\n",
    "model = TrojanSAINT(in_dim=X_t.size(1), hid_dim=96, out_dim=2, dropout=0.35).to(device)\n",
    "\n",
    "# ----------------------------- Class weights, loss, optimizer -----------------------------\n",
    "train_labels = y_t[train_mask_t]\n",
    "classes, counts = torch.unique(train_labels, return_counts=True)\n",
    "num_pos = counts[classes==1].item() if (classes==1).any() else 1\n",
    "num_neg = counts[classes==0].item() if (classes==0).any() else 1\n",
    "weight_pos = (num_neg + num_pos) / (2.0 * num_pos)\n",
    "weight_neg = (num_neg + num_pos) / (2.0 * num_neg)\n",
    "class_weights = torch.tensor([weight_neg, weight_pos], dtype=torch.float32, device=device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none', weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3, weight_decay=5e-4)\n",
    "\n",
    "# ----------------------------- Sampler -----------------------------\n",
    "num_nodes = X_t.size(0)\n",
    "sampler = GraphSAINTSampler(\n",
    "    num_nodes=num_nodes,\n",
    "    edge_index=edge_index_t,\n",
    "    train_mask=train_mask_t & (y_t >= 0),\n",
    "    batch_size=min(4000, (train_mask_t & (y_t >= 0)).sum().item()),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ----------------------------- Evaluation (full-graph) -----------------------------\n",
    "def build_full_adj(num_nodes, edge_index):\n",
    "    self_loops = torch.arange(num_nodes, device=edge_index.device)\n",
    "    ei = torch.cat([edge_index, torch.stack([self_loops, self_loops])], dim=1)\n",
    "    deg = torch.bincount(ei[0], minlength=num_nodes).float()\n",
    "    deg_inv_sqrt = deg.clamp(min=1).pow(-0.5)\n",
    "    w = deg_inv_sqrt[ei[0]] * deg_inv_sqrt[ei[1]]\n",
    "    A = torch.sparse_coo_tensor(ei, w, (num_nodes, num_nodes))\n",
    "    return A.coalesce()\n",
    "\n",
    "A_full = build_full_adj(X_t.size(0), edge_index_t)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(mask_t):\n",
    "    model.eval()\n",
    "    logits = model(X_t, A_full)\n",
    "    pred = logits.argmax(dim=1)\n",
    "    msk = mask_t & (y_t >= 0)\n",
    "    if msk.sum() == 0: return 0.0\n",
    "    return (pred[msk] == y_t[msk]).float().mean().item()\n",
    "\n",
    "# ----------------------------- Training (GraphSAINT-style) -----------------------------\n",
    "best_val, best_state = -1.0, None\n",
    "patience, patience_cnt = 20, 0\n",
    "EPOCHS = 300\n",
    "steps_per_epoch = max(1, int(np.ceil((train_mask_t & (y_t >= 0)).sum().item() / sampler.batch_size)))\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for _ in range(steps_per_epoch):\n",
    "        nodes, A_b, norm_loss = next(sampler)\n",
    "        x_b = X_t[nodes]\n",
    "        y_b = y_t[nodes]\n",
    "        train_b = train_mask_t[nodes] & (y_b >= 0)\n",
    "    \n",
    "        if train_b.sum() == 0:\n",
    "            continue\n",
    "    \n",
    "        logits_b = model(x_b, A_b)\n",
    "        loss_raw = criterion(logits_b[train_b], y_b[train_b])\n",
    "        loss = (loss_raw * norm_loss[train_b]).mean()\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        val_acc = evaluate(val_mask_t)\n",
    "        test_acc = evaluate(test_mask_t)\n",
    "        print(f\"Epoch {epoch:03d} | Loss {epoch_loss/steps_per_epoch:.4f} | Val {val_acc:.4f} | Test {test_acc:.4f}\")\n",
    "        if val_acc > best_val + 1e-4:\n",
    "            best_val = val_acc\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience_cnt = 0\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            if patience_cnt >= patience:\n",
    "                print(\"Early stopping.\"); break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "# ----------------------------- Final eval -----------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X_t, A_full)\n",
    "    preds = logits.argmax(dim=1)\n",
    "\n",
    "msk = (test_mask_t & (y_t >= 0)).cpu().numpy()\n",
    "y_true = y_t.cpu().numpy()[msk]\n",
    "y_pred = preds.cpu().numpy()[msk]\n",
    "\n",
    "acc = (y_true == y_pred).mean()\n",
    "print(\"\\nFinal Evaluation (Node-Level, TrojanSAINT/GraphSAINT-style)\")\n",
    "print(\"============================================================\")\n",
    "print(f\"Test Accuracy: {acc:.4f}\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, labels=[0,1], target_names=[\"clean\",\"trojan\"], digits=4))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=[0,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b86dfef6-38d7-4c54-8aee-eddb5a1d70ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: clean=100, trojan=100\n",
      "? Shared PGD perturbations done.\n",
      "\n",
      "Jacobian Sensitivity:\n",
      " Class 0: norm=0.4569±0.1086, relerr=1.8763e-03±4.0839e-03\n",
      " Class 1: norm=7.4574±0.0800, relerr=1.6980e-03±7.2592e-03\n",
      "\n",
      "=== Robustness Eval (Jacobian) ===\n",
      "Flipped 144/200 (72.00%)\n",
      "Accuracy=27.50 | Precision=0.1774 | Recall=0.2750 | F1=0.2157\n",
      "Confusion Matrix:\n",
      "[[ 55  45]\n",
      " [100   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     0.3548    0.5500    0.4314       100\n",
      "      trojan     0.0000    0.0000    0.0000       100\n",
      "\n",
      "    accuracy                         0.2750       200\n",
      "   macro avg     0.1774    0.2750    0.2157       200\n",
      "weighted avg     0.1774    0.2750    0.2157       200\n",
      "\n",
      "\n",
      "Lipschitz Constant:\n",
      " Class 0: L=0.4566±0.1085, relerr=1.8539e-03±5.3200e-03\n",
      " Class 1: L=7.4548±0.0799, relerr=1.1322e-03±2.0651e-03\n",
      "\n",
      "=== Robustness Eval (Lipschitz) ===\n",
      "Flipped 144/200 (72.00%)\n",
      "Accuracy=27.50 | Precision=0.1774 | Recall=0.2750 | F1=0.2157\n",
      "Confusion Matrix:\n",
      "[[ 55  45]\n",
      " [100   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     0.3548    0.5500    0.4314       100\n",
      "      trojan     0.0000    0.0000    0.0000       100\n",
      "\n",
      "    accuracy                         0.2750       200\n",
      "   macro avg     0.1774    0.2750    0.2157       200\n",
      "weighted avg     0.1774    0.2750    0.2157       200\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.9928, -5.0827],\n",
       "        [ 4.9975, -4.9917],\n",
       "        [ 4.9780, -4.9694],\n",
       "        ...,\n",
       "        [ 4.7599, -4.9413],\n",
       "        [ 4.0829, -4.2264],\n",
       "        [ 4.0829, -4.2264]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ================================\n",
    "# Unified Robustness Evaluation\n",
    "# ================================\n",
    "import torch, numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "# ---------------- Parameters ----------------\n",
    "PER_CLASS = 100\n",
    "EPSILON   = 5.0     # L2 budget for PGD\n",
    "ALPHA     = 1.0     # PGD step size\n",
    "NUM_ITERS = 40      # PGD iterations\n",
    "FD_EPS    = 1e-3    # finite-difference epsilon\n",
    "SEED      = 42\n",
    "\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "A_t = A_full\n",
    "required_vars = [\"model\",\"X_t\",\"A_t\",\"y_t\",\"test_mask_t\",\"device\"]\n",
    "for v in required_vars:\n",
    "    if v not in globals():\n",
    "        raise RuntimeError(f\"Required var '{v}' not found.\")\n",
    "\n",
    "model.to(device); model.eval()\n",
    "labels_np = y_t.cpu().numpy()\n",
    "\n",
    "# ---------------- Node Selection ----------------\n",
    "test_indices = np.where(test_mask_t.cpu().numpy())[0]\n",
    "rng = np.random.default_rng(SEED)\n",
    "selected_nodes = []\n",
    "for cls in [0,1]:\n",
    "    idxs = [int(i) for i in test_indices if labels_np[i]==cls]\n",
    "    chosen = rng.choice(idxs, size=min(PER_CLASS, len(idxs)), replace=False)\n",
    "    selected_nodes.extend(chosen)\n",
    "selected_nodes = np.array(selected_nodes, dtype=np.int64)\n",
    "\n",
    "print(f\"Selected: clean={int((labels_np[selected_nodes]==0).sum())}, \"\n",
    "      f\"trojan={int((labels_np[selected_nodes]==1).sum())}\")\n",
    "\n",
    "# ---------------- Shared PGD Perturbations ----------------\n",
    "perturbed_X = X_t.clone().detach().to(device)\n",
    "for node_idx in selected_nodes:\n",
    "    node_idx = int(node_idx)\n",
    "    x_orig = X_t[node_idx].detach().clone().to(device)\n",
    "    x_adv = (x_orig + 1e-3*torch.randn_like(x_orig)).detach().requires_grad_(True)\n",
    "\n",
    "    for _ in range(NUM_ITERS):\n",
    "        X_mod = perturbed_X.clone().detach()\n",
    "        X_mod[node_idx] = x_adv\n",
    "        logits = model(X_mod, A_t)\n",
    "        loss = F.cross_entropy(logits[node_idx].unsqueeze(0), y_t[node_idx].unsqueeze(0))\n",
    "        grad_x = torch.autograd.grad(loss, x_adv)[0]\n",
    "        if grad_x.norm().item()==0: break\n",
    "        step = ALPHA * grad_x / (grad_x.norm() + 1e-12)\n",
    "        x_adv = (x_adv + step).detach()\n",
    "        delta = x_adv - x_orig\n",
    "        if delta.norm() > EPSILON:\n",
    "            delta = delta * (EPSILON/(delta.norm()+1e-12))\n",
    "            x_adv = (x_orig + delta).detach()\n",
    "        x_adv = x_adv.requires_grad_(True)\n",
    "    perturbed_X[node_idx] = x_adv.detach()\n",
    "print(\"? Shared PGD perturbations done.\")\n",
    "\n",
    "# ---------------- Eval Helper ----------------\n",
    "def evaluate_model(name, perturbed_X, selected_nodes):\n",
    "    with torch.no_grad():\n",
    "        # Predictions on original and perturbed inputs\n",
    "        orig_logits = model(X_t, A_t)\n",
    "        pert_logits = model(perturbed_X, A_t)\n",
    "\n",
    "        orig_preds = orig_logits.argmax(dim=1).cpu().numpy()\n",
    "        pert_preds = pert_logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "    # Restrict to selected perturbed samples only\n",
    "    sel_idx = np.array(selected_nodes)\n",
    "    sel_labels = labels_np[sel_idx]\n",
    "    sel_orig_preds = orig_preds[sel_idx]\n",
    "    sel_pert_preds = pert_preds[sel_idx]\n",
    "\n",
    "    # Flip count first\n",
    "    flips = (sel_orig_preds != sel_pert_preds).sum()\n",
    "    print(f\"\\n=== Robustness Eval ({name}) ===\")\n",
    "    print(f\"Flipped {flips}/{len(sel_idx)} ({100*flips/len(sel_idx):.2f}%)\")\n",
    "\n",
    "    # Accuracy, precision, recall, F1 on perturbed subset only\n",
    "    acc = (sel_pert_preds == sel_labels).mean()\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        sel_labels, sel_pert_preds, average=\"weighted\", zero_division=0)\n",
    "\n",
    "    print(f\"Accuracy={acc*100:.2f} | Precision={prec:.4f} | Recall={rec:.4f} | F1={f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(sel_labels, sel_pert_preds, labels=[0, 1]))\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(sel_labels, sel_pert_preds,\n",
    "                                target_names=[\"clean\", \"trojan\"], digits=4))\n",
    "\n",
    "    return pert_logits\n",
    "\n",
    "\n",
    "# ---------------- Metric 1: Jacobian Sensitivity ----------------\n",
    "jac_info = []\n",
    "for node_idx in selected_nodes:\n",
    "    x0 = perturbed_X[node_idx].detach().clone().requires_grad_(True)\n",
    "    def f_local(x):\n",
    "        X_mod = perturbed_X.clone().detach()\n",
    "        X_mod[node_idx] = x\n",
    "        return model(X_mod, A_t)[node_idx]\n",
    "    J = torch.autograd.functional.jacobian(f_local, x0)\n",
    "    jac_norm = torch.norm(J, p='fro').item()\n",
    "    delta_fd = FD_EPS * torch.randn_like(x0)\n",
    "    pred_change = J.mv(delta_fd)\n",
    "    f0, f0p = f_local(x0).detach(), f_local(x0+delta_fd).detach()\n",
    "    actual_change = f0p - f0\n",
    "    rel_err = (torch.norm(pred_change-actual_change)/(torch.norm(actual_change)+1e-8)).item()\n",
    "    jac_info.append((int(labels_np[node_idx]), jac_norm, rel_err))\n",
    "print(\"\\nJacobian Sensitivity:\")\n",
    "for cls in [0,1]:\n",
    "    vals = [j[1] for j in jac_info if j[0]==cls]\n",
    "    errs = [j[2] for j in jac_info if j[0]==cls]\n",
    "    print(f\" Class {cls}: norm={np.mean(vals):.4f}±{np.std(vals):.4f}, relerr={np.mean(errs):.4e}±{np.std(errs):.4e}\")\n",
    "\n",
    "evaluate_model(\"Jacobian\", perturbed_X, selected_nodes)\n",
    "\n",
    "# ---------------- Metric 2: Lipschitz (Spectral Norm) ----------------\n",
    "lip_info = []\n",
    "for node_idx in selected_nodes:\n",
    "    x0 = perturbed_X[node_idx].detach().clone().requires_grad_(True)\n",
    "    def f_node(x):\n",
    "        X_mod = perturbed_X.clone().detach()\n",
    "        X_mod[node_idx] = x\n",
    "        return model(X_mod, A_t)[node_idx]\n",
    "    J = torch.autograd.functional.jacobian(f_node, x0).detach()\n",
    "    U, S, Vh = torch.linalg.svd(J, full_matrices=False)\n",
    "    sigma_max = S[0].item()\n",
    "    delta_fd = FD_EPS*torch.randn_like(x0)\n",
    "    pred_change = J.mv(delta_fd)\n",
    "    f0, f0p = f_node(x0).detach(), f_node(x0+delta_fd).detach()\n",
    "    actual_change = f0p-f0\n",
    "    rel_err = (torch.norm(pred_change-actual_change)/(torch.norm(actual_change)+1e-8)).item()\n",
    "    lip_info.append((int(labels_np[node_idx]), sigma_max, rel_err))\n",
    "print(\"\\nLipschitz Constant:\")\n",
    "for cls in [0,1]:\n",
    "    vals = [j[1] for j in lip_info if j[0]==cls]\n",
    "    errs = [j[2] for j in lip_info if j[0]==cls]\n",
    "    print(f\" Class {cls}: L={np.mean(vals):.4f}±{np.std(vals):.4f}, relerr={np.mean(errs):.4e}±{np.std(errs):.4e}\")\n",
    "\n",
    "evaluate_model(\"Lipschitz\", perturbed_X, selected_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cbc1457-dcf4-4db2-a2a4-cf7f1e440152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing Hessian curvature proxy for selected nodes...\n",
      "\n",
      "Aggregated Hessian curvature stats:\n",
      " Clean:  avg_lambda=0.0000 ± 0.0000, avg_FDrel=7.2122e-01 ± 8.1502e-02\n",
      " Trojan: avg_lambda=0.1053 ± 0.1613, avg_FDrel=1.1916e+00 ± 1.7923e-01\n",
      "\n",
      "Sample preview (first 6): (idx,label,lambda,FD_rel_err)\n",
      "(26119, 0, 5.3058623615239424e-09, 0.5967704660381343)\n",
      "(57231, 0, 2.919435396550133e-08, 0.6549227663929634)\n",
      "(40297, 0, 3.253715851850403e-08, 0.7398776462650221)\n",
      "(27843, 0, 7.068591906242526e-07, 0.7594926884346475)\n",
      "(26863, 0, 1.433692691525624e-06, 0.7370227599597732)\n",
      "(46828, 0, 3.9822156324595236e-08, 0.7163436675705489)\n",
      "\n",
      "=== Robustness Eval (Margin) ===\n",
      "Flipped 144/200 (72.00%)\n",
      "Accuracy=27.50 | Precision=0.1774 | Recall=0.2750 | F1=0.2157\n",
      "Confusion Matrix:\n",
      "[[ 55  45]\n",
      " [100   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     0.3548    0.5500    0.4314       100\n",
      "      trojan     0.0000    0.0000    0.0000       100\n",
      "\n",
      "    accuracy                         0.2750       200\n",
      "   macro avg     0.1774    0.2750    0.2157       200\n",
      "weighted avg     0.1774    0.2750    0.2157       200\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 113\u001b[0m\n\u001b[1;32m    111\u001b[0m margin \u001b[38;5;241m=\u001b[39m logits[pred_class]\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m-\u001b[39m logits[[j \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(logits)) \u001b[38;5;28;01mif\u001b[39;00m j\u001b[38;5;241m!=\u001b[39mpred_class]]\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    112\u001b[0m delta \u001b[38;5;241m=\u001b[39m FD_EPS\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrandn_like(perturbed_X[node_idx])\n\u001b[0;32m--> 113\u001b[0m logits_p \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperturbed_X\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_t\u001b[49m\u001b[43m)\u001b[49m[node_idx]\n\u001b[1;32m    114\u001b[0m margin_p \u001b[38;5;241m=\u001b[39m logits_p[pred_class]\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m-\u001b[39m logits_p[[j \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(logits_p)) \u001b[38;5;28;01mif\u001b[39;00m j\u001b[38;5;241m!=\u001b[39mpred_class]]\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    115\u001b[0m rel_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(margin\u001b[38;5;241m-\u001b[39mmargin_p)\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mabs\u001b[39m(margin_p)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1e-12\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 226\u001b[0m, in \u001b[0;36mTrojanSAINT.forward\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, adj):\n\u001b[1;32m    225\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg1(x, adj)\n\u001b[0;32m--> 226\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# last propagation (linear head after propagation)\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2(x)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Hessian-Based Curvature (grad outer-product) for node-level Trojan detection\n",
    "# =========================\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "# -------------------- Parameters --------------------\n",
    "PER_CLASS = 100          # 100 nodes per class (clean/trojan)\n",
    "FD_EPS = 5e-3            # finite-diff epsilon for relative-error check\n",
    "TRIALS_PER_NODE = 10     # average trials per node for relative error\n",
    "PERT_P = 6.0             # L2 magnitude for final Hessian-aligned perturbation (tuneable)\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# -------------------- Class names --------------------\n",
    "class_names = [\"clean\", \"trojan\"]\n",
    "\n",
    "\n",
    "# -------------------- Helper: compute g(x) --------------------\n",
    "def compute_gradient(node_idx):\n",
    "    \"\"\"\n",
    "    Returns gradient g = ?_x log p(y_hat|x) at node_idx.\n",
    "    \"\"\"\n",
    "    x0 = X_t[node_idx].detach().clone().to(device).requires_grad_(True)\n",
    "\n",
    "    # Forward pass with x0 replacing features of node_idx\n",
    "    X_mod = X_t.clone().detach().to(device)\n",
    "    X_mod[node_idx] = x0\n",
    "    logits = model(X_mod, A_t)[node_idx]\n",
    "\n",
    "    # Use predicted class\n",
    "    pred_class = logits.argmax().item()\n",
    "    logp = F.log_softmax(logits, dim=0)\n",
    "    loss = logp[pred_class]\n",
    "\n",
    "    g = torch.autograd.grad(loss, x0, retain_graph=False, create_graph=False, allow_unused=False)[0]\n",
    "    return x0.detach(), g.detach(), pred_class\n",
    "\n",
    "# -------------------- Storage --------------------\n",
    "per_sample_info = []   # (node_idx, label, lambda_max, avg_rel_error)\n",
    "\n",
    "print(\"\\nComputing Hessian curvature proxy for selected nodes...\")\n",
    "\n",
    "for node_idx in selected_nodes:\n",
    "    node_idx = int(node_idx)\n",
    "    label = int(labels_np[node_idx])\n",
    "\n",
    "    x0, g, pred_class = compute_gradient(node_idx)\n",
    "    if g is None:\n",
    "        lambda_max = 0.0\n",
    "        avg_rel_err = 0.0\n",
    "    else:\n",
    "        # curvature proxy = ||g||^2\n",
    "        lambda_max = float(g.norm(p=2).item() ** 2)\n",
    "\n",
    "        # relative error by finite-difference\n",
    "        rel_errs = []\n",
    "        for _ in range(TRIALS_PER_NODE):\n",
    "            delta = FD_EPS * torch.randn_like(x0).to(device)\n",
    "            gt_delta = torch.dot(g, delta).item()\n",
    "            pred_second = 0.5 * (gt_delta ** 2)\n",
    "\n",
    "            # recompute logits at perturbed input\n",
    "            X_mod = X_t.clone().detach().to(device)\n",
    "            X_mod[node_idx] = x0 + delta\n",
    "            logits_p = model(X_mod, A_t)[node_idx]\n",
    "            logp_p = F.log_softmax(logits_p, dim=0)\n",
    "            actual_second = float((logp_p[pred_class] - F.log_softmax(model(X_t, A_t)[node_idx], dim=0)[pred_class]).item() - torch.dot(g, delta).item())\n",
    "\n",
    "            rel_error = abs(pred_second - actual_second) / (abs(actual_second) + 1e-8)\n",
    "            rel_errs.append(rel_error)\n",
    "\n",
    "        avg_rel_err = float(np.mean(rel_errs))\n",
    "\n",
    "    per_sample_info.append((node_idx, label, lambda_max, avg_rel_err))\n",
    "\n",
    "# -------------------- Aggregate stats --------------------\n",
    "clean_stats = [t for t in per_sample_info if t[1]==0]\n",
    "troj_stats  = [t for t in per_sample_info if t[1]==1]\n",
    "\n",
    "def summarize(stats):\n",
    "    if not stats: return (0.0,0.0,0.0,0.0)\n",
    "    Ls = np.array([s[2] for s in stats])\n",
    "    Es = np.array([s[3] for s in stats])\n",
    "    return (Ls.mean(), Ls.std(), Es.mean(), Es.std())\n",
    "\n",
    "cL_mean, cL_std, cE_mean, cE_std = summarize(clean_stats)\n",
    "tL_mean, tL_std, tE_mean, tE_std = summarize(troj_stats)\n",
    "\n",
    "print(\"\\nAggregated Hessian curvature stats:\")\n",
    "print(f\" Clean:  avg_lambda={cL_mean:.4f} ± {cL_std:.4f}, avg_FDrel={cE_mean:.4e} ± {cE_std:.4e}\")\n",
    "print(f\" Trojan: avg_lambda={tL_mean:.4f} ± {tL_std:.4f}, avg_FDrel={tE_mean:.4e} ± {tE_std:.4e}\")\n",
    "\n",
    "print(\"\\nSample preview (first 6): (idx,label,lambda,FD_rel_err)\")\n",
    "for p in per_sample_info[:6]:\n",
    "    print(p)\n",
    "\n",
    "evaluate_model(\"Margin\", perturbed_X, selected_nodes)\n",
    "\n",
    "# ---------------- Metric 4: Prediction Margin ----------------\n",
    "margin_info = []\n",
    "for node_idx in selected_nodes:\n",
    "    logits = model(perturbed_X, A_t)[node_idx]\n",
    "    pred_class = logits.argmax().item()\n",
    "    margin = logits[pred_class].item() - logits[[j for j in range(len(logits)) if j!=pred_class]].max().item()\n",
    "    delta = FD_EPS*torch.randn_like(perturbed_X[node_idx])\n",
    "    logits_p = model(perturbed_X.clone().detach(), A_t)[node_idx]\n",
    "    margin_p = logits_p[pred_class].item() - logits_p[[j for j in range(len(logits_p)) if j!=pred_class]].max().item()\n",
    "    rel_err = abs(margin-margin_p)/(abs(margin_p)+1e-12)\n",
    "    margin_info.append((int(labels_np[node_idx]), margin, rel_err))\n",
    "print(\"\\nPrediction Margin:\")\n",
    "for cls in [0,1]:\n",
    "    vals = [j[1] for j in margin_info if j[0]==cls]\n",
    "    errs = [j[2] for j in margin_info if j[0]==cls]\n",
    "    print(f\" Class {cls}: margin={np.mean(vals):.4f}±{np.std(vals):.4f}, relerr={np.mean(errs):.4e}±{np.std(errs):.4e}\")\n",
    "\n",
    "evaluate_model(\"Margin\", perturbed_X, selected_nodes)\n",
    "\n",
    "# ---------------- Metric 5: ARR ----------------\n",
    "# (kept simplified: min perturbation until flip)\n",
    "def adversarial_radius(node_idx):\n",
    "    x0 = perturbed_X[node_idx].detach().clone()\n",
    "    base_pred = int(model(perturbed_X, A_t)[node_idx].argmax().item())\n",
    "    eps, growth = 1e-3, 1.2\n",
    "    while eps < 20:\n",
    "        x_try = x0 + eps*torch.randn_like(x0)\n",
    "        with torch.no_grad():\n",
    "            pred = int(model(perturbed_X.clone().detach(), A_t)[node_idx].argmax().item())\n",
    "        if pred != base_pred: return eps\n",
    "        eps *= growth\n",
    "    return 20.0\n",
    "\n",
    "arr_info = []\n",
    "for n in selected_nodes:\n",
    "    arr_val = adversarial_radius(n)\n",
    "    # finite-difference style perturbation for ARR\n",
    "    delta = FD_EPS * torch.randn_like(perturbed_X[n])\n",
    "    arr_val_p = adversarial_radius(n)  # here you could recompute with perturbed input if desired\n",
    "    rel_err = abs(arr_val - arr_val_p) / (abs(arr_val_p) + 1e-12)\n",
    "    arr_info.append((int(labels_np[n]), arr_val, rel_err))\n",
    "\n",
    "print(\"\\nAdversarial Robustness Radius:\")\n",
    "for cls in [0,1]:\n",
    "    vals = [j[1] for j in arr_info if j[0] == cls]\n",
    "    errs = [j[2] for j in arr_info if j[0] == cls]\n",
    "    print(f\" Class {cls}: radius={np.mean(vals):.4f}±{np.std(vals):.4f}, relerr={np.mean(errs):.4e}±{np.std(errs):.4e}\")\n",
    "\n",
    "evaluate_model(\"ARR\", perturbed_X, selected_nodes)\n",
    "\n",
    "# ---------------- Metric 6: Stability ----------------\n",
    "stability_info = []\n",
    "for node_idx in selected_nodes:\n",
    "    base_logits = model(perturbed_X, A_t)[node_idx].detach()\n",
    "    diffs = []\n",
    "    for _ in range(10):\n",
    "        noise = 0.05 * torch.randn_like(perturbed_X[node_idx])\n",
    "        X_mod = perturbed_X.clone().detach()\n",
    "        X_mod[node_idx] = perturbed_X[node_idx] + noise\n",
    "        with torch.no_grad():\n",
    "            logits_n = model(X_mod, A_t)[node_idx]\n",
    "        diffs.append(torch.norm(logits_n - base_logits).item())\n",
    "    stability_val = np.mean(diffs)\n",
    "    # finite-difference style perturbation for stability\n",
    "    noise_fd = 0.05 * torch.randn_like(perturbed_X[node_idx])\n",
    "    X_fd = perturbed_X.clone().detach()\n",
    "    X_fd[node_idx] = perturbed_X[node_idx] + noise_fd\n",
    "    with torch.no_grad():\n",
    "        logits_fd = model(X_fd, A_t)[node_idx]\n",
    "    diffs_fd = [torch.norm(logits_fd - base_logits).item()]\n",
    "    stability_val_p = np.mean(diffs_fd)\n",
    "    rel_err = abs(stability_val - stability_val_p) / (abs(stability_val_p) + 1e-12)\n",
    "    stability_info.append((int(labels_np[node_idx]), stability_val, rel_err))\n",
    "\n",
    "print(\"\\nStability Under Noise:\")\n",
    "for cls in [0,1]:\n",
    "    vals = [j[1] for j in stability_info if j[0] == cls]\n",
    "    errs = [j[2] for j in stability_info if j[0] == cls]\n",
    "    print(f\" Class {cls}: stability={np.mean(vals):.4f}±{np.std(vals):.4f}, relerr={np.mean(errs):.4e}±{np.std(errs):.4e}\")\n",
    "\n",
    "evaluate_model(\"Stability\", perturbed_X, selected_nodes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fe04ca3-b97e-4d31-932a-1a1e7f1c8571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction Margin:\n",
      " Class 0: margin=1.7415±1.5443, relerr=0.0000e+00±0.0000e+00\n",
      " Class 1: margin=38.8759±1.5805, relerr=0.0000e+00±0.0000e+00\n",
      "\n",
      "=== Robustness Eval (Margin) ===\n",
      "Flipped 144/200 (72.00%)\n",
      "Accuracy=27.50 | Precision=0.1774 | Recall=0.2750 | F1=0.2157\n",
      "Confusion Matrix:\n",
      "[[ 55  45]\n",
      " [100   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     0.3548    0.5500    0.4314       100\n",
      "      trojan     0.0000    0.0000    0.0000       100\n",
      "\n",
      "    accuracy                         0.2750       200\n",
      "   macro avg     0.1774    0.2750    0.2157       200\n",
      "weighted avg     0.1774    0.2750    0.2157       200\n",
      "\n",
      "\n",
      "Adversarial Robustness Radius:\n",
      " Class 0: radius=20.0000±0.0000, relerr=0.0000e+00±0.0000e+00\n",
      " Class 1: radius=20.0000±0.0000, relerr=0.0000e+00±0.0000e+00\n",
      "\n",
      "=== Robustness Eval (ARR) ===\n",
      "Flipped 144/200 (72.00%)\n",
      "Accuracy=27.50 | Precision=0.1774 | Recall=0.2750 | F1=0.2157\n",
      "Confusion Matrix:\n",
      "[[ 55  45]\n",
      " [100   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     0.3548    0.5500    0.4314       100\n",
      "      trojan     0.0000    0.0000    0.0000       100\n",
      "\n",
      "    accuracy                         0.2750       200\n",
      "   macro avg     0.1774    0.2750    0.2157       200\n",
      "weighted avg     0.1774    0.2750    0.2157       200\n",
      "\n",
      "\n",
      "Stability Under Noise:\n",
      " Class 0: stability=0.0184±0.0063, relerr=2.7858e+00±5.1980e+00\n",
      " Class 1: stability=0.3003±0.0743, relerr=3.5168e+00±1.5404e+01\n",
      "\n",
      "=== Robustness Eval (Stability) ===\n",
      "Flipped 144/200 (72.00%)\n",
      "Accuracy=27.50 | Precision=0.1774 | Recall=0.2750 | F1=0.2157\n",
      "Confusion Matrix:\n",
      "[[ 55  45]\n",
      " [100   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     0.3548    0.5500    0.4314       100\n",
      "      trojan     0.0000    0.0000    0.0000       100\n",
      "\n",
      "    accuracy                         0.2750       200\n",
      "   macro avg     0.1774    0.2750    0.2157       200\n",
      "weighted avg     0.1774    0.2750    0.2157       200\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.9928, -5.0827],\n",
       "        [ 4.9975, -4.9917],\n",
       "        [ 4.9780, -4.9694],\n",
       "        ...,\n",
       "        [ 4.7599, -4.9413],\n",
       "        [ 4.0829, -4.2264],\n",
       "        [ 4.0829, -4.2264]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ---------------- Metric 4: Prediction Margin ----------------\n",
    "margin_info = []\n",
    "for node_idx in selected_nodes:\n",
    "    logits = model(perturbed_X, A_t)[node_idx]\n",
    "    pred_class = logits.argmax().item()\n",
    "    margin = logits[pred_class].item() - logits[[j for j in range(len(logits)) if j!=pred_class]].max().item()\n",
    "    delta = FD_EPS*torch.randn_like(perturbed_X[node_idx])\n",
    "    logits_p = model(perturbed_X.clone().detach(), A_t)[node_idx]\n",
    "    margin_p = logits_p[pred_class].item() - logits_p[[j for j in range(len(logits_p)) if j!=pred_class]].max().item()\n",
    "    rel_err = abs(margin-margin_p)/(abs(margin_p)+1e-12)\n",
    "    margin_info.append((int(labels_np[node_idx]), margin, rel_err))\n",
    "print(\"\\nPrediction Margin:\")\n",
    "for cls in [0,1]:\n",
    "    vals = [j[1] for j in margin_info if j[0]==cls]\n",
    "    errs = [j[2] for j in margin_info if j[0]==cls]\n",
    "    print(f\" Class {cls}: margin={np.mean(vals):.4f}±{np.std(vals):.4f}, relerr={np.mean(errs):.4e}±{np.std(errs):.4e}\")\n",
    "\n",
    "evaluate_model(\"Margin\", perturbed_X, selected_nodes)\n",
    "\n",
    "# ---------------- Metric 5: ARR ----------------\n",
    "# (kept simplified: min perturbation until flip)\n",
    "def adversarial_radius(node_idx):\n",
    "    x0 = perturbed_X[node_idx].detach().clone()\n",
    "    base_pred = int(model(perturbed_X, A_t)[node_idx].argmax().item())\n",
    "    eps, growth = 1e-3, 1.2\n",
    "    while eps < 20:\n",
    "        x_try = x0 + eps*torch.randn_like(x0)\n",
    "        with torch.no_grad():\n",
    "            pred = int(model(perturbed_X.clone().detach(), A_t)[node_idx].argmax().item())\n",
    "        if pred != base_pred: return eps\n",
    "        eps *= growth\n",
    "    return 20.0\n",
    "\n",
    "arr_info = []\n",
    "for n in selected_nodes:\n",
    "    arr_val = adversarial_radius(n)\n",
    "    # finite-difference style perturbation for ARR\n",
    "    delta = FD_EPS * torch.randn_like(perturbed_X[n])\n",
    "    arr_val_p = adversarial_radius(n)  # here you could recompute with perturbed input if desired\n",
    "    rel_err = abs(arr_val - arr_val_p) / (abs(arr_val_p) + 1e-12)\n",
    "    arr_info.append((int(labels_np[n]), arr_val, rel_err))\n",
    "\n",
    "print(\"\\nAdversarial Robustness Radius:\")\n",
    "for cls in [0,1]:\n",
    "    vals = [j[1] for j in arr_info if j[0] == cls]\n",
    "    errs = [j[2] for j in arr_info if j[0] == cls]\n",
    "    print(f\" Class {cls}: radius={np.mean(vals):.4f}±{np.std(vals):.4f}, relerr={np.mean(errs):.4e}±{np.std(errs):.4e}\")\n",
    "\n",
    "evaluate_model(\"ARR\", perturbed_X, selected_nodes)\n",
    "\n",
    "# ---------------- Metric 6: Stability ----------------\n",
    "stability_info = []\n",
    "for node_idx in selected_nodes:\n",
    "    base_logits = model(perturbed_X, A_t)[node_idx].detach()\n",
    "    diffs = []\n",
    "    for _ in range(10):\n",
    "        noise = 0.05 * torch.randn_like(perturbed_X[node_idx])\n",
    "        X_mod = perturbed_X.clone().detach()\n",
    "        X_mod[node_idx] = perturbed_X[node_idx] + noise\n",
    "        with torch.no_grad():\n",
    "            logits_n = model(X_mod, A_t)[node_idx]\n",
    "        diffs.append(torch.norm(logits_n - base_logits).item())\n",
    "    stability_val = np.mean(diffs)\n",
    "    # finite-difference style perturbation for stability\n",
    "    noise_fd = 0.05 * torch.randn_like(perturbed_X[node_idx])\n",
    "    X_fd = perturbed_X.clone().detach()\n",
    "    X_fd[node_idx] = perturbed_X[node_idx] + noise_fd\n",
    "    with torch.no_grad():\n",
    "        logits_fd = model(X_fd, A_t)[node_idx]\n",
    "    diffs_fd = [torch.norm(logits_fd - base_logits).item()]\n",
    "    stability_val_p = np.mean(diffs_fd)\n",
    "    rel_err = abs(stability_val - stability_val_p) / (abs(stability_val_p) + 1e-12)\n",
    "    stability_info.append((int(labels_np[node_idx]), stability_val, rel_err))\n",
    "\n",
    "print(\"\\nStability Under Noise:\")\n",
    "for cls in [0,1]:\n",
    "    vals = [j[1] for j in stability_info if j[0] == cls]\n",
    "    errs = [j[2] for j in stability_info if j[0] == cls]\n",
    "    print(f\" Class {cls}: stability={np.mean(vals):.4f}±{np.std(vals):.4f}, relerr={np.mean(errs):.4e}±{np.std(errs):.4e}\")\n",
    "\n",
    "evaluate_model(\"Stability\", perturbed_X, selected_nodes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef166527-5f71-4589-a2e5-3cc666863802",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
