{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6caf6b8f-50f8-475a-9da8-142ac5ead8a8",
   "metadata": {},
   "source": [
    "#### Training Evaluation similar to https://github.com/AICPS/hw2vec\n",
    "\n",
    "Model to a HW2VEC-style GIN architecture (MLP + BatchNorm + ReLU with learnable e and sum-aggregation), while keeping all preprocessing, splits, training loop, optimizer, class weights, and evaluation unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbf1215c-0a9e-4619-a418-218eb6db72c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss 1.1374 | Val 0.2816 | Test 0.2836\n",
      "Epoch 010 | Loss 0.0668 | Val 0.9928 | Test 0.9918\n",
      "Epoch 020 | Loss 0.0134 | Val 0.9832 | Test 0.9849\n",
      "Epoch 030 | Loss 0.0082 | Val 0.9999 | Test 0.9999\n",
      "Epoch 040 | Loss 0.0048 | Val 1.0000 | Test 1.0000\n",
      "Epoch 050 | Loss 0.0031 | Val 1.0000 | Test 0.9999\n",
      "Epoch 060 | Loss 0.0020 | Val 1.0000 | Test 0.9999\n",
      "Epoch 070 | Loss 0.0013 | Val 1.0000 | Test 1.0000\n",
      "Epoch 080 | Loss 0.0010 | Val 1.0000 | Test 1.0000\n",
      "Epoch 090 | Loss 0.0008 | Val 1.0000 | Test 1.0000\n",
      "Epoch 100 | Loss 0.0006 | Val 1.0000 | Test 1.0000\n",
      "Epoch 110 | Loss 0.0007 | Val 1.0000 | Test 1.0000\n",
      "Epoch 120 | Loss 0.0005 | Val 1.0000 | Test 1.0000\n",
      "Epoch 130 | Loss 0.0005 | Val 1.0000 | Test 1.0000\n",
      "Epoch 140 | Loss 0.0005 | Val 1.0000 | Test 1.0000\n",
      "Epoch 150 | Loss 0.0005 | Val 1.0000 | Test 1.0000\n",
      "Epoch 160 | Loss 0.0005 | Val 1.0000 | Test 1.0000\n",
      "Epoch 170 | Loss 0.0004 | Val 1.0000 | Test 1.0000\n",
      "Epoch 180 | Loss 0.0004 | Val 1.0000 | Test 1.0000\n",
      "Epoch 190 | Loss 0.0004 | Val 1.0000 | Test 1.0000\n",
      "Epoch 200 | Loss 0.0004 | Val 1.0000 | Test 1.0000\n",
      "Epoch 210 | Loss 0.0005 | Val 1.0000 | Test 1.0000\n",
      "Epoch 220 | Loss 0.0004 | Val 1.0000 | Test 1.0000\n",
      "Epoch 230 | Loss 0.0004 | Val 1.0000 | Test 1.0000\n",
      "Early stopping.\n",
      "\n",
      "Final Evaluation (Node-Level)\n",
      "=============================\n",
      "Test Accuracy: 0.9999\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     0.9997    0.9999    0.9998      9159\n",
      "      trojan     1.0000    0.9999    0.9999     27556\n",
      "\n",
      "    accuracy                         0.9999     36715\n",
      "   macro avg     0.9998    0.9999    0.9999     36715\n",
      "weighted avg     0.9999    0.9999    0.9999     36715\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 9158     1]\n",
      " [    3 27553]]\n"
     ]
    }
   ],
   "source": [
    "# train_gcn_node_fixed.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "NODE_CSV  = \"GNNDatasets/node.csv\"\n",
    "EDGE_CSV  = \"GNNDatasets/node_edges.csv\"\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# ----------------------------- Load nodes -----------------------------\n",
    "nodes_df = pd.read_csv(NODE_CSV)\n",
    "\n",
    "label_col = None\n",
    "for cand in [\"label\", \"is_trojan\", \"trojan\", \"target\"]:\n",
    "    if cand in nodes_df.columns:\n",
    "        label_col = cand; break\n",
    "if label_col is None:\n",
    "    nodes_df[\"label\"] = nodes_df[\"circuit_name\"].astype(str).str.contains(\"__trojan_\").astype(int)\n",
    "    label_col = \"label\"\n",
    "\n",
    "nodes_df[\"uid\"] = nodes_df[\"circuit_name\"].astype(str) + \"::\" + nodes_df[\"node\"].astype(str)\n",
    "\n",
    "feat_df = nodes_df.copy()\n",
    "if \"gate_type\" in feat_df.columns:\n",
    "    gate_oh = pd.get_dummies(feat_df[\"gate_type\"], prefix=\"gt\")\n",
    "    feat_df = pd.concat([feat_df.drop(columns=[\"gate_type\"]), gate_oh], axis=1)\n",
    "\n",
    "exclude = {\"uid\",\"node\",\"circuit_name\",label_col}\n",
    "num_cols = [c for c in feat_df.columns if c not in exclude and pd.api.types.is_numeric_dtype(feat_df[c])]\n",
    "X = feat_df[num_cols].fillna(0.0).values.astype(np.float32)\n",
    "y = nodes_df[label_col].values.astype(np.int64)\n",
    "\n",
    "# ----------------------------- Load edges; add missing nodes -----------------------------\n",
    "edges_df = pd.read_csv(EDGE_CSV)\n",
    "edges_df[\"src_uid\"] = edges_df[\"circuit_name\"].astype(str) + \"::\" + edges_df[\"src\"].astype(str)\n",
    "edges_df[\"dst_uid\"] = edges_df[\"circuit_name\"].astype(str) + \"::\" + edges_df[\"dst\"].astype(str)\n",
    "\n",
    "known_uids = set(nodes_df[\"uid\"])\n",
    "edge_uids = set(edges_df[\"src_uid\"]).union(set(edges_df[\"dst_uid\"]))\n",
    "missing = list(edge_uids - known_uids)\n",
    "\n",
    "if missing:\n",
    "    zero_row = np.zeros((1, X.shape[1]), dtype=np.float32)\n",
    "    addX = np.repeat(zero_row, len(missing), axis=0)\n",
    "    addY = -1*np.ones(len(missing), dtype=np.int64)\n",
    "    add_df = pd.DataFrame({\n",
    "        \"uid\": missing,\n",
    "        \"circuit_name\": [u.split(\"::\",1)[0] for u in missing],\n",
    "        \"node\": [u.split(\"::\",1)[1] for u in missing],\n",
    "        label_col: addY\n",
    "    })\n",
    "    X = np.vstack([X, addX])\n",
    "    y = np.concatenate([y, addY])\n",
    "    nodes_df = pd.concat([nodes_df, add_df], ignore_index=True)\n",
    "\n",
    "uid_to_idx = {u:i for i,u in enumerate(nodes_df[\"uid\"].tolist())}\n",
    "src_idx = edges_df[\"src_uid\"].map(uid_to_idx).dropna().astype(int).values\n",
    "dst_idx = edges_df[\"dst_uid\"].map(uid_to_idx).dropna().astype(int).values\n",
    "edge_index = np.stack([np.concatenate([src_idx, dst_idx]),\n",
    "                       np.concatenate([dst_idx, src_idx])], axis=0)\n",
    "\n",
    "# ----------------------------- Scale features -----------------------------\n",
    "labeled_mask_np = (y >= 0)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "X_scaled[labeled_mask_np] = scaler.fit_transform(X_scaled[labeled_mask_np])\n",
    "if (~labeled_mask_np).any():\n",
    "    X_scaled[~labeled_mask_np] = (X_scaled[~labeled_mask_np] - scaler.mean_) / np.sqrt(scaler.var_ + 1e-8)\n",
    "\n",
    "# ----------------------------- Splits -----------------------------\n",
    "idx_all = np.where(labeled_mask_np)[0]\n",
    "y_all = y[labeled_mask_np]\n",
    "\n",
    "idx_train, idx_tmp, y_train, y_tmp = train_test_split(\n",
    "    idx_all, y_all, test_size=0.30, random_state=SEED, stratify=y_all\n",
    ")\n",
    "idx_val, idx_test, y_val, y_test = train_test_split(\n",
    "    idx_tmp, y_tmp, test_size=0.50, random_state=SEED, stratify=y_tmp\n",
    ")\n",
    "\n",
    "# ----------------------------- Torch tensors (FIX: masks as torch.bool) -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_t = torch.from_numpy(X_scaled).to(device)\n",
    "y_t = torch.from_numpy(y).to(device)\n",
    "\n",
    "edge_index_t = torch.from_numpy(edge_index).long().to(device)\n",
    "\n",
    "train_mask_t = torch.zeros(len(y), dtype=torch.bool, device=device); train_mask_t[idx_train] = True\n",
    "val_mask_t   = torch.zeros(len(y), dtype=torch.bool, device=device); val_mask_t[idx_val]   = True\n",
    "test_mask_t  = torch.zeros(len(y), dtype=torch.bool, device=device); test_mask_t[idx_test]  = True\n",
    "labeled_mask_t = torch.from_numpy(labeled_mask_np).to(device)\n",
    "\n",
    "# ----------------------------- Build GCN adjacency -----------------------------\n",
    "def build_adj(num_nodes, edge_index):\n",
    "    self_loops = torch.arange(num_nodes, device=edge_index.device)\n",
    "    ei = torch.cat([edge_index, torch.stack([self_loops, self_loops])], dim=1)\n",
    "    deg = torch.bincount(ei[0], minlength=num_nodes).float()\n",
    "    deg_inv_sqrt = deg.clamp(min=1).pow(-0.5)\n",
    "    w = deg_inv_sqrt[ei[0]] * deg_inv_sqrt[ei[1]]\n",
    "    A = torch.sparse_coo_tensor(ei, w, (num_nodes, num_nodes))\n",
    "    return A.coalesce()\n",
    "\n",
    "A_t = build_adj(X_t.size(0), edge_index_t)\n",
    "\n",
    "# ----------------------------- Model -----------------------------\n",
    "# ----------------------------- Build unnormalized adjacency (GIN-style sum aggregation) -----------------------------\n",
    "def build_adj_unnorm(num_nodes, edge_index):\n",
    "    # Add self-loops; unit weights; coalesce\n",
    "    self_loops = torch.arange(num_nodes, device=edge_index.device)\n",
    "    ei = torch.cat([edge_index, torch.stack([self_loops, self_loops])], dim=1)\n",
    "    vals = torch.ones(ei.size(1), device=edge_index.device)\n",
    "    A = torch.sparse_coo_tensor(ei, vals, (num_nodes, num_nodes))\n",
    "    return A.coalesce()\n",
    "\n",
    "A_t = build_adj_unnorm(X_t.size(0), edge_index_t)\n",
    "\n",
    "# ----------------------------- HW2VEC-style GIN Model for node classification -----------------------------\n",
    "class GINLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, dropout=0.35, eps_init=0.0):\n",
    "        super().__init__()\n",
    "        # MLP as in GIN: Linear -> BN -> ReLU -> Linear\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim, bias=True),\n",
    "            nn.BatchNorm1d(out_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(out_dim, out_dim, bias=True),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Learnable epsilon for (1 + eps) * h + sum_neighbors(h)\n",
    "        self.eps = nn.Parameter(torch.tensor(eps_init, dtype=torch.float32))\n",
    "        # Init\n",
    "        for m in self.mlp:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x, adj_unnorm):\n",
    "        # sum aggregation with self-loops already in adj\n",
    "        agg = torch.sparse.mm(adj_unnorm, x)\n",
    "        # Separate self contribution: since adj has self-loops of weight 1,\n",
    "        # agg = h + sum_neighbors(h). Then (1+eps)*h + sum_neighbors(h) = agg + eps*h\n",
    "        out = agg + self.eps * x\n",
    "        out = self.mlp(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class HW2VEC_GIN_Node(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=96, out_dim=2, dropout=0.35, eps_init=0.0):\n",
    "        super().__init__()\n",
    "        self.g1 = GINLayer(in_dim, hid_dim, dropout=dropout, eps_init=eps_init)\n",
    "        self.g2 = GINLayer(hid_dim, hid_dim, dropout=dropout, eps_init=eps_init)\n",
    "        self.lin_out = nn.Linear(hid_dim, out_dim, bias=True)\n",
    "        nn.init.xavier_uniform_(self.lin_out.weight)\n",
    "        if self.lin_out.bias is not None:\n",
    "            nn.init.zeros_(self.lin_out.bias)\n",
    "\n",
    "    def forward(self, x, adj_unnorm):\n",
    "        x = self.g1(x, adj_unnorm)\n",
    "        x = self.g2(x, adj_unnorm)\n",
    "        x = self.lin_out(x)\n",
    "        return x\n",
    "\n",
    "# Replace your previous model with HW2VEC-style GIN\n",
    "model = HW2VEC_GIN_Node(in_dim=X_t.size(1), hid_dim=96, out_dim=2, dropout=0.35, eps_init=0.0).to(device)\n",
    "\n",
    "\n",
    "# ----------------------------- Loss, optimizer -----------------------------\n",
    "train_labels = y_t[train_mask_t]\n",
    "classes, counts = torch.unique(train_labels, return_counts=True)\n",
    "num_pos = counts[classes==1].item() if (classes==1).any() else 1\n",
    "num_neg = counts[classes==0].item() if (classes==0).any() else 1\n",
    "weight_pos = (num_neg + num_pos) / (2.0 * num_pos)\n",
    "weight_neg = (num_neg + num_pos) / (2.0 * num_neg)\n",
    "class_weights = torch.tensor([weight_neg, weight_pos], dtype=torch.float32, device=device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3, weight_decay=5e-4)\n",
    "\n",
    "# ----------------------------- Training -----------------------------\n",
    "def evaluate(mask_t):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_t, A_t)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        msk = mask_t & (y_t >= 0)\n",
    "        if msk.sum() == 0: return 0.0\n",
    "        return (pred[msk] == y_t[msk]).float().mean().item()\n",
    "\n",
    "best_val, best_state = -1.0, None\n",
    "patience, patience_cnt = 20, 0\n",
    "EPOCHS = 300\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(X_t, A_t)\n",
    "    loss = criterion(logits[train_mask_t], y_t[train_mask_t])\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        val_acc = evaluate(val_mask_t)\n",
    "        test_acc = evaluate(test_mask_t)\n",
    "        print(f\"Epoch {epoch:03d} | Loss {loss.item():.4f} | Val {val_acc:.4f} | Test {test_acc:.4f}\")\n",
    "        if val_acc > best_val + 1e-4:\n",
    "            best_val = val_acc\n",
    "            best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "            patience_cnt = 0\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            if patience_cnt >= patience:\n",
    "                print(\"Early stopping.\"); break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "# ----------------------------- Final eval -----------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X_t, A_t)\n",
    "    preds = logits.argmax(dim=1)\n",
    "\n",
    "msk = (test_mask_t & (y_t >= 0)).cpu().numpy()\n",
    "y_true = y_t.cpu().numpy()[msk]\n",
    "y_pred = preds.cpu().numpy()[msk]\n",
    "\n",
    "acc = (y_true == y_pred).mean()\n",
    "print(\"\\nFinal Evaluation (Node-Level)\")\n",
    "print(\"=============================\")\n",
    "print(f\"Test Accuracy: {acc:.4f}\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, labels=[0,1], target_names=[\"clean\",\"trojan\"], digits=4))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b86dfef6-38d7-4c54-8aee-eddb5a1d70ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: clean=100, trojan=100\n",
      "? Shared PGD perturbations done.\n",
      "\n",
      "Jacobian Sensitivity:\n",
      " Class 0: norm=1.5677±0.6970, relerr=3.7685e-03±7.6597e-03\n",
      " Class 1: norm=1.3663±0.1219, relerr=2.3876e-02±1.8866e-01\n",
      "\n",
      "=== Robustness Eval (Jacobian) ===\n",
      "Flipped 93/200 (46.50%)\n",
      "Accuracy=53.50 | Precision=0.5350 | Recall=0.5350 | F1=0.5350\n",
      "Confusion Matrix:\n",
      "[[53 47]\n",
      " [46 54]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     0.5354    0.5300    0.5327       100\n",
      "      trojan     0.5347    0.5400    0.5373       100\n",
      "\n",
      "    accuracy                         0.5350       200\n",
      "   macro avg     0.5350    0.5350    0.5350       200\n",
      "weighted avg     0.5350    0.5350    0.5350       200\n",
      "\n",
      "\n",
      "Lipschitz Constant:\n",
      " Class 0: L=1.5646±0.6972, relerr=6.9201e-03±2.4854e-02\n",
      " Class 1: L=1.3654±0.1218, relerr=1.7527e-02±1.2781e-01\n",
      "\n",
      "=== Robustness Eval (Lipschitz) ===\n",
      "Flipped 93/200 (46.50%)\n",
      "Accuracy=53.50 | Precision=0.5350 | Recall=0.5350 | F1=0.5350\n",
      "Confusion Matrix:\n",
      "[[53 47]\n",
      " [46 54]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     0.5354    0.5300    0.5327       100\n",
      "      trojan     0.5347    0.5400    0.5373       100\n",
      "\n",
      "    accuracy                         0.5350       200\n",
      "   macro avg     0.5350    0.5350    0.5350       200\n",
      "weighted avg     0.5350    0.5350    0.5350       200\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 11.1726, -11.6568],\n",
       "        [  9.0651,  -9.4224],\n",
       "        [  9.9202, -10.1688],\n",
       "        ...,\n",
       "        [  5.3161,  -5.7546],\n",
       "        [  5.3161,  -5.7546],\n",
       "        [  3.2757,  -3.6181]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ================================\n",
    "# Unified Robustness Evaluation\n",
    "# ================================\n",
    "import torch, numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "# ---------------- Parameters ----------------\n",
    "PER_CLASS = 100\n",
    "EPSILON   = 5.0     # L2 budget for PGD\n",
    "ALPHA     = 1.0     # PGD step size\n",
    "NUM_ITERS = 40      # PGD iterations\n",
    "FD_EPS    = 1e-3    # finite-difference epsilon\n",
    "SEED      = 42\n",
    "\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "required_vars = [\"model\",\"X_t\",\"A_t\",\"y_t\",\"test_mask_t\",\"device\"]\n",
    "for v in required_vars:\n",
    "    if v not in globals():\n",
    "        raise RuntimeError(f\"Required var '{v}' not found.\")\n",
    "\n",
    "model.to(device); model.eval()\n",
    "labels_np = y_t.cpu().numpy()\n",
    "\n",
    "# ---------------- Node Selection ----------------\n",
    "test_indices = np.where(test_mask_t.cpu().numpy())[0]\n",
    "rng = np.random.default_rng(SEED)\n",
    "selected_nodes = []\n",
    "for cls in [0,1]:\n",
    "    idxs = [int(i) for i in test_indices if labels_np[i]==cls]\n",
    "    chosen = rng.choice(idxs, size=min(PER_CLASS, len(idxs)), replace=False)\n",
    "    selected_nodes.extend(chosen)\n",
    "selected_nodes = np.array(selected_nodes, dtype=np.int64)\n",
    "\n",
    "print(f\"Selected: clean={int((labels_np[selected_nodes]==0).sum())}, \"\n",
    "      f\"trojan={int((labels_np[selected_nodes]==1).sum())}\")\n",
    "\n",
    "# ---------------- Shared PGD Perturbations ----------------\n",
    "perturbed_X = X_t.clone().detach().to(device)\n",
    "for node_idx in selected_nodes:\n",
    "    node_idx = int(node_idx)\n",
    "    x_orig = X_t[node_idx].detach().clone().to(device)\n",
    "    x_adv = (x_orig + 1e-3*torch.randn_like(x_orig)).detach().requires_grad_(True)\n",
    "\n",
    "    for _ in range(NUM_ITERS):\n",
    "        X_mod = perturbed_X.clone().detach()\n",
    "        X_mod[node_idx] = x_adv\n",
    "        logits = model(X_mod, A_t)\n",
    "        loss = F.cross_entropy(logits[node_idx].unsqueeze(0), y_t[node_idx].unsqueeze(0))\n",
    "        grad_x = torch.autograd.grad(loss, x_adv)[0]\n",
    "        if grad_x.norm().item()==0: break\n",
    "        step = ALPHA * grad_x / (grad_x.norm() + 1e-12)\n",
    "        x_adv = (x_adv + step).detach()\n",
    "        delta = x_adv - x_orig\n",
    "        if delta.norm() > EPSILON:\n",
    "            delta = delta * (EPSILON/(delta.norm()+1e-12))\n",
    "            x_adv = (x_orig + delta).detach()\n",
    "        x_adv = x_adv.requires_grad_(True)\n",
    "    perturbed_X[node_idx] = x_adv.detach()\n",
    "print(\"? Shared PGD perturbations done.\")\n",
    "\n",
    "# ---------------- Eval Helper ----------------\n",
    "def evaluate_model(name, perturbed_X, selected_nodes):\n",
    "    with torch.no_grad():\n",
    "        # Predictions on original and perturbed inputs\n",
    "        orig_logits = model(X_t, A_t)\n",
    "        pert_logits = model(perturbed_X, A_t)\n",
    "\n",
    "        orig_preds = orig_logits.argmax(dim=1).cpu().numpy()\n",
    "        pert_preds = pert_logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "    # Restrict to selected perturbed samples only\n",
    "    sel_idx = np.array(selected_nodes)\n",
    "    sel_labels = labels_np[sel_idx]\n",
    "    sel_orig_preds = orig_preds[sel_idx]\n",
    "    sel_pert_preds = pert_preds[sel_idx]\n",
    "\n",
    "    # Flip count first\n",
    "    flips = (sel_orig_preds != sel_pert_preds).sum()\n",
    "    print(f\"\\n=== Robustness Eval ({name}) ===\")\n",
    "    print(f\"Flipped {flips}/{len(sel_idx)} ({100*flips/len(sel_idx):.2f}%)\")\n",
    "\n",
    "    # Accuracy, precision, recall, F1 on perturbed subset only\n",
    "    acc = (sel_pert_preds == sel_labels).mean()\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        sel_labels, sel_pert_preds, average=\"weighted\", zero_division=0)\n",
    "\n",
    "    print(f\"Accuracy={acc*100:.2f} | Precision={prec:.4f} | Recall={rec:.4f} | F1={f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(sel_labels, sel_pert_preds, labels=[0, 1]))\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(sel_labels, sel_pert_preds,\n",
    "                                target_names=[\"clean\", \"trojan\"], digits=4))\n",
    "\n",
    "    return pert_logits\n",
    "\n",
    "\n",
    "# ---------------- Metric 1: Jacobian Sensitivity ----------------\n",
    "jac_info = []\n",
    "for node_idx in selected_nodes:\n",
    "    x0 = perturbed_X[node_idx].detach().clone().requires_grad_(True)\n",
    "    def f_local(x):\n",
    "        X_mod = perturbed_X.clone().detach()\n",
    "        X_mod[node_idx] = x\n",
    "        return model(X_mod, A_t)[node_idx]\n",
    "    J = torch.autograd.functional.jacobian(f_local, x0)\n",
    "    jac_norm = torch.norm(J, p='fro').item()\n",
    "    delta_fd = FD_EPS * torch.randn_like(x0)\n",
    "    pred_change = J.mv(delta_fd)\n",
    "    f0, f0p = f_local(x0).detach(), f_local(x0+delta_fd).detach()\n",
    "    actual_change = f0p - f0\n",
    "    rel_err = (torch.norm(pred_change-actual_change)/(torch.norm(actual_change)+1e-8)).item()\n",
    "    jac_info.append((int(labels_np[node_idx]), jac_norm, rel_err))\n",
    "print(\"\\nJacobian Sensitivity:\")\n",
    "for cls in [0,1]:\n",
    "    vals = [j[1] for j in jac_info if j[0]==cls]\n",
    "    errs = [j[2] for j in jac_info if j[0]==cls]\n",
    "    print(f\" Class {cls}: norm={np.mean(vals):.4f}±{np.std(vals):.4f}, relerr={np.mean(errs):.4e}±{np.std(errs):.4e}\")\n",
    "\n",
    "evaluate_model(\"Jacobian\", perturbed_X, selected_nodes)\n",
    "\n",
    "# ---------------- Metric 2: Lipschitz (Spectral Norm) ----------------\n",
    "lip_info = []\n",
    "for node_idx in selected_nodes:\n",
    "    x0 = perturbed_X[node_idx].detach().clone().requires_grad_(True)\n",
    "    def f_node(x):\n",
    "        X_mod = perturbed_X.clone().detach()\n",
    "        X_mod[node_idx] = x\n",
    "        return model(X_mod, A_t)[node_idx]\n",
    "    J = torch.autograd.functional.jacobian(f_node, x0).detach()\n",
    "    U, S, Vh = torch.linalg.svd(J, full_matrices=False)\n",
    "    sigma_max = S[0].item()\n",
    "    delta_fd = FD_EPS*torch.randn_like(x0)\n",
    "    pred_change = J.mv(delta_fd)\n",
    "    f0, f0p = f_node(x0).detach(), f_node(x0+delta_fd).detach()\n",
    "    actual_change = f0p-f0\n",
    "    rel_err = (torch.norm(pred_change-actual_change)/(torch.norm(actual_change)+1e-8)).item()\n",
    "    lip_info.append((int(labels_np[node_idx]), sigma_max, rel_err))\n",
    "print(\"\\nLipschitz Constant:\")\n",
    "for cls in [0,1]:\n",
    "    vals = [j[1] for j in lip_info if j[0]==cls]\n",
    "    errs = [j[2] for j in lip_info if j[0]==cls]\n",
    "    print(f\" Class {cls}: L={np.mean(vals):.4f}±{np.std(vals):.4f}, relerr={np.mean(errs):.4e}±{np.std(errs):.4e}\")\n",
    "\n",
    "evaluate_model(\"Lipschitz\", perturbed_X, selected_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cbc1457-dcf4-4db2-a2a4-cf7f1e440152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing Hessian curvature proxy for selected nodes...\n",
      "\n",
      "Aggregated Hessian curvature stats:\n",
      " Clean:  avg_lambda=0.0023 ± 0.0162, avg_FDrel=3.4178e-01 ± 3.4251e-01\n",
      " Trojan: avg_lambda=0.0082 ± 0.0364, avg_FDrel=7.5314e-01 ± 1.3846e-01\n",
      "\n",
      "Sample preview (first 6): (idx,label,lambda,FD_rel_err)\n",
      "(26119, 0, 3.0676810870633265e-26, 5.7264076863641344e-08)\n",
      "(57231, 0, 1.8918313797500821e-19, 0.00019066434799292788)\n",
      "(40297, 0, 4.435684259553356e-12, 0.4839210733958579)\n",
      "(27843, 0, 1.9685782326908182e-12, 0.3691970972634096)\n",
      "(26863, 0, 8.190093011877588e-10, 0.776914613113268)\n",
      "(46828, 0, 3.023158947433883e-15, 0.03561590148276072)\n",
      "\n",
      "=== Robustness Eval (Margin) ===\n",
      "Flipped 93/200 (46.50%)\n",
      "Accuracy=53.50 | Precision=0.5350 | Recall=0.5350 | F1=0.5350\n",
      "Confusion Matrix:\n",
      "[[53 47]\n",
      " [46 54]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     0.5354    0.5300    0.5327       100\n",
      "      trojan     0.5347    0.5400    0.5373       100\n",
      "\n",
      "    accuracy                         0.5350       200\n",
      "   macro avg     0.5350    0.5350    0.5350       200\n",
      "weighted avg     0.5350    0.5350    0.5350       200\n",
      "\n",
      "\n",
      "Prediction Margin:\n",
      " Class 0: margin=11.2636±17.2496, relerr=0.0000e+00±0.0000e+00\n",
      " Class 1: margin=1.8358±1.7138, relerr=0.0000e+00±0.0000e+00\n",
      "\n",
      "=== Robustness Eval (Margin) ===\n",
      "Flipped 93/200 (46.50%)\n",
      "Accuracy=53.50 | Precision=0.5350 | Recall=0.5350 | F1=0.5350\n",
      "Confusion Matrix:\n",
      "[[53 47]\n",
      " [46 54]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     0.5354    0.5300    0.5327       100\n",
      "      trojan     0.5347    0.5400    0.5373       100\n",
      "\n",
      "    accuracy                         0.5350       200\n",
      "   macro avg     0.5350    0.5350    0.5350       200\n",
      "weighted avg     0.5350    0.5350    0.5350       200\n",
      "\n",
      "\n",
      "Adversarial Robustness Radius:\n",
      " Class 0: radius=20.0000±0.0000, relerr=0.0000e+00±0.0000e+00\n",
      " Class 1: radius=20.0000±0.0000, relerr=0.0000e+00±0.0000e+00\n",
      "\n",
      "=== Robustness Eval (ARR) ===\n",
      "Flipped 93/200 (46.50%)\n",
      "Accuracy=53.50 | Precision=0.5350 | Recall=0.5350 | F1=0.5350\n",
      "Confusion Matrix:\n",
      "[[53 47]\n",
      " [46 54]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     0.5354    0.5300    0.5327       100\n",
      "      trojan     0.5347    0.5400    0.5373       100\n",
      "\n",
      "    accuracy                         0.5350       200\n",
      "   macro avg     0.5350    0.5350    0.5350       200\n",
      "weighted avg     0.5350    0.5350    0.5350       200\n",
      "\n",
      "\n",
      "Stability Under Noise:\n",
      " Class 0: stability=0.0645±0.0312, relerr=1.6753e+00±2.5638e+00\n",
      " Class 1: stability=0.0552±0.0126, relerr=2.3488e+00±5.8386e+00\n",
      "\n",
      "=== Robustness Eval (Stability) ===\n",
      "Flipped 93/200 (46.50%)\n",
      "Accuracy=53.50 | Precision=0.5350 | Recall=0.5350 | F1=0.5350\n",
      "Confusion Matrix:\n",
      "[[53 47]\n",
      " [46 54]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     0.5354    0.5300    0.5327       100\n",
      "      trojan     0.5347    0.5400    0.5373       100\n",
      "\n",
      "    accuracy                         0.5350       200\n",
      "   macro avg     0.5350    0.5350    0.5350       200\n",
      "weighted avg     0.5350    0.5350    0.5350       200\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 11.1726, -11.6568],\n",
       "        [  9.0651,  -9.4224],\n",
       "        [  9.9202, -10.1688],\n",
       "        ...,\n",
       "        [  5.3161,  -5.7546],\n",
       "        [  5.3161,  -5.7546],\n",
       "        [  3.2757,  -3.6181]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# Hessian-Based Curvature (grad outer-product) for node-level Trojan detection\n",
    "# =========================\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "# -------------------- Parameters --------------------\n",
    "PER_CLASS = 100          # 100 nodes per class (clean/trojan)\n",
    "FD_EPS = 5e-3            # finite-diff epsilon for relative-error check\n",
    "TRIALS_PER_NODE = 10     # average trials per node for relative error\n",
    "PERT_P = 6.0             # L2 magnitude for final Hessian-aligned perturbation (tuneable)\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# -------------------- Class names --------------------\n",
    "class_names = [\"clean\", \"trojan\"]\n",
    "\n",
    "\n",
    "# -------------------- Helper: compute g(x) --------------------\n",
    "def compute_gradient(node_idx):\n",
    "    \"\"\"\n",
    "    Returns gradient g = ?_x log p(y_hat|x) at node_idx.\n",
    "    \"\"\"\n",
    "    x0 = X_t[node_idx].detach().clone().to(device).requires_grad_(True)\n",
    "\n",
    "    # Forward pass with x0 replacing features of node_idx\n",
    "    X_mod = X_t.clone().detach().to(device)\n",
    "    X_mod[node_idx] = x0\n",
    "    logits = model(X_mod, A_t)[node_idx]\n",
    "\n",
    "    # Use predicted class\n",
    "    pred_class = logits.argmax().item()\n",
    "    logp = F.log_softmax(logits, dim=0)\n",
    "    loss = logp[pred_class]\n",
    "\n",
    "    g = torch.autograd.grad(loss, x0, retain_graph=False, create_graph=False, allow_unused=False)[0]\n",
    "    return x0.detach(), g.detach(), pred_class\n",
    "\n",
    "# -------------------- Storage --------------------\n",
    "per_sample_info = []   # (node_idx, label, lambda_max, avg_rel_error)\n",
    "\n",
    "print(\"\\nComputing Hessian curvature proxy for selected nodes...\")\n",
    "\n",
    "for node_idx in selected_nodes:\n",
    "    node_idx = int(node_idx)\n",
    "    label = int(labels_np[node_idx])\n",
    "\n",
    "    x0, g, pred_class = compute_gradient(node_idx)\n",
    "    if g is None:\n",
    "        lambda_max = 0.0\n",
    "        avg_rel_err = 0.0\n",
    "    else:\n",
    "        # curvature proxy = ||g||^2\n",
    "        lambda_max = float(g.norm(p=2).item() ** 2)\n",
    "\n",
    "        # relative error by finite-difference\n",
    "        rel_errs = []\n",
    "        for _ in range(TRIALS_PER_NODE):\n",
    "            delta = FD_EPS * torch.randn_like(x0).to(device)\n",
    "            gt_delta = torch.dot(g, delta).item()\n",
    "            pred_second = 0.5 * (gt_delta ** 2)\n",
    "\n",
    "            # recompute logits at perturbed input\n",
    "            X_mod = X_t.clone().detach().to(device)\n",
    "            X_mod[node_idx] = x0 + delta\n",
    "            logits_p = model(X_mod, A_t)[node_idx]\n",
    "            logp_p = F.log_softmax(logits_p, dim=0)\n",
    "            actual_second = float((logp_p[pred_class] - F.log_softmax(model(X_t, A_t)[node_idx], dim=0)[pred_class]).item() - torch.dot(g, delta).item())\n",
    "\n",
    "            rel_error = abs(pred_second - actual_second) / (abs(actual_second) + 1e-8)\n",
    "            rel_errs.append(rel_error)\n",
    "\n",
    "        avg_rel_err = float(np.mean(rel_errs))\n",
    "\n",
    "    per_sample_info.append((node_idx, label, lambda_max, avg_rel_err))\n",
    "\n",
    "# -------------------- Aggregate stats --------------------\n",
    "clean_stats = [t for t in per_sample_info if t[1]==0]\n",
    "troj_stats  = [t for t in per_sample_info if t[1]==1]\n",
    "\n",
    "def summarize(stats):\n",
    "    if not stats: return (0.0,0.0,0.0,0.0)\n",
    "    Ls = np.array([s[2] for s in stats])\n",
    "    Es = np.array([s[3] for s in stats])\n",
    "    return (Ls.mean(), Ls.std(), Es.mean(), Es.std())\n",
    "\n",
    "cL_mean, cL_std, cE_mean, cE_std = summarize(clean_stats)\n",
    "tL_mean, tL_std, tE_mean, tE_std = summarize(troj_stats)\n",
    "\n",
    "print(\"\\nAggregated Hessian curvature stats:\")\n",
    "print(f\" Clean:  avg_lambda={cL_mean:.4f} ± {cL_std:.4f}, avg_FDrel={cE_mean:.4e} ± {cE_std:.4e}\")\n",
    "print(f\" Trojan: avg_lambda={tL_mean:.4f} ± {tL_std:.4f}, avg_FDrel={tE_mean:.4e} ± {tE_std:.4e}\")\n",
    "\n",
    "print(\"\\nSample preview (first 6): (idx,label,lambda,FD_rel_err)\")\n",
    "for p in per_sample_info[:6]:\n",
    "    print(p)\n",
    "\n",
    "evaluate_model(\"Margin\", perturbed_X, selected_nodes)\n",
    "\n",
    "# ---------------- Metric 4: Prediction Margin ----------------\n",
    "margin_info = []\n",
    "for node_idx in selected_nodes:\n",
    "    logits = model(perturbed_X, A_t)[node_idx]\n",
    "    pred_class = logits.argmax().item()\n",
    "    margin = logits[pred_class].item() - logits[[j for j in range(len(logits)) if j!=pred_class]].max().item()\n",
    "    delta = FD_EPS*torch.randn_like(perturbed_X[node_idx])\n",
    "    logits_p = model(perturbed_X.clone().detach(), A_t)[node_idx]\n",
    "    margin_p = logits_p[pred_class].item() - logits_p[[j for j in range(len(logits_p)) if j!=pred_class]].max().item()\n",
    "    rel_err = abs(margin-margin_p)/(abs(margin_p)+1e-12)\n",
    "    margin_info.append((int(labels_np[node_idx]), margin, rel_err))\n",
    "print(\"\\nPrediction Margin:\")\n",
    "for cls in [0,1]:\n",
    "    vals = [j[1] for j in margin_info if j[0]==cls]\n",
    "    errs = [j[2] for j in margin_info if j[0]==cls]\n",
    "    print(f\" Class {cls}: margin={np.mean(vals):.4f}±{np.std(vals):.4f}, relerr={np.mean(errs):.4e}±{np.std(errs):.4e}\")\n",
    "\n",
    "evaluate_model(\"Margin\", perturbed_X, selected_nodes)\n",
    "\n",
    "# ---------------- Metric 5: ARR ----------------\n",
    "# (kept simplified: min perturbation until flip)\n",
    "def adversarial_radius(node_idx):\n",
    "    x0 = perturbed_X[node_idx].detach().clone()\n",
    "    base_pred = int(model(perturbed_X, A_t)[node_idx].argmax().item())\n",
    "    eps, growth = 1e-3, 1.2\n",
    "    while eps < 20:\n",
    "        x_try = x0 + eps*torch.randn_like(x0)\n",
    "        with torch.no_grad():\n",
    "            pred = int(model(perturbed_X.clone().detach(), A_t)[node_idx].argmax().item())\n",
    "        if pred != base_pred: return eps\n",
    "        eps *= growth\n",
    "    return 20.0\n",
    "\n",
    "arr_info = []\n",
    "for n in selected_nodes:\n",
    "    arr_val = adversarial_radius(n)\n",
    "    # finite-difference style perturbation for ARR\n",
    "    delta = FD_EPS * torch.randn_like(perturbed_X[n])\n",
    "    arr_val_p = adversarial_radius(n)  # here you could recompute with perturbed input if desired\n",
    "    rel_err = abs(arr_val - arr_val_p) / (abs(arr_val_p) + 1e-12)\n",
    "    arr_info.append((int(labels_np[n]), arr_val, rel_err))\n",
    "\n",
    "print(\"\\nAdversarial Robustness Radius:\")\n",
    "for cls in [0,1]:\n",
    "    vals = [j[1] for j in arr_info if j[0] == cls]\n",
    "    errs = [j[2] for j in arr_info if j[0] == cls]\n",
    "    print(f\" Class {cls}: radius={np.mean(vals):.4f}±{np.std(vals):.4f}, relerr={np.mean(errs):.4e}±{np.std(errs):.4e}\")\n",
    "\n",
    "evaluate_model(\"ARR\", perturbed_X, selected_nodes)\n",
    "\n",
    "# ---------------- Metric 6: Stability ----------------\n",
    "stability_info = []\n",
    "for node_idx in selected_nodes:\n",
    "    base_logits = model(perturbed_X, A_t)[node_idx].detach()\n",
    "    diffs = []\n",
    "    for _ in range(10):\n",
    "        noise = 0.05 * torch.randn_like(perturbed_X[node_idx])\n",
    "        X_mod = perturbed_X.clone().detach()\n",
    "        X_mod[node_idx] = perturbed_X[node_idx] + noise\n",
    "        with torch.no_grad():\n",
    "            logits_n = model(X_mod, A_t)[node_idx]\n",
    "        diffs.append(torch.norm(logits_n - base_logits).item())\n",
    "    stability_val = np.mean(diffs)\n",
    "    # finite-difference style perturbation for stability\n",
    "    noise_fd = 0.05 * torch.randn_like(perturbed_X[node_idx])\n",
    "    X_fd = perturbed_X.clone().detach()\n",
    "    X_fd[node_idx] = perturbed_X[node_idx] + noise_fd\n",
    "    with torch.no_grad():\n",
    "        logits_fd = model(X_fd, A_t)[node_idx]\n",
    "    diffs_fd = [torch.norm(logits_fd - base_logits).item()]\n",
    "    stability_val_p = np.mean(diffs_fd)\n",
    "    rel_err = abs(stability_val - stability_val_p) / (abs(stability_val_p) + 1e-12)\n",
    "    stability_info.append((int(labels_np[node_idx]), stability_val, rel_err))\n",
    "\n",
    "print(\"\\nStability Under Noise:\")\n",
    "for cls in [0,1]:\n",
    "    vals = [j[1] for j in stability_info if j[0] == cls]\n",
    "    errs = [j[2] for j in stability_info if j[0] == cls]\n",
    "    print(f\" Class {cls}: stability={np.mean(vals):.4f}±{np.std(vals):.4f}, relerr={np.mean(errs):.4e}±{np.std(errs):.4e}\")\n",
    "\n",
    "evaluate_model(\"Stability\", perturbed_X, selected_nodes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe04ca3-b97e-4d31-932a-1a1e7f1c8571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
