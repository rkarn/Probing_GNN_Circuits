{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7dc29d7-5b96-44ea-a9a7-260997a08eff",
   "metadata": {},
   "source": [
    "#### Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ae98cf3-e6db-4a7e-880c-9166ab2b75a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch_cluster/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "/home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch_spline_conv/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(\n",
      "/home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circuits split -> train:64 val:14 test:14\n",
      "Pretraining will use nodes from 78 circuits (train+val) and exclude 14 test circuits.\n",
      "Node pretraining sizes (labeled): train=130706 val=28008\n",
      "Pretrain Epoch 001 | Loss 0.7781 | Val 0.6488\n",
      "Pretrain Epoch 005 | Loss 0.6883 | Val 0.6977\n",
      "Pretrain Epoch 010 | Loss 0.5939 | Val 0.9208\n",
      "Pretrain Epoch 015 | Loss 0.5102 | Val 0.9453\n",
      "Pretrain Epoch 020 | Loss 0.4307 | Val 0.9594\n",
      "Pretrain Epoch 025 | Loss 0.3524 | Val 0.9863\n",
      "Pretrain Epoch 030 | Loss 0.2770 | Val 0.9936\n",
      "Pretrain Epoch 035 | Loss 0.2106 | Val 0.9944\n",
      "Pretrain Epoch 040 | Loss 0.1548 | Val 0.9961\n",
      "Pretrain Epoch 045 | Loss 0.1138 | Val 0.9991\n",
      "Pretrain Epoch 050 | Loss 0.0838 | Val 0.9996\n",
      "Pretrain Epoch 055 | Loss 0.0618 | Val 0.9997\n",
      "Pretrain Epoch 060 | Loss 0.0478 | Val 0.9998\n",
      "Pretrain Epoch 065 | Loss 0.0380 | Val 0.9998\n",
      "Pretrain Epoch 070 | Loss 0.0316 | Val 0.9998\n",
      "Pretrain Epoch 075 | Loss 0.0256 | Val 0.9998\n",
      "Pretrain Epoch 080 | Loss 0.0221 | Val 0.9998\n",
      "Pretrain Epoch 085 | Loss 0.0190 | Val 0.9999\n",
      "Pretrain Epoch 090 | Loss 0.0159 | Val 0.9999\n",
      "Pretrain Epoch 095 | Loss 0.0145 | Val 0.9999\n",
      "Pretrain Epoch 100 | Loss 0.0127 | Val 0.9999\n",
      "Saved node-level model checkpoint: node_gcn_pretrained.pth\n",
      "\n",
      "Building graph-level Data objects for fine-tuning (train/val/test circuits) ...\n",
      "Built 92 graphs.\n",
      "Graph counts -> train: 64, val: 14, test: 14\n",
      "\n",
      "Stage 1: freeze encoder and train classifier head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115853/3650526264.py:308: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  node_state = torch.load(PRETRAIN_CHECKPOINT, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune (frozen) Epoch 001 | Val Acc 1.0000 | AvgLoss 0.5945\n",
      "Fine-tune (frozen) Epoch 005 | Val Acc 1.0000 | AvgLoss 0.4577\n",
      "Fine-tune (frozen) Epoch 010 | Val Acc 1.0000 | AvgLoss 0.2530\n",
      "Fine-tune (frozen) Epoch 015 | Val Acc 1.0000 | AvgLoss 0.1850\n",
      "Fine-tune (frozen) Epoch 020 | Val Acc 1.0000 | AvgLoss 0.1366\n",
      "Fine-tune (frozen) Epoch 025 | Val Acc 1.0000 | AvgLoss 0.0988\n",
      "Fine-tune (frozen) Epoch 030 | Val Acc 1.0000 | AvgLoss 0.1019\n",
      "Early stopping fine-tune stage.\n",
      "After head training -> Test Acc: 0.9286\n",
      "\n",
      "Stage 2: unfreeze encoder and fine-tune entire model\n",
      "Fine-tune (all) Epoch 001 | Val Acc 1.0000 | AvgLoss 0.5179\n",
      "Fine-tune (all) Epoch 005 | Val Acc 1.0000 | AvgLoss 0.3262\n",
      "Fine-tune (all) Epoch 010 | Val Acc 1.0000 | AvgLoss 0.2033\n",
      "Fine-tune (all) Epoch 015 | Val Acc 1.0000 | AvgLoss 0.1076\n",
      "Fine-tune (all) Epoch 020 | Val Acc 1.0000 | AvgLoss 0.0639\n",
      "Fine-tune (all) Epoch 025 | Val Acc 1.0000 | AvgLoss 0.0353\n",
      "Fine-tune (all) Epoch 030 | Val Acc 1.0000 | AvgLoss 0.0246\n",
      "Early stopping fine-tune stage.\n",
      "After full fine-tune -> Test Acc: 0.9286\n",
      "\n",
      "Final Evaluation (Graph-level after transfer)\n",
      "=============================================\n",
      "Test Accuracy: 0.9286\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.7500    0.8571         4\n",
      "           1     0.9091    1.0000    0.9524        10\n",
      "\n",
      "    accuracy                         0.9286        14\n",
      "   macro avg     0.9545    0.8750    0.9048        14\n",
      "weighted avg     0.9351    0.9286    0.9252        14\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 3  1]\n",
      " [ 0 10]]\n"
     ]
    }
   ],
   "source": [
    "import os, random, math\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "# ----------------- Config -----------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NODE_CSV = \"GNNDatasets/node.csv\"\n",
    "NODE_EDGE_CSV = \"GNNDatasets/node_edges.csv\"\n",
    "GRAPH_CSV = \"GNNDatasets/graph.csv\"\n",
    "GRAPH_EDGE_CSV = \"GNNDatasets/graph_edges.csv\"\n",
    "\n",
    "PRETRAIN_CHECKPOINT = \"node_gcn_pretrained.pth\"\n",
    "\n",
    "# Hyperparams (tweakable)\n",
    "HID_DIM = 64\n",
    "PRETRAIN_LR = 1e-3\n",
    "PRETRAIN_EPOCHS = 100\n",
    "PRETRAIN_BATCH = None   # full-graph pretraining uses adjacency; we do whole-graph training (no batch)\n",
    "\n",
    "FT_HEAD_LR = 1e-3\n",
    "FT_FINETUNE_LR = 5e-4\n",
    "FT_EPOCHS_HEAD = 60\n",
    "FT_EPOCHS_FINETUNE = 120\n",
    "BATCH_SIZE = 16\n",
    "EARLY_STOPPING = 30\n",
    "DROPOUT = 0.35\n",
    "\n",
    "# ----------------- Load graph-level labels and split circuits -----------------\n",
    "graph_df = pd.read_csv(GRAPH_CSV)\n",
    "# find graph label column\n",
    "graph_label_col = None\n",
    "for cand in [\"label_graph\", \"label\", \"is_trojan\", \"trojan\"]:\n",
    "    if cand in graph_df.columns:\n",
    "        graph_label_col = cand; break\n",
    "if graph_label_col is None:\n",
    "    graph_df[\"label_graph\"] = graph_df[\"circuit_name\"].astype(str).str.contains(\"__trojan_\").astype(int)\n",
    "    graph_label_col = \"label_graph\"\n",
    "\n",
    "circuits = graph_df[\"circuit_name\"].tolist()\n",
    "graph_labels = [int(x) for x in graph_df[graph_label_col].tolist()]\n",
    "\n",
    "# stratified split of circuits for final graph-level evaluation\n",
    "train_circuits, temp_circuits, y_train_c, y_temp_c = train_test_split(\n",
    "    circuits, graph_labels, test_size=0.30, random_state=SEED, stratify=graph_labels\n",
    ")\n",
    "val_circuits, test_circuits, y_val_c, y_test_c = train_test_split(\n",
    "    temp_circuits, y_temp_c, test_size=0.50, random_state=SEED, stratify=y_temp_c\n",
    ")\n",
    "\n",
    "print(f\"Circuits split -> train:{len(train_circuits)} val:{len(val_circuits)} test:{len(test_circuits)}\")\n",
    "\n",
    "# ----------------- Prepare node data for pretraining (exclude test circuits!) -----------------\n",
    "nodes_df = pd.read_csv(NODE_CSV)\n",
    "edges_df = pd.read_csv(NODE_EDGE_CSV)\n",
    "\n",
    "# identify node label column\n",
    "node_label_col = None\n",
    "for cand in [\"label\", \"is_trojan\", \"trojan\", \"target\"]:\n",
    "    if cand in nodes_df.columns:\n",
    "        node_label_col = cand; break\n",
    "if node_label_col is None:\n",
    "    nodes_df[\"label\"] = nodes_df[\"circuit_name\"].astype(str).str.contains(\"__trojan_\").astype(int)\n",
    "    node_label_col = \"label\"\n",
    "\n",
    "nodes_df[\"uid\"] = nodes_df[\"circuit_name\"].astype(str) + \"::\" + nodes_df[\"node\"].astype(str)\n",
    "\n",
    "# Pretrain set circuits: combine train + val circuits (we exclude graph test circuits)\n",
    "pretrain_circuits = set(train_circuits + val_circuits)\n",
    "print(f\"Pretraining will use nodes from {len(pretrain_circuits)} circuits (train+val) and exclude {len(test_circuits)} test circuits.\")\n",
    "\n",
    "# Filter nodes/edges for pretraining graph\n",
    "nodes_pretrain_df = nodes_df[nodes_df[\"circuit_name\"].isin(pretrain_circuits)].reset_index(drop=True)\n",
    "edges_pretrain_df = edges_df[edges_df[\"circuit_name\"].isin(pretrain_circuits)].reset_index(drop=True)\n",
    "\n",
    "# feature columns (numeric)  exclude metadata\n",
    "feat_df = nodes_pretrain_df.copy()\n",
    "if \"gate_type\" in feat_df.columns:\n",
    "    # one-hot encode gate_type\n",
    "    gate_oh = pd.get_dummies(feat_df[\"gate_type\"], prefix=\"gt\")\n",
    "    feat_df = pd.concat([feat_df.drop(columns=[\"gate_type\"]), gate_oh], axis=1)\n",
    "\n",
    "exclude = {\"uid\",\"node\",\"circuit_name\", node_label_col}\n",
    "feature_cols = [c for c in feat_df.columns if c not in exclude and pd.api.types.is_numeric_dtype(feat_df[c])]\n",
    "if len(feature_cols) == 0:\n",
    "    raise SystemExit(\"No numeric feature columns found in node CSV. Please include features.\")\n",
    "\n",
    "# Build X_all for pretraining nodes and mapping\n",
    "X_pre = feat_df[feature_cols].fillna(0.0).to_numpy(dtype=np.float32)\n",
    "y_pre = nodes_pretrain_df[node_label_col].to_numpy(dtype=np.int64)\n",
    "uids_pre = nodes_pretrain_df[\"uid\"].tolist()\n",
    "uid_to_idx_pre = {u: i for i,u in enumerate(uids_pre)}\n",
    "\n",
    "# Some edges may include nodes not present in nodes_pretrain_df (rare)  filter edges\n",
    "def map_uid(signal, circuit):\n",
    "    return f\"{circuit}::{signal}\"\n",
    "\n",
    "edge_src_uids = edges_pretrain_df.apply(lambda r: map_uid(r[\"src\"], r[\"circuit_name\"]), axis=1)\n",
    "edge_dst_uids = edges_pretrain_df.apply(lambda r: map_uid(r[\"dst\"], r[\"circuit_name\"]), axis=1)\n",
    "\n",
    "edge_src_idx = edge_src_uids.map(uid_to_idx_pre).dropna().astype(int).values\n",
    "edge_dst_idx = edge_dst_uids.map(uid_to_idx_pre).dropna().astype(int).values\n",
    "\n",
    "if len(edge_src_idx) == 0:\n",
    "    raise SystemExit(\"No edges left after filtering to pretrain circuits; check your GNNDatasets files.\")\n",
    "\n",
    "edge_index_pre = np.stack([np.concatenate([edge_src_idx, edge_dst_idx]),\n",
    "                           np.concatenate([edge_dst_idx, edge_src_idx])], axis=0)  # undirected\n",
    "\n",
    "# Scale features using labeled nodes only (within pretrain set)\n",
    "# ensure labels are available\n",
    "labeled_mask_pre = (y_pre >= 0)\n",
    "scaler = StandardScaler()\n",
    "X_pre_scaled = X_pre.copy()\n",
    "if labeled_mask_pre.sum() == 0:\n",
    "    raise SystemExit(\"No labeled nodes in pretraining set.\")\n",
    "X_pre_scaled[labeled_mask_pre] = scaler.fit_transform(X_pre_scaled[labeled_mask_pre])\n",
    "X_pre_scaled[~labeled_mask_pre] = (X_pre_scaled[~labeled_mask_pre] - scaler.mean_) / np.sqrt(scaler.var_ + 1e-8)\n",
    "\n",
    "# Convert to torch\n",
    "X_pre_t = torch.from_numpy(X_pre_scaled).to(DEVICE)\n",
    "y_pre_t = torch.from_numpy(y_pre).to(DEVICE)\n",
    "edge_index_pre_t = torch.from_numpy(edge_index_pre).long().to(DEVICE)\n",
    "\n",
    "# Create train/val split (node-level) for early stopping on pretraining\n",
    "idx_nodes = np.where(labeled_mask_pre)[0]\n",
    "y_nodes = y_pre[labeled_mask_pre]\n",
    "n_train_nodes, n_tmp_nodes = train_test_split(idx_nodes, test_size=0.30, random_state=SEED, stratify=y_nodes)\n",
    "n_val_nodes, n_test_nodes = train_test_split(n_tmp_nodes, test_size=0.50, random_state=SEED,\n",
    "                                             stratify=y_pre[n_tmp_nodes])\n",
    "\n",
    "train_mask_nodes = torch.zeros(len(y_pre), dtype=torch.bool, device=DEVICE); train_mask_nodes[n_train_nodes] = True\n",
    "val_mask_nodes   = torch.zeros(len(y_pre), dtype=torch.bool, device=DEVICE);   val_mask_nodes[n_val_nodes] = True\n",
    "# note: we won't use node_test_nodes later\n",
    "\n",
    "# ----------------- Node GCN pretraining (PyG-style conv) -----------------\n",
    "class NodeGCN(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=HID_DIM, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hid_dim)\n",
    "        self.conv2 = GCNConv(hid_dim, hid_dim)\n",
    "        self.head = nn.Linear(hid_dim, 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return self.head(x)  # per-node logits\n",
    "\n",
    "node_model = NodeGCN(in_dim=X_pre_t.shape[1], hid_dim=HID_DIM).to(DEVICE)\n",
    "\n",
    "# class weights on node-level training labels\n",
    "train_labels_nodes = y_pre_t[train_mask_nodes]\n",
    "classes, counts = torch.unique(train_labels_nodes, return_counts=True)\n",
    "num_pos = int((train_labels_nodes==1).sum().item()) if (train_labels_nodes==1).any() else 1\n",
    "num_neg = int((train_labels_nodes==0).sum().item()) if (train_labels_nodes==0).any() else 1\n",
    "w_pos = (num_neg + num_pos) / (2.0 * num_pos)\n",
    "w_neg = (num_neg + num_pos) / (2.0 * num_neg)\n",
    "class_weights_nodes = torch.tensor([w_neg, w_pos], dtype=torch.float32, device=DEVICE)\n",
    "crit_node = nn.CrossEntropyLoss(weight=class_weights_nodes)\n",
    "opt_node = torch.optim.Adam(node_model.parameters(), lr=PRETRAIN_LR, weight_decay=5e-4)\n",
    "\n",
    "# Pretraining loop\n",
    "best_val = -1.0; best_state = None; patience_cnt = 0\n",
    "print(\"Node pretraining sizes (labeled): train=%d val=%d\" % (train_mask_nodes.sum().item(), val_mask_nodes.sum().item()))\n",
    "for epoch in range(1, PRETRAIN_EPOCHS+1):\n",
    "    node_model.train()\n",
    "    opt_node.zero_grad()\n",
    "    logits_nodes = node_model(X_pre_t, edge_index_pre_t)\n",
    "    loss = crit_node(logits_nodes[train_mask_nodes], y_pre_t[train_mask_nodes])\n",
    "    loss.backward()\n",
    "    opt_node.step()\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        node_model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits_val = node_model(X_pre_t, edge_index_pre_t)\n",
    "            preds_val = logits_val.argmax(dim=1)\n",
    "            if val_mask_nodes.sum() > 0:\n",
    "                val_acc = (preds_val[val_mask_nodes] == y_pre_t[val_mask_nodes]).float().mean().item()\n",
    "            else:\n",
    "                val_acc = 0.0\n",
    "        print(f\"Pretrain Epoch {epoch:03d} | Loss {loss.item():.4f} | Val {val_acc:.4f}\")\n",
    "        if val_acc > best_val + 1e-5:\n",
    "            best_val = val_acc\n",
    "            best_state = {k:v.detach().cpu().clone() for k,v in node_model.state_dict().items()}\n",
    "            patience_cnt = 0\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            if patience_cnt >= EARLY_STOPPING:\n",
    "                print(\"Early stopping node pretrain.\")\n",
    "                break\n",
    "\n",
    "if best_state is not None:\n",
    "    node_model.load_state_dict(best_state)\n",
    "torch.save(node_model.state_dict(), PRETRAIN_CHECKPOINT)\n",
    "print(\"Saved node-level model checkpoint:\", PRETRAIN_CHECKPOINT)\n",
    "\n",
    "# ----------------- Build graph-level dataset (per-circuit Data objects) -----------------\n",
    "print(\"\\nBuilding graph-level Data objects for fine-tuning (train/val/test circuits) ...\")\n",
    "# load full nodes/edges for graphs (use nodes_df, edges_df from earlier)\n",
    "nodes_full_df = pd.read_csv(NODE_CSV)\n",
    "edges_full_df = pd.read_csv(GRAPH_EDGE_CSV)\n",
    "\n",
    "# Prepare feature scaler: use the scaler fitted during pretraining (we already have StandardScaler scaler)\n",
    "# For nodes not seen in pretrain set, we'll apply same scaler transform using scaler.mean_/var_\n",
    "def node_uid(circuit, node):\n",
    "    return f\"{circuit}::{node}\"\n",
    "\n",
    "# build uid->feature map for all nodes (apply scaler to full dataset)\n",
    "feat_full_df = nodes_full_df.copy()\n",
    "if \"gate_type\" in feat_full_df.columns:\n",
    "    gate_oh = pd.get_dummies(feat_full_df[\"gate_type\"], prefix=\"gt\")\n",
    "    feat_full_df = pd.concat([feat_full_df.drop(columns=[\"gate_type\"]), gate_oh], axis=1)\n",
    "# ensure feature columns compatible: if full has extra gate dummies or missing ones compared to pretrain, align\n",
    "for col in feature_cols:\n",
    "    if col not in feat_full_df.columns:\n",
    "        feat_full_df[col] = 0.0\n",
    "feat_full_df = feat_full_df[[\"circuit_name\",\"node\"] + feature_cols]\n",
    "\n",
    "# scale using pretrain scaler (note: scaler was fitted on pretrain labeled nodes' features)\n",
    "full_X = feat_full_df[feature_cols].fillna(0.0).to_numpy(dtype=np.float32)\n",
    "full_X_scaled = (full_X - scaler.mean_) / np.sqrt(scaler.var_ + 1e-8)  # consistent transform\n",
    "feat_full_df[\"scaled_feat\"] = list(full_X_scaled.tolist())\n",
    "\n",
    "# build edges by circuit\n",
    "edges_full_df[\"src_uid\"] = edges_full_df[\"circuit_name\"].astype(str) + \"::\" + edges_full_df[\"src\"].astype(str)\n",
    "edges_full_df[\"dst_uid\"] = edges_full_df[\"circuit_name\"].astype(str) + \"::\" + edges_full_df[\"dst\"].astype(str)\n",
    "edges_by_circuit = defaultdict(list)\n",
    "for _, r in edges_full_df.iterrows():\n",
    "    edges_by_circuit[r[\"circuit_name\"]].append((r[\"src\"], r[\"dst\"]))\n",
    "\n",
    "# build per-circuit Data (only circuits present in graph_labels)\n",
    "graph_data_list = []\n",
    "graph_names = []\n",
    "graph_target = []\n",
    "for _, row in graph_df.iterrows():\n",
    "    ckt = row[\"circuit_name\"]\n",
    "    lbl = int(row[graph_label_col])\n",
    "    # nodes of this circuit\n",
    "    sub_nodes = feat_full_df[feat_full_df[\"circuit_name\"]==ckt]\n",
    "    if sub_nodes.shape[0] == 0: \n",
    "        continue\n",
    "    node_names = sub_nodes[\"node\"].tolist()\n",
    "    uid_map = {n:i for i,n in enumerate(node_names)}\n",
    "    X_nodes = np.vstack(sub_nodes[\"scaled_feat\"].values).astype(np.float32)\n",
    "    # build edge_index\n",
    "    srcs, dsts = [], []\n",
    "    if ckt in edges_by_circuit:\n",
    "        for u,v in edges_by_circuit[ckt]:\n",
    "            if u in uid_map and v in uid_map:\n",
    "                srcs.extend([uid_map[u], uid_map[v]])\n",
    "                dsts.extend([uid_map[v], uid_map[u]])\n",
    "    if len(srcs) == 0:\n",
    "        # skip graphs without edges (unlikely)\n",
    "        continue\n",
    "    edge_index = torch.tensor([srcs, dsts], dtype=torch.long)\n",
    "    data = Data(x=torch.tensor(X_nodes, dtype=torch.float), edge_index=edge_index, y=torch.tensor([lbl], dtype=torch.long))\n",
    "    data.circuit_name = ckt\n",
    "    graph_data_list.append(data)\n",
    "    graph_names.append(ckt)\n",
    "    graph_target.append(lbl)\n",
    "\n",
    "print(f\"Built {len(graph_data_list)} graphs.\")\n",
    "\n",
    "# Create train/val/test lists by circuit split we made earlier\n",
    "def filter_by_circuit(list_data, circuits_set):\n",
    "    idxs = [i for i,d in enumerate(list_data) if d.circuit_name in circuits_set]\n",
    "    return [list_data[i] for i in idxs]\n",
    "\n",
    "train_graphs = filter_by_circuit(graph_data_list, set(train_circuits))\n",
    "val_graphs   = filter_by_circuit(graph_data_list, set(val_circuits))\n",
    "test_graphs  = filter_by_circuit(graph_data_list, set(test_circuits))\n",
    "\n",
    "print(f\"Graph counts -> train: {len(train_graphs)}, val: {len(val_graphs)}, test: {len(test_graphs)}\")\n",
    "\n",
    "# ----------------- Graph classifier reusing conv layers from node_model -----------------\n",
    "class GraphClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=HID_DIM, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hid_dim)\n",
    "        self.conv2 = GCNConv(hid_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hid_dim, 2)\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = F.relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h, edge_index)\n",
    "        g = global_mean_pool(h, batch)\n",
    "        return self.classifier(g)\n",
    "\n",
    "graph_model = GraphClassifier(in_dim=X_pre_t.shape[1], hid_dim=HID_DIM).to(DEVICE)\n",
    "\n",
    "# Copy conv weights from node_model -> graph_model (shapes align because both use GCNConv)\n",
    "node_state = torch.load(PRETRAIN_CHECKPOINT, map_location=\"cpu\")\n",
    "# node_state contains keys: conv1.lin.weight, conv1.lin.bias? Check keys\n",
    "for k_src, v_src in node_state.items():\n",
    "    if \"conv1\" in k_src and \"weight\" in k_src:\n",
    "        # copy to graph_model conv1 weight if exists\n",
    "        if k_src in graph_model.state_dict() and graph_model.state_dict()[k_src].shape == v_src.shape:\n",
    "            graph_model.state_dict()[k_src].copy_(v_src)\n",
    "    if \"conv2\" in k_src and \"weight\" in k_src:\n",
    "        if k_src in graph_model.state_dict() and graph_model.state_dict()[k_src].shape == v_src.shape:\n",
    "            graph_model.state_dict()[k_src].copy_(v_src)\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# --- replace the function definition (rename it) ---\n",
    "def train_graphs_fn(model, train_set, val_set, test_set, freeze_encoder=True):\n",
    "    # dataloaders\n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    if freeze_encoder:\n",
    "        for p in model.conv1.parameters(): p.requires_grad = False\n",
    "        for p in model.conv2.parameters(): p.requires_grad = False\n",
    "    else:\n",
    "        for p in model.parameters(): p.requires_grad = True\n",
    "\n",
    "    # class weights\n",
    "    ytrain = np.array([int(d.y.item()) for d in train_set])\n",
    "    if len(np.unique(ytrain)) == 2:\n",
    "        counts = np.bincount(ytrain); w = torch.tensor([ (counts.sum()/counts[0]), (counts.sum()/counts[1]) ], dtype=torch.float32).to(DEVICE)\n",
    "        criterion = nn.CrossEntropyLoss(weight=w)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=FT_HEAD_LR if freeze_encoder else FT_FINETUNE_LR, weight_decay=5e-4)\n",
    "    best_val = -1.0; best_state = None; pcount = 0\n",
    "\n",
    "    for epoch in range(1, FT_EPOCHS_HEAD + 1 if freeze_encoder else FT_EPOCHS_FINETUNE + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch.x, batch.edge_index, batch.batch)\n",
    "            loss = criterion(logits, batch.y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        ys, ps = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(DEVICE)\n",
    "                out = model(batch.x, batch.edge_index, batch.batch)\n",
    "                preds = out.argmax(dim=1)\n",
    "                ys.extend(batch.y.cpu().numpy()); ps.extend(preds.cpu().numpy())\n",
    "        val_acc = accuracy_score(ys, ps) if len(ys)>0 else 0.0\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(f\"Fine-tune ({'frozen' if freeze_encoder else 'all'}) Epoch {epoch:03d} | Val Acc {val_acc:.4f} | AvgLoss {total_loss / max(1,len(train_set)):.4f}\")\n",
    "        if val_acc > best_val + 1e-4:\n",
    "            best_val = val_acc\n",
    "            best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "            pcount = 0\n",
    "        else:\n",
    "            pcount += 1\n",
    "            if pcount >= EARLY_STOPPING:\n",
    "                print(\"Early stopping fine-tune stage.\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # test eval\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            out = model(batch.x, batch.edge_index, batch.batch)\n",
    "            preds = out.argmax(dim=1)\n",
    "            ys.extend(batch.y.cpu().numpy()); ps.extend(preds.cpu().numpy())\n",
    "    test_acc = accuracy_score(ys, ps) if len(ys)>0 else 0.0\n",
    "    return test_acc, ys, ps\n",
    "\n",
    "# Stage 1: freeze encoder, train head\n",
    "print(\"\\nStage 1: freeze encoder and train classifier head\")\n",
    "for p in graph_model.conv1.parameters(): p.requires_grad = False\n",
    "for p in graph_model.conv2.parameters(): p.requires_grad = False\n",
    "# Stage 1 call:\n",
    "test_acc_1, ys1, ps1 = train_graphs_fn(graph_model, train_graphs, val_graphs, test_graphs, freeze_encoder=True)\n",
    "print(\"After head training -> Test Acc: {:.4f}\".format(test_acc_1))\n",
    "\n",
    "# Stage 2: unfreeze and fine-tune all (lower lr)\n",
    "print(\"\\nStage 2: unfreeze encoder and fine-tune entire model\")\n",
    "for p in graph_model.parameters(): p.requires_grad = True\n",
    "# update global lr for finetune\n",
    "FT_HEAD_LR = FT_FINETUNE_LR\n",
    "# Stage 2 call:\n",
    "test_acc_2, ys2, ps2 = train_graphs_fn(graph_model, train_graphs, val_graphs, test_graphs, freeze_encoder=False)\n",
    "print(\"After full fine-tune -> Test Acc: {:.4f}\".format(test_acc_2))\n",
    "\n",
    "# Final report\n",
    "final_ys, final_ps = (ys2, ps2) if len(ys2)>0 else (ys1, ps1)\n",
    "print(\"\\nFinal Evaluation (Graph-level after transfer)\")\n",
    "print(\"=============================================\")\n",
    "print(f\"Test Accuracy: {accuracy_score(final_ys, final_ps):.4f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(final_ys, final_ps, digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(final_ys, final_ps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d457b9a0-8cfa-4866-8441-20df386f4e83",
   "metadata": {},
   "source": [
    "#### Jacobain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38ebd150-af93-4a07-a6ad-de4dc701b036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test graphs: 14 | class counts = [ 4 10]\n",
      "Selected to perturb: {0: 4, 1: 10}\n",
      "\n",
      "Running PGD on selected graphs (strong settings)...\n",
      "? PGD finished.\n",
      "\n",
      "============= Robustness Evaluation (Full Test Set: perturbed selected + rest original) =============\n",
      "Accuracy: 71.43%\n",
      "Precision: 0.6786, Recall: 0.7143, F1: 0.6797\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     0.5000    0.2500    0.3333         4\n",
      "      trojan     0.7500    0.9000    0.8182        10\n",
      "\n",
      "    accuracy                         0.7143        14\n",
      "   macro avg     0.6250    0.5750    0.5758        14\n",
      "weighted avg     0.6786    0.7143    0.6797        14\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1 3]\n",
      " [1 9]]\n",
      "\n",
      "Selected graphs: 14. Flipped after attack: 3 (21.43%).\n",
      "\n",
      "Computing Jacobian norms & FD relative error at the PERTURBED graphs...\n",
      "\n",
      "--- Jacobian Frobenius Norms & FD Relative Errors (Graph-level) ---\n",
      " Clean graphs :  avg_norm=0.0178 ± 0.0071, avg_FDrel=6.4374e-01 ± 4.7658e-01\n",
      " Trojan graphs:  avg_norm=0.0210 ± 0.0291, avg_FDrel=1.2029e+00 ± 7.0398e-01\n",
      "\n",
      "Sample preview (first 6): (idx,label,jacobian_frob,FD_rel_err)\n",
      "(0, 0, 0.02333747036755085, 0.040569789707660675)\n",
      "(1, 0, 0.019794734194874763, 0.7364052534103394)\n",
      "(2, 0, 0.022423483431339264, 0.4485333263874054)\n",
      "(3, 0, 0.005778668448328972, 1.3494393825531006)\n",
      "(4, 1, 0.003849786939099431, 0.9224197864532471)\n",
      "(5, 1, 0.017460467293858528, 0.5983080267906189)\n",
      "\n",
      "Done. (Order: PGD -> full-test evaluation -> Jacobian @ perturbed inputs.)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Graph-level: PGD perturbation -> evaluation -> Jacobian+FD\n",
    "# ============================================================\n",
    "import torch, numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# ---------------------- PARAMETERS (tuned to induce large shifts) ----------------------\n",
    "PER_CLASS = 250        # graphs per class to perturb (adjust based on your class counts)\n",
    "EPSILON   = 20.0       # L2 radius on flattened node-feature tensor (strong)\n",
    "ALPHA     = 10.2       # PGD step size (normalized by grad norm)\n",
    "NUM_ITERS = 100        # PGD iterations\n",
    "FD_EPS    = 1e-1      # finite-difference epsilon for Jacobian check\n",
    "SEED      = 42\n",
    "\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# ---------------------- Sanity checks & setup ----------------------\n",
    "required = [\"graph_model\", \"test_graphs\", \"DEVICE\"]\n",
    "missing = [v for v in required if v not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing required variables in notebook: {missing}. \"\n",
    "                       \"Run your training cell first to define graph_model/test_graphs/DEVICE.\")\n",
    "\n",
    "model = graph_model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# We'll reuse BATCH_SIZE if present; otherwise default small batch for eval loaders\n",
    "BATCH_EVAL = globals().get(\"BATCH_SIZE\", 16)\n",
    "\n",
    "# Freeze randomness for selection\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# ---------------------- Build a stable list (order) of test graphs ----------------------\n",
    "test_list = list(test_graphs)   # preserve order\n",
    "test_labels = np.array([int(d.y.item()) for d in test_list])\n",
    "print(f\"Test graphs: {len(test_list)} | class counts = {np.bincount(test_labels) if len(test_labels)>0 else '[]'}\")\n",
    "\n",
    "# ---------------------- Select graphs to perturb (PER_CLASS per class) ----------------------\n",
    "selected_idxs = []\n",
    "for cls in [0, 1]:\n",
    "    idxs = np.where(test_labels == cls)[0]\n",
    "    if len(idxs) == 0:\n",
    "        continue\n",
    "    k = min(PER_CLASS, len(idxs))\n",
    "    chosen = rng.choice(idxs, size=k, replace=False)\n",
    "    selected_idxs.extend(chosen.tolist())\n",
    "selected_idxs = np.array(sorted(selected_idxs), dtype=np.int64)\n",
    "print(\"Selected to perturb:\", {0:int((test_labels[selected_idxs]==0).sum()),\n",
    "                               1:int((test_labels[selected_idxs]==1).sum())})\n",
    "\n",
    "# ---------------------- PGD perturbation for selected graphs ----------------------\n",
    "perturbed_map = {}  # idx -> perturbed node-feature matrix (Tensor on DEVICE)\n",
    "print(\"\\nRunning PGD on selected graphs (strong settings)...\")\n",
    "\n",
    "for idx in selected_idxs:\n",
    "    data = test_list[int(idx)]\n",
    "    x_orig = data.x.detach().to(DEVICE)                 # [N, F]\n",
    "    edge_index = data.edge_index.to(DEVICE)\n",
    "    n_nodes, feat_dim = x_orig.shape\n",
    "    batch_zero = torch.zeros(n_nodes, dtype=torch.long, device=DEVICE)\n",
    "    y_true = data.y.view(-1).to(DEVICE)                 # shape [1]\n",
    "\n",
    "    # init random direction with L2 = EPSILON on the flattened x\n",
    "    delta = torch.randn_like(x_orig, device=DEVICE)\n",
    "    delta = delta * (EPSILON / (delta.view(-1).norm() + 1e-12))\n",
    "    x_adv = (x_orig + delta).detach().clone().requires_grad_(True)\n",
    "\n",
    "    for _ in range(NUM_ITERS):\n",
    "        out = model(x_adv, edge_index, batch_zero)      # [1, C]\n",
    "        loss = F.cross_entropy(out, y_true)             # targeted to true label; pushes away from correct\n",
    "        grad_x = torch.autograd.grad(loss, x_adv, retain_graph=False, create_graph=False)[0]\n",
    "\n",
    "        gnorm = grad_x.view(-1).norm().item()\n",
    "        if gnorm == 0:\n",
    "            break\n",
    "        step = (ALPHA * grad_x) / (gnorm + 1e-12)\n",
    "        x_adv = (x_adv + step).detach()\n",
    "\n",
    "        # Project back to L2 ball around x_orig\n",
    "        delta = x_adv - x_orig\n",
    "        dnorm = delta.view(-1).norm().item()\n",
    "        if dnorm > EPSILON:\n",
    "            delta = delta * (EPSILON / (dnorm + 1e-12))\n",
    "            x_adv = (x_orig + delta).detach()\n",
    "        x_adv = x_adv.requires_grad_(True)\n",
    "\n",
    "    perturbed_map[int(idx)] = x_adv.detach().clone()\n",
    "\n",
    "print(\"? PGD finished.\")\n",
    "\n",
    "# ---------------------- Full test evaluation (perturbed selected + originals) ----------------------\n",
    "y_true_list, y_pred_list = [], []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_list):\n",
    "        x_in = perturbed_map[i] if i in perturbed_map else data.x.to(DEVICE)\n",
    "        edge_index = data.edge_index.to(DEVICE)\n",
    "        batch_zero = torch.zeros(x_in.size(0), dtype=torch.long, device=DEVICE)\n",
    "        out = model(x_in, edge_index, batch_zero)          # [1, C]\n",
    "        pred = int(out.argmax(dim=1).item())\n",
    "        y_pred_list.append(pred)\n",
    "        y_true_list.append(int(data.y.item()))\n",
    "\n",
    "y_true_arr = np.array(y_true_list)\n",
    "y_pred_arr = np.array(y_pred_list)\n",
    "\n",
    "print(\"\\n============= Robustness Evaluation (Full Test Set: perturbed selected + rest original) =============\")\n",
    "acc = float((y_true_arr == y_pred_arr).mean())\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true_arr, y_pred_arr, average='weighted', zero_division=0)\n",
    "print(f\"Accuracy: {acc*100:.2f}%\")\n",
    "print(f\"Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\\n\")\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_true_arr, y_pred_arr, labels=[0,1], target_names=['clean','trojan'], digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true_arr, y_pred_arr, labels=[0,1]))\n",
    "\n",
    "# Flip statistics for the selected set\n",
    "orig_sel_preds, adv_sel_preds = [], []\n",
    "with torch.no_grad():\n",
    "    for idx in selected_idxs:\n",
    "        d = test_list[int(idx)]\n",
    "        ez = torch.zeros(d.x.size(0), dtype=torch.long, device=DEVICE)\n",
    "        po = model(d.x.to(DEVICE), d.edge_index.to(DEVICE), ez).argmax(dim=1).item()\n",
    "        pa = model(perturbed_map[int(idx)], d.edge_index.to(DEVICE), ez).argmax(dim=1).item()\n",
    "        orig_sel_preds.append(int(po)); adv_sel_preds.append(int(pa))\n",
    "orig_sel_preds = np.array(orig_sel_preds); adv_sel_preds = np.array(adv_sel_preds)\n",
    "num_flips = int((orig_sel_preds != adv_sel_preds).sum())\n",
    "print(f\"\\nSelected graphs: {len(selected_idxs)}. Flipped after attack: {num_flips} ({100.0*num_flips/len(selected_idxs):.2f}%).\")\n",
    "\n",
    "# ---------------------- Jacobian & FD relative error at PERTURBED graphs ----------------------\n",
    "print(\"\\nComputing Jacobian norms & FD relative error at the PERTURBED graphs...\")\n",
    "per_sample_info = []   # (idx, label, jacobian_fro_norm, fd_rel_err)\n",
    "\n",
    "for idx in selected_idxs:\n",
    "    d = test_list[int(idx)]\n",
    "    x_adv = perturbed_map[int(idx)].detach().clone().to(DEVICE)  # [N,F]\n",
    "    x_adv = x_adv.requires_grad_(True)\n",
    "    edge_index = d.edge_index.to(DEVICE)\n",
    "    n_nodes, feat_dim = x_adv.shape\n",
    "    batch_zero = torch.zeros(n_nodes, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    # flatten node features -> vector\n",
    "    x_flat = x_adv.view(-1).detach().clone().requires_grad_(True)\n",
    "\n",
    "    def f_flat(z):\n",
    "        x_mat = z.view_as(x_adv)\n",
    "        out = model(x_mat, edge_index, batch_zero)   # [1, C]\n",
    "        return out.squeeze(0)                        # (C,)\n",
    "\n",
    "    # Compute Jacobian J \\in R^{C x D} where D = N*F\n",
    "    try:\n",
    "        J = torch.autograd.functional.jacobian(f_flat, x_flat)  # (C, D)\n",
    "    except RuntimeError:\n",
    "        # Row-by-row fallback (rarely needed)\n",
    "        C = int(model(x_adv, edge_index, batch_zero).shape[1])\n",
    "        rows = []\n",
    "        for out_i in range(C):\n",
    "            def scalar_f(z, i=out_i): return f_flat(z)[i]\n",
    "            r = torch.autograd.functional.jacobian(scalar_f, x_flat)  # (D,)\n",
    "            rows.append(r.unsqueeze(0))\n",
    "        J = torch.cat(rows, dim=0)\n",
    "\n",
    "    J = J.detach()\n",
    "    jac_frob = float(torch.norm(J, p='fro').item())\n",
    "\n",
    "    # FD relative error at the perturbed point\n",
    "    delta_fd = FD_EPS * torch.randn_like(x_flat).to(DEVICE)\n",
    "    pred_change = J @ delta_fd                          # (C,)\n",
    "    f0  = f_flat(x_flat).detach()\n",
    "    f0p = f_flat(x_flat + delta_fd).detach()\n",
    "    actual_change = f0p - f0\n",
    "    rel_err = float((torch.norm(pred_change - actual_change) /\n",
    "                    (torch.norm(actual_change) + 1e-8)).item())\n",
    "\n",
    "    per_sample_info.append((int(idx), int(d.y.item()), jac_frob, rel_err))\n",
    "\n",
    "# ---------------------- Aggregate & print ----------------------\n",
    "if len(per_sample_info) > 0:\n",
    "    arr = np.array(per_sample_info, dtype=object)  # columns: idx, label, J_frob, FD_rel\n",
    "    clean_mask = (arr[:,1] == 0)\n",
    "    troj_mask  = (arr[:,1] == 1)\n",
    "\n",
    "    def mstd(a): \n",
    "        return (float(np.mean(a)), float(np.std(a))) if len(a) else (0.0, 0.0)\n",
    "\n",
    "    cJ, cJsd = mstd(arr[clean_mask, 2].astype(float))\n",
    "    tJ, tJsd = mstd(arr[troj_mask,  2].astype(float))\n",
    "    cE, cEsd = mstd(arr[clean_mask, 3].astype(float))\n",
    "    tE, tEsd = mstd(arr[troj_mask,  3].astype(float))\n",
    "\n",
    "    print(\"\\n--- Jacobian Frobenius Norms & FD Relative Errors (Graph-level) ---\")\n",
    "    print(f\" Clean graphs :  avg_norm={cJ:.4f} ± {cJsd:.4f}, avg_FDrel={cE:.4e} ± {cEsd:.4e}\")\n",
    "    print(f\" Trojan graphs:  avg_norm={tJ:.4f} ± {tJsd:.4f}, avg_FDrel={tE:.4e} ± {tEsd:.4e}\")\n",
    "\n",
    "    print(\"\\nSample preview (first 6): (idx,label,jacobian_frob,FD_rel_err)\")\n",
    "    for t in per_sample_info[:6]:\n",
    "        print(t)\n",
    "else:\n",
    "    print(\"No Jacobian samples computed (empty selected set).\")\n",
    "\n",
    "print(\"\\nDone. (Order: PGD -> full-test evaluation -> Jacobian @ perturbed inputs.)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381b029d-0a5f-4504-ac56-8b97ff23d76e",
   "metadata": {},
   "source": [
    "#### Local Lipschitz Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f80db13d-5238-4507-b23d-ab86d8e1687b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected perturbation pool: {0: 4, 1: 10}\n",
      "\n",
      "Running Lipschitz-directed PGD for selected graphs...\n",
      "? Finished perturbations.\n",
      "\n",
      "============= Robustness Evaluation (Full Test Set) =============\n",
      "Accuracy: 35.71%\n",
      "Precision: 0.4490, Recall: 0.3571, F1: 0.3881\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     0.1429    0.2500    0.1818         4\n",
      "      trojan     0.5714    0.4000    0.4706        10\n",
      "\n",
      "    accuracy                         0.3571        14\n",
      "   macro avg     0.3571    0.3250    0.3262        14\n",
      "weighted avg     0.4490    0.3571    0.3881        14\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1 3]\n",
      " [6 4]]\n",
      "\n",
      "Selected graphs: 14. Flipped after perturbation: 8 (57.14%).\n",
      "\n",
      "Computing Local Lipschitz constants + FD relative errors...\n",
      "\n",
      "--- Local Lipschitz Constants (graph-level) ---\n",
      " Clean:  avg_L=0.0200 ± 0.0084, avg_FDrel=4.0474e-01 ± 2.2832e-01\n",
      " Trojan: avg_L=0.0167 ± 0.0198, avg_FDrel=9.6798e-01 ± 2.9770e-01\n",
      "\n",
      "Sample preview (first 6): (idx,label,L,FD_rel_err)\n",
      "(0, 0, 0.023152219131588936, 0.19119632244110107)\n",
      "(1, 0, 0.025629963725805283, 0.2860594391822815)\n",
      "(2, 0, 0.025734560564160347, 0.3544752299785614)\n",
      "(3, 0, 0.005626301746815443, 0.7872381210327148)\n",
      "(4, 1, 0.0036303005181252956, 1.002577543258667)\n",
      "(5, 1, 0.01738113723695278, 1.3039883375167847)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Graph-level: Local Lipschitz Constants (with PGD perturbations)\n",
    "# ============================================================\n",
    "import torch, numpy as np, torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "# ---------------------- PARAMETERS ----------------------\n",
    "PER_CLASS = 25      # graphs per class to perturb\n",
    "EPSILON   = 50.0     # L2 budget\n",
    "ALPHA     = 10.0     # PGD step size\n",
    "NUM_ITERS = 100     # PGD iterations\n",
    "FD_EPS    = 1e-1   # finite-diff epsilon\n",
    "SEED      = 42\n",
    "\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# ---------------------- Sanity check ----------------------\n",
    "if 'graph_model' not in globals() or 'test_graphs' not in globals():\n",
    "    raise RuntimeError(\"Need `graph_model` and `test_graphs` defined from training cell.\")\n",
    "\n",
    "model = graph_model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# ---------------------- Build dataset view ----------------------\n",
    "dataset = list(test_graphs)\n",
    "labels_np = np.array([int(d.y.item()) for d in dataset])\n",
    "\n",
    "# ---------------------- Select graphs (PER_CLASS/class) ----------------------\n",
    "rng = np.random.default_rng(SEED)\n",
    "selected = []\n",
    "for cls in [0, 1]:\n",
    "    idxs = np.where(labels_np == cls)[0]\n",
    "    if len(idxs) == 0: \n",
    "        continue\n",
    "    k = min(PER_CLASS, len(idxs))\n",
    "    chosen = rng.choice(idxs, size=k, replace=False)\n",
    "    selected.extend(chosen.tolist())\n",
    "selected = np.array(sorted(selected), dtype=np.int64)\n",
    "print(\"Selected perturbation pool:\", {0:int((labels_np[selected]==0).sum()), 1:int((labels_np[selected]==1).sum())})\n",
    "\n",
    "# ---------------------- PGD: Lipschitz-directed ----------------------\n",
    "perturbed_map = {}\n",
    "orig_preds_list, adv_preds_list = [], []\n",
    "\n",
    "print(\"\\nRunning Lipschitz-directed PGD for selected graphs...\")\n",
    "for idx in selected:\n",
    "    data = dataset[int(idx)]\n",
    "    x_orig = data.x.detach().clone().to(DEVICE)       # [N,F]\n",
    "    edge_index = data.edge_index.to(DEVICE)\n",
    "    y_true = torch.tensor([int(data.y.item())], device=DEVICE)\n",
    "\n",
    "    batch_zero = torch.zeros(x_orig.size(0), dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    # define f_local\n",
    "    def f_local(x):\n",
    "        return model(x, edge_index, batch_zero).squeeze(0)\n",
    "\n",
    "    # Jacobian at x_orig\n",
    "    try:\n",
    "        J = torch.autograd.functional.jacobian(f_local, x_orig)   # (C,N,F)\n",
    "    except RuntimeError:\n",
    "        logits0 = f_local(x_orig).detach()\n",
    "        C = logits0.shape[0]\n",
    "        rows = []\n",
    "        for c in range(C):\n",
    "            def scalar_f(x, cidx=c): return f_local(x)[cidx]\n",
    "            row = torch.autograd.functional.jacobian(scalar_f, x_orig)\n",
    "            rows.append(row.unsqueeze(0))\n",
    "        J = torch.cat(rows, dim=0)\n",
    "\n",
    "    C, N, Fdim = J.shape\n",
    "    J_flat = J.reshape(C, N*Fdim)\n",
    "\n",
    "    # leading right-singular vector\n",
    "    try:\n",
    "        _, S, Vh = torch.linalg.svd(J_flat, full_matrices=False)\n",
    "        v = Vh[0,:].detach()\n",
    "    except RuntimeError:\n",
    "        v = torch.randn(J_flat.shape[1], device=DEVICE)\n",
    "    v = v / (v.norm() + 1e-12)\n",
    "\n",
    "    v_mat = v.view_as(x_orig)\n",
    "    x_adv = (x_orig + 0.5*EPSILON*v_mat).detach().clone().requires_grad_(True)\n",
    "\n",
    "    # PGD loop\n",
    "    for it in range(NUM_ITERS):\n",
    "        logits = model(x_adv, edge_index, batch_zero)\n",
    "        loss = F.cross_entropy(logits, y_true)\n",
    "        grad = torch.autograd.grad(loss, x_adv)[0]\n",
    "        gnorm = grad.view(-1).norm().item()\n",
    "        if gnorm == 0: break\n",
    "        step = ALPHA * grad / (gnorm + 1e-12)\n",
    "        x_adv = (x_adv + step).detach()\n",
    "\n",
    "        delta = (x_adv - x_orig).view(-1)\n",
    "        dnorm = delta.norm().item()\n",
    "        if dnorm > EPSILON:\n",
    "            delta = delta * (EPSILON / (dnorm+1e-12))\n",
    "            x_adv = (x_orig + delta.view_as(x_orig)).detach()\n",
    "        x_adv = x_adv.requires_grad_(True)\n",
    "\n",
    "    perturbed_map[int(idx)] = x_adv.detach()\n",
    "\n",
    "    # record flips\n",
    "    with torch.no_grad():\n",
    "        p_orig = f_local(x_orig).argmax().item()\n",
    "        p_adv  = f_local(x_adv).argmax().item()\n",
    "    orig_preds_list.append(p_orig); adv_preds_list.append(p_adv)\n",
    "\n",
    "print(\"? Finished perturbations.\")\n",
    "\n",
    "# ---------------------- Evaluate full test set ----------------------\n",
    "print(\"\\n============= Robustness Evaluation (Full Test Set) =============\")\n",
    "y_true_all, y_pred_all = [], []\n",
    "with torch.no_grad():\n",
    "    for i, d in enumerate(dataset):\n",
    "        x_eval = perturbed_map[i] if i in perturbed_map else d.x.to(DEVICE)\n",
    "        edge_index = d.edge_index.to(DEVICE)\n",
    "        batch_zero = torch.zeros(x_eval.size(0), dtype=torch.long, device=DEVICE)\n",
    "        logits = model(x_eval, edge_index, batch_zero)\n",
    "        y_true_all.append(int(d.y.item()))\n",
    "        y_pred_all.append(int(logits.argmax(dim=1).item()))\n",
    "\n",
    "y_true_all = np.array(y_true_all)\n",
    "y_pred_all = np.array(y_pred_all)\n",
    "\n",
    "acc = (y_true_all == y_pred_all).mean()\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true_all, y_pred_all, average='weighted', zero_division=0)\n",
    "print(f\"Accuracy: {acc*100:.2f}%\")\n",
    "print(f\"Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true_all, y_pred_all, target_names=['clean','trojan'], digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true_all, y_pred_all, labels=[0,1]))\n",
    "\n",
    "num_flips = int((np.array(orig_preds_list) != np.array(adv_preds_list)).sum())\n",
    "print(f\"\\nSelected graphs: {len(selected)}. Flipped after perturbation: {num_flips} ({100.0*num_flips/len(selected):.2f}%).\")\n",
    "\n",
    "# ---------------------- Compute Local Lipschitz on perturbed graphs ----------------------\n",
    "print(\"\\nComputing Local Lipschitz constants + FD relative errors...\")\n",
    "per_sample_info = []\n",
    "for idx in selected:\n",
    "    d = dataset[int(idx)]\n",
    "    x_adv = perturbed_map[int(idx)].detach().clone().to(DEVICE).requires_grad_(True)\n",
    "    edge_index = d.edge_index.to(DEVICE)\n",
    "    label = int(d.y.item())\n",
    "    batch_zero = torch.zeros(x_adv.size(0), dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    def f_local_adv(x): return model(x, edge_index, batch_zero).squeeze(0)\n",
    "\n",
    "    # Jacobian\n",
    "    try:\n",
    "        J = torch.autograd.functional.jacobian(f_local_adv, x_adv).detach()\n",
    "    except RuntimeError:\n",
    "        logits0 = f_local_adv(x_adv).detach()\n",
    "        C = logits0.shape[0]; rows = []\n",
    "        for c in range(C):\n",
    "            def scalar_f(x, cidx=c): return f_local_adv(x)[cidx]\n",
    "            row = torch.autograd.functional.jacobian(scalar_f, x_adv)\n",
    "            rows.append(row.unsqueeze(0))\n",
    "        J = torch.cat(rows, dim=0)\n",
    "\n",
    "    C, N, Fdim = J.shape\n",
    "    J_flat = J.reshape(C, N*Fdim)\n",
    "\n",
    "    # spectral norm\n",
    "    try:\n",
    "        _, S, _ = torch.linalg.svd(J_flat, full_matrices=False)\n",
    "        sigma_max = float(S[0].item())\n",
    "    except RuntimeError:\n",
    "        JJT = (J_flat @ J_flat.T).cpu().numpy()\n",
    "        eigvals = np.linalg.eigvalsh(JJT)\n",
    "        sigma_max = float(np.sqrt(max(eigvals.max(), 0.0)))\n",
    "\n",
    "    # FD relative error\n",
    "    delta_fd = FD_EPS * torch.randn(N*Fdim, device=DEVICE)\n",
    "    pred_change = J_flat @ delta_fd\n",
    "    f0  = f_local_adv(x_adv).detach()\n",
    "    f0p = f_local_adv((x_adv + delta_fd.view_as(x_adv))).detach()\n",
    "    actual_change = f0p - f0\n",
    "    fd_rel_err = (torch.norm(pred_change - actual_change) / (torch.norm(actual_change)+1e-8)).item()\n",
    "\n",
    "    per_sample_info.append((int(idx), label, sigma_max, fd_rel_err))\n",
    "\n",
    "# ---------------------- Aggregate ----------------------\n",
    "clean_stats = [p for p in per_sample_info if p[1]==0]\n",
    "troj_stats  = [p for p in per_sample_info if p[1]==1]\n",
    "\n",
    "def aggs(stats):\n",
    "    if not stats: return (0,0,0,0)\n",
    "    Ls = np.array([s[2] for s in stats])\n",
    "    Es = np.array([s[3] for s in stats])\n",
    "    return (Ls.mean(), Ls.std(), Es.mean(), Es.std())\n",
    "\n",
    "cL_mean,cL_std,cE_mean,cE_std = aggs(clean_stats)\n",
    "tL_mean,tL_std,tE_mean,tE_std = aggs(troj_stats)\n",
    "\n",
    "print(\"\\n--- Local Lipschitz Constants (graph-level) ---\")\n",
    "print(f\" Clean:  avg_L={cL_mean:.4f} ± {cL_std:.4f}, avg_FDrel={cE_mean:.4e} ± {cE_std:.4e}\")\n",
    "print(f\" Trojan: avg_L={tL_mean:.4f} ± {tL_std:.4f}, avg_FDrel={tE_mean:.4e} ± {tE_std:.4e}\")\n",
    "\n",
    "print(\"\\nSample preview (first 6): (idx,label,L,FD_rel_err)\")\n",
    "for p in per_sample_info[:6]:\n",
    "    print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf73822-cdde-48b2-a56b-fb85bfa192f5",
   "metadata": {},
   "source": [
    "#### Hessian-based Curvature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5539e633-f917-4027-bc21-0fbe6ee0c023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 test graphs.\n",
      "Class counts in test set: [ 4 10]\n",
      "Selected pool size -> clean: 4, trojan: 10\n",
      "\n",
      "Computing Hessian-curvature proxies on selected graphs... (this will compute per-graph grads)\n",
      "\n",
      "Constructing Hessian-aligned perturbations (using negative gradient of true label log-prob)...\n",
      "Perturbations built for 14 selected graphs.\n",
      "\n",
      "================ Robustness Evaluation (Full Test Set: clean vs perturbed) ===============\n",
      "\n",
      "--- CLEAN (original features) ---\n",
      "Accuracy: 92.86%\n",
      "Precision: 0.9351, Recall: 0.9286, F1: 0.9252\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     1.0000    0.7500    0.8571         4\n",
      "      trojan     0.9091    1.0000    0.9524        10\n",
      "\n",
      "    accuracy                         0.9286        14\n",
      "   macro avg     0.9545    0.8750    0.9048        14\n",
      "weighted avg     0.9351    0.9286    0.9252        14\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 3  1]\n",
      " [ 0 10]]\n",
      "\n",
      "--- PERTURBED (Hessian-aligned on selected graphs) ---\n",
      "Accuracy: 50.00%\n",
      "Precision: 0.5333, Recall: 0.5000, F1: 0.5146\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     0.2000    0.2500    0.2222         4\n",
      "      trojan     0.6667    0.6000    0.6316        10\n",
      "\n",
      "    accuracy                         0.5000        14\n",
      "   macro avg     0.4333    0.4250    0.4269        14\n",
      "weighted avg     0.5333    0.5000    0.5146        14\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1 3]\n",
      " [4 6]]\n",
      "\n",
      "Selected graphs: 14. Flipped after perturbation: 6 (42.86%).\n",
      "\n",
      "--- Hessian Curvature Stats (grad outer-product proxy) ---\n",
      " Clean:  avg_lambda=0.0001 ± 0.0000, avg_FDrel=9.9949e-01 ± 1.7091e-03\n",
      " Trojan: avg_lambda=0.0000 ± 0.0001, avg_FDrel=1.0079e+00 ± 2.0711e-02\n",
      "\n",
      "Sample preview (first 6): (idx,label,lambda_proxy,FD_rel_err)\n",
      "(0, 0, 0.00013302891948102028, 0.9965778088777583)\n",
      "(1, 0, 9.664059793856958e-05, 1.000895407167922)\n",
      "(2, 0, 0.00011940403243237974, 1.000460089823712)\n",
      "(3, 0, 7.160921814152821e-06, 1.0000252959235916)\n",
      "(4, 1, 2.7324202071766585e-06, 0.999871482130579)\n",
      "(5, 1, 6.0165264973419987e-05, 0.9949334628129456)\n",
      "\n",
      "Done. (Hessian-proxy computation -> perturbations -> clean/perturbed evaluation.)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------- Hessian-Based Curvature (Graph-Level, notebook-friendly) -----------------------\n",
    "import torch, numpy as np, torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# ---------- PARAMETERS (tweak if needed) ----------\n",
    "PER_CLASS = 250        # graphs per class to include in curvature pool (will be clipped by available graphs)\n",
    "FD_EPS = 5e-1          # finite-difference epsilon for FD error\n",
    "TRIALS_PER_GRAPH = 5   # FD trials per graph\n",
    "PERT_P = 50.0          # L2 magnitude (applied per-node-feature matrix) for Hessian-aligned perturbation\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# ---------- locate test graphs in notebook (support test_graphs or test_loader.dataset) ----------\n",
    "required = [\"graph_model\", \"DEVICE\"]\n",
    "missing = [v for v in required if v not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing required variables in notebook: {missing}. Run your training/fine-tune cell first.\")\n",
    "\n",
    "if \"test_graphs\" in globals():\n",
    "    test_list = list(test_graphs)\n",
    "elif \"test_loader\" in globals():\n",
    "    # try to materialize dataset from test_loader.dataset\n",
    "    try:\n",
    "        ds = test_loader.dataset\n",
    "        test_list = [ds[i] for i in range(len(ds))]\n",
    "    except Exception:\n",
    "        # fallback: try to iterate loader (may yield batched Data objects)\n",
    "        test_list = []\n",
    "        for b in test_loader:\n",
    "            # if loader yields batched Data, break them out to individual graphs is complex;\n",
    "            # usually `test_graphs` is present in your notebook so this branch is rarely used.\n",
    "            raise RuntimeError(\"Cannot materialize individual graphs from test_loader. Prefer having `test_graphs` list in notebook.\")\n",
    "else:\n",
    "    raise RuntimeError(\"Need `test_graphs` or `test_loader` defined in the notebook environment.\")\n",
    "\n",
    "print(f\"Found {len(test_list)} test graphs.\")\n",
    "\n",
    "# ---------- quick helpers ----------\n",
    "device = DEVICE\n",
    "model = graph_model.to(device)\n",
    "model.eval()\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "def batch_for(x):\n",
    "    # single-graph batching: all nodes belong to batch index 0\n",
    "    return torch.zeros(x.size(0), dtype=torch.long, device=device)\n",
    "\n",
    "# ---------- base predictions (clean) ----------\n",
    "base_preds = []\n",
    "with torch.no_grad():\n",
    "    for data in test_list:\n",
    "        x = data.x.to(device)\n",
    "        edge_index = data.edge_index.to(device)\n",
    "        out = model(x, edge_index, batch_for(x))    # [1, C]\n",
    "        base_preds.append(int(out.argmax(dim=1).item()))\n",
    "base_preds = np.array(base_preds)\n",
    "labels_all = np.array([int(d.y.item()) for d in test_list])\n",
    "print(\"Class counts in test set:\", np.bincount(labels_all) if len(labels_all)>0 else \"[]\")\n",
    "\n",
    "# ---------- pick PER_CLASS graphs / class ----------\n",
    "selected = []\n",
    "for cls in [0, 1]:\n",
    "    idxs = np.where(labels_all == cls)[0]\n",
    "    if len(idxs) == 0:\n",
    "        continue\n",
    "    k = min(PER_CLASS, len(idxs))\n",
    "    chosen = rng.choice(idxs, size=k, replace=False).tolist()\n",
    "    selected.extend(chosen)\n",
    "selected = np.array(sorted(selected), dtype=np.int64)\n",
    "print(f\"Selected pool size -> clean: {(labels_all[selected]==0).sum()}, trojan: {(labels_all[selected]==1).sum()}\")\n",
    "\n",
    "# ---------- compute Hessian-curvature proxies on selected graphs ----------\n",
    "per_sample_info = []   # tuples: (idx, label, lambda_proxy, avg_fd_rel_err)\n",
    "print(\"\\nComputing Hessian-curvature proxies on selected graphs... (this will compute per-graph grads)\")\n",
    "\n",
    "for idx in selected:\n",
    "    data = test_list[int(idx)]\n",
    "    x0 = data.x.detach().clone().to(device).requires_grad_(True)   # [N, F]\n",
    "    edge_index = data.edge_index.to(device)\n",
    "\n",
    "    # predicted class at x0 (use that class to estimate curvature proxy)\n",
    "    with torch.no_grad():\n",
    "        logits = model(x0, edge_index, batch_for(x0))\n",
    "    pred_class = int(logits.argmax(dim=1).item())\n",
    "\n",
    "    # scalar function h(x) = log P(pred_class | x)\n",
    "    def h_pred(x):\n",
    "        logits = model(x, edge_index, batch_for(x))    # [1, C]\n",
    "        lp = F.log_softmax(logits.squeeze(0), dim=0)\n",
    "        return lp[pred_class]\n",
    "\n",
    "    h0 = h_pred(x0)\n",
    "    # gradient of log-prob of predicted class wrt node features (shape [N, F])\n",
    "    g = torch.autograd.grad(h0, x0, retain_graph=False, create_graph=False)[0].detach()\n",
    "\n",
    "    lambda_proxy = float((g.norm(p=2).item() ** 2))\n",
    "\n",
    "    # finite-difference second-order relative error (to check local quadratic approx)\n",
    "    errs = []\n",
    "    for _ in range(TRIALS_PER_GRAPH):\n",
    "        delta = FD_EPS * torch.randn_like(x0).to(device)\n",
    "        gt_delta = float((g * delta).sum().item())\n",
    "        pred_second = 0.5 * (gt_delta ** 2)\n",
    "        # actual second-order term: h(x0+delta) - h0 - g^T delta\n",
    "        actual_second = float((h_pred((x0 + delta).detach()) - h0 - (g * delta).sum()).item())\n",
    "        rel_err = abs(pred_second - actual_second) / (abs(actual_second) + 1e-8)\n",
    "        errs.append(rel_err)\n",
    "    avg_rel_err = float(np.mean(errs))\n",
    "    per_sample_info.append((int(idx), int(data.y.item()), lambda_proxy, avg_rel_err))\n",
    "\n",
    "# ---------- construct Hessian-aligned perturbations ----------\n",
    "print(\"\\nConstructing Hessian-aligned perturbations (using negative gradient of true label log-prob)...\")\n",
    "perturbed_map = {}\n",
    "for (idx, label, lam, fd_err) in per_sample_info:\n",
    "    data = test_list[int(idx)]\n",
    "    x0 = data.x.detach().clone().to(device).requires_grad_(True)\n",
    "    edge_index = data.edge_index.to(device)\n",
    "\n",
    "    # define h_true = log P(true_label | x) so negative grad reduces true-class score\n",
    "    true_label = int(data.y.item())\n",
    "    def h_true(x):\n",
    "        logits = model(x, edge_index, batch_for(x))\n",
    "        lp = F.log_softmax(logits.squeeze(0), dim=0)\n",
    "        return lp[true_label]\n",
    "\n",
    "    g_true = torch.autograd.grad(h_true(x0), x0, retain_graph=False, create_graph=False)[0].detach()\n",
    "    gnorm = g_true.norm().item()\n",
    "    if gnorm < 1e-12:\n",
    "        dir_vec = torch.randn_like(x0).to(device)\n",
    "    else:\n",
    "        dir_vec = - g_true / (gnorm + 1e-12)   # direction to reduce true class score\n",
    "\n",
    "    delta = (PERT_P * dir_vec).detach()\n",
    "    perturbed_map[int(idx)] = (x0 + delta).detach()\n",
    "\n",
    "print(f\"Perturbations built for {len(perturbed_map)} selected graphs.\")\n",
    "\n",
    "# ---------- evaluate model on entire test set: clean vs perturbed ----------\n",
    "print(\"\\n================ Robustness Evaluation (Full Test Set: clean vs perturbed) ===============\")\n",
    "preds_clean = []\n",
    "preds_pert  = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_list):\n",
    "        x_clean = data.x.to(device)\n",
    "        eidx = data.edge_index.to(device)\n",
    "        out_clean = model(x_clean, eidx, batch_for(x_clean))\n",
    "        preds_clean.append(int(out_clean.argmax(dim=1).item()))\n",
    "\n",
    "        if int(i) in perturbed_map:\n",
    "            x_eval = perturbed_map[int(i)].to(device)\n",
    "        else:\n",
    "            x_eval = x_clean\n",
    "        out_pert = model(x_eval, eidx, batch_for(x_eval))\n",
    "        preds_pert.append(int(out_pert.argmax(dim=1).item()))\n",
    "\n",
    "preds_clean = np.array(preds_clean)\n",
    "preds_pert  = np.array(preds_pert)\n",
    "\n",
    "# ----- metrics printing helper -----\n",
    "def print_metrics(name, preds, labels):\n",
    "    acc = (preds == labels).mean()\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(f\"Accuracy: {acc*100:.2f}%\")\n",
    "    print(f\"Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(labels, preds, target_names=['clean','trojan'], digits=4))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(labels, preds, labels=[0,1]))\n",
    "\n",
    "print_metrics(\"CLEAN (original features)\", preds_clean, labels_all)\n",
    "print_metrics(\"PERTURBED (Hessian-aligned on selected graphs)\", preds_pert, labels_all)\n",
    "\n",
    "# ---------- flip statistics for selected set ----------\n",
    "orig_sel_preds = base_preds[selected]\n",
    "adv_sel_preds  = np.array([preds_pert[i] for i in selected])\n",
    "num_flips = int((orig_sel_preds != adv_sel_preds).sum())\n",
    "print(f\"\\nSelected graphs: {len(selected)}. Flipped after perturbation: {num_flips} ({100.0*num_flips/len(selected):.2f}%).\")\n",
    "\n",
    "# ---------- aggregate curvature stats by class ----------\n",
    "clean_stats = [p for p in per_sample_info if p[1] == 0]\n",
    "troj_stats  = [p for p in per_sample_info if p[1] == 1]\n",
    "\n",
    "def summarize(stats):\n",
    "    if not stats: return (0.0, 0.0, 0.0, 0.0)\n",
    "    Ls = np.array([s[2] for s in stats])\n",
    "    Es = np.array([s[3] for s in stats])\n",
    "    return (Ls.mean(), Ls.std(), Es.mean(), Es.std())\n",
    "\n",
    "cL_mean, cL_std, cE_mean, cE_std = summarize(clean_stats)\n",
    "tL_mean, tL_std, tE_mean, tE_std = summarize(troj_stats)\n",
    "\n",
    "print(\"\\n--- Hessian Curvature Stats (grad outer-product proxy) ---\")\n",
    "print(f\" Clean:  avg_lambda={cL_mean:.4f} ± {cL_std:.4f}, avg_FDrel={cE_mean:.4e} ± {cE_std:.4e}\")\n",
    "print(f\" Trojan: avg_lambda={tL_mean:.4f} ± {tL_std:.4f}, avg_FDrel={tE_mean:.4e} ± {tE_std:.4e}\")\n",
    "\n",
    "print(\"\\nSample preview (first 6): (idx,label,lambda_proxy,FD_rel_err)\")\n",
    "for p in per_sample_info[:6]:\n",
    "    print(p)\n",
    "\n",
    "print(\"\\nDone. (Hessian-proxy computation -> perturbations -> clean/perturbed evaluation.)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc12ff7-45db-4dac-b182-37b3d2fbb450",
   "metadata": {},
   "source": [
    "#### Prediction Margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bda5853-374f-4582-99e8-344d5f083015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 test graphs.\n",
      "Selected pool: clean=4, trojan=10\n",
      "\n",
      "Running PGD perturbations on selected graphs...\n",
      "? Finished perturbations.\n",
      "\n",
      "--- CLEAN (original features) ---\n",
      "Accuracy: 92.86%\n",
      "Precision: 0.9351, Recall: 0.9286, F1: 0.9252\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     1.0000    0.7500    0.8571         4\n",
      "      trojan     0.9091    1.0000    0.9524        10\n",
      "\n",
      "    accuracy                         0.9286        14\n",
      "   macro avg     0.9545    0.8750    0.9048        14\n",
      "weighted avg     0.9351    0.9286    0.9252        14\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 3  1]\n",
      " [ 0 10]]\n",
      "\n",
      "--- PERTURBED (PGD, selected graphs) ---\n",
      "Accuracy: 42.86%\n",
      "Precision: 0.4940, Recall: 0.4286, F1: 0.4540\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     0.1667    0.2500    0.2000         4\n",
      "      trojan     0.6250    0.5000    0.5556        10\n",
      "\n",
      "    accuracy                         0.4286        14\n",
      "   macro avg     0.3958    0.3750    0.3778        14\n",
      "weighted avg     0.4940    0.4286    0.4540        14\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1 3]\n",
      " [5 5]]\n",
      "\n",
      "Selected graphs: 14. Flipped after perturbation: 7 (50.00%).\n",
      "\n",
      "--- Prediction Margin Stats (on perturbed graphs) ---\n",
      " Clean:  avg_margin=1.0803 ± 0.6429, avg_FDrel=2.0046e-03 ± 8.6171e-04\n",
      " Trojan: avg_margin=0.6429 ± 1.2412, avg_FDrel=8.1105e-02 ± 1.3079e-01\n",
      "\n",
      "Sample preview (first 6): (idx,label,margin,FD_rel_err)\n",
      "(0, 0, 1.9972741603851318, 0.0034133547850313164)\n",
      "(1, 0, 1.0253452062606812, 0.001382222020074586)\n",
      "(2, 0, 1.1173126101493835, 0.001991486271908331)\n",
      "(3, 0, 0.18136080354452133, 0.001231256905759435)\n",
      "(4, 1, 0.45322123169898987, 0.004158838422209681)\n",
      "(5, 1, 0.5322955846786499, 0.007307755212519939)\n",
      "\n",
      "? Done. (Prediction Margin: PGD perturbations -> clean/perturbed eval -> margin stats.)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------- Prediction Margin (Graph-Level, notebook-friendly) -----------------------\n",
    "import torch, numpy as np, torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# ----- parameters -----\n",
    "PER_CLASS = 250       # graphs per class\n",
    "EPSILON = 50.0         # PGD perturbation budget (L2 norm)\n",
    "ALPHA = 0.9           # PGD step size\n",
    "NUM_ITERS = 150        # PGD iterations\n",
    "FD_EPS = 1e-1         # finite-difference perturbation\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# ----- locate dataset -----\n",
    "if \"test_graphs\" in globals():\n",
    "    dataset = list(test_graphs)\n",
    "elif \"test_loader\" in globals():\n",
    "    dataset = [test_loader.dataset[i] for i in range(len(test_loader.dataset))]\n",
    "else:\n",
    "    raise RuntimeError(\"Need `test_graphs` or `test_loader` defined in notebook.\")\n",
    "\n",
    "labels_np = np.array([int(d.y.item()) for d in dataset])\n",
    "n_test = len(dataset)\n",
    "print(f\"Found {n_test} test graphs.\")\n",
    "\n",
    "# ----- select PER_CLASS graphs per class -----\n",
    "rng = np.random.default_rng(SEED)\n",
    "selected = []\n",
    "for cls in [0, 1]:\n",
    "    idxs = np.where(labels_np == cls)[0].tolist()\n",
    "    if len(idxs) == 0: continue\n",
    "    k = min(PER_CLASS, len(idxs))\n",
    "    chosen = rng.choice(idxs, size=k, replace=False).tolist()\n",
    "    selected.extend(chosen)\n",
    "selected = np.array(sorted(selected), dtype=np.int64)\n",
    "print(f\"Selected pool: clean={int((labels_np[selected]==0).sum())}, trojan={int((labels_np[selected]==1).sum())}\")\n",
    "\n",
    "# helper for batching\n",
    "def batch_for(x): \n",
    "    return torch.zeros(x.size(0), dtype=torch.long, device=DEVICE)\n",
    "\n",
    "# ----- base predictions -----\n",
    "model = graph_model.to(DEVICE)\n",
    "model.eval()\n",
    "base_preds = []\n",
    "with torch.no_grad():\n",
    "    for data in dataset:\n",
    "        logits = model(data.x.to(DEVICE), data.edge_index.to(DEVICE), batch_for(data.x.to(DEVICE)))\n",
    "        base_preds.append(int(logits.argmax()))\n",
    "base_preds = np.array(base_preds)\n",
    "\n",
    "# ----- adversarial perturbations (PGD) -----\n",
    "perturbed_map = {}\n",
    "print(\"\\nRunning PGD perturbations on selected graphs...\")\n",
    "for idx in selected:\n",
    "    data = dataset[int(idx)]\n",
    "    x_orig = data.x.detach().clone().to(DEVICE)\n",
    "    edge_index = data.edge_index.to(DEVICE)\n",
    "    y_scalar = int(data.y.item())\n",
    "    target = torch.tensor([y_scalar], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    # random init inside epsilon-ball\n",
    "    delta = torch.randn_like(x_orig).to(DEVICE)\n",
    "    delta = EPSILON * delta / (delta.norm() + 1e-12)\n",
    "    x_adv = (x_orig + delta).detach().clone().requires_grad_(True)\n",
    "\n",
    "    for it in range(NUM_ITERS):\n",
    "        logits = model(x_adv, edge_index, batch_for(x_adv))\n",
    "        loss = F.cross_entropy(logits, target)\n",
    "        grad = torch.autograd.grad(loss, x_adv, retain_graph=False, create_graph=False)[0]\n",
    "        step = ALPHA * grad / (grad.norm() + 1e-12)\n",
    "        x_adv = (x_adv + step).detach()\n",
    "        delta = x_adv - x_orig\n",
    "        if delta.norm() > EPSILON:\n",
    "            delta = delta * (EPSILON / (delta.norm() + 1e-12))\n",
    "            x_adv = (x_orig + delta).detach()\n",
    "        x_adv = x_adv.requires_grad_(True)\n",
    "\n",
    "    perturbed_map[int(idx)] = x_adv.detach()\n",
    "print(\"? Finished perturbations.\")\n",
    "\n",
    "# ----- evaluate on full test set (clean vs perturbed) -----\n",
    "preds_clean, preds_pert = [], []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(dataset):\n",
    "        # clean\n",
    "        logits = model(data.x.to(DEVICE), data.edge_index.to(DEVICE), batch_for(data.x.to(DEVICE)))\n",
    "        preds_clean.append(int(logits.argmax()))\n",
    "\n",
    "        # perturbed if selected, else clean\n",
    "        if int(i) in perturbed_map:\n",
    "            x_eval = perturbed_map[int(i)]\n",
    "        else:\n",
    "            x_eval = data.x.to(DEVICE)\n",
    "        logits = model(x_eval, data.edge_index.to(DEVICE), batch_for(x_eval))\n",
    "        preds_pert.append(int(logits.argmax()))\n",
    "\n",
    "preds_clean = np.array(preds_clean)\n",
    "preds_pert  = np.array(preds_pert)\n",
    "\n",
    "labels_all = labels_np\n",
    "\n",
    "def print_metrics(name, preds, labels):\n",
    "    acc = (preds == labels).mean()\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(f\"Accuracy: {acc*100:.2f}%\")\n",
    "    print(f\"Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(labels, preds, target_names=['clean','trojan'], digits=4))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(labels, preds, labels=[0,1]))\n",
    "\n",
    "print_metrics(\"CLEAN (original features)\", preds_clean, labels_all)\n",
    "print_metrics(\"PERTURBED (PGD, selected graphs)\", preds_pert, labels_all)\n",
    "\n",
    "# ----- flip stats -----\n",
    "orig_sel_preds = base_preds[selected]\n",
    "adv_sel_preds  = np.array([preds_pert[i] for i in selected])\n",
    "num_flips = int((orig_sel_preds != adv_sel_preds).sum())\n",
    "print(f\"\\nSelected graphs: {len(selected)}. Flipped after perturbation: {num_flips} ({100.0*num_flips/len(selected):.2f}%).\")\n",
    "\n",
    "# ----- compute prediction margin + FD relative error -----\n",
    "per_sample_info = []  # (idx, label, margin, FD_rel_err)\n",
    "for idx in selected:\n",
    "    data = dataset[int(idx)]\n",
    "    x_eval = perturbed_map[int(idx)]\n",
    "    edge_index = data.edge_index.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(x_eval, edge_index, batch_for(x_eval)).squeeze(0)\n",
    "    pred_class = int(logits.argmax().item())\n",
    "    pred_logit = logits[pred_class].item()\n",
    "    other_logits = logits.clone(); other_logits[pred_class] = -float(\"inf\")\n",
    "    second_max = other_logits.max().item()\n",
    "    margin = pred_logit - second_max\n",
    "\n",
    "    # finite-difference check\n",
    "    delta = FD_EPS * torch.randn_like(x_eval).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits_p = model(x_eval + delta, edge_index, batch_for(x_eval)).squeeze(0)\n",
    "    pred_logit_p = logits_p[pred_class].item()\n",
    "    other_logits_p = logits_p.clone(); other_logits_p[pred_class] = -float(\"inf\")\n",
    "    second_max_p = other_logits_p.max().item()\n",
    "    margin_p = pred_logit_p - second_max_p\n",
    "    rel_err = abs(margin - margin_p) / (abs(margin_p) + 1e-12)\n",
    "\n",
    "    per_sample_info.append((int(idx), int(data.y.item()), float(margin), float(rel_err)))\n",
    "\n",
    "# ----- aggregate stats -----\n",
    "def summarize(stats):\n",
    "    if not stats: return (0.0,0.0,0.0,0.0)\n",
    "    Ms = np.array([s[2] for s in stats])\n",
    "    Es = np.array([s[3] for s in stats])\n",
    "    return (Ms.mean(), Ms.std(), Es.mean(), Es.std())\n",
    "\n",
    "clean_stats = [p for p in per_sample_info if p[1]==0]\n",
    "troj_stats  = [p for p in per_sample_info if p[1]==1]\n",
    "\n",
    "cM_mean, cM_std, cE_mean, cE_std = summarize(clean_stats)\n",
    "tM_mean, tM_std, tE_mean, tE_std = summarize(troj_stats)\n",
    "\n",
    "print(\"\\n--- Prediction Margin Stats (on perturbed graphs) ---\")\n",
    "print(f\" Clean:  avg_margin={cM_mean:.4f} ± {cM_std:.4f}, avg_FDrel={cE_mean:.4e} ± {cE_std:.4e}\")\n",
    "print(f\" Trojan: avg_margin={tM_mean:.4f} ± {tM_std:.4f}, avg_FDrel={tE_mean:.4e} ± {tE_std:.4e}\")\n",
    "\n",
    "print(\"\\nSample preview (first 6): (idx,label,margin,FD_rel_err)\")\n",
    "for p in per_sample_info[:6]:\n",
    "    print(p)\n",
    "\n",
    "print(\"\\n? Done. (Prediction Margin: PGD perturbations -> clean/perturbed eval -> margin stats.)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74e016-e0b1-417c-8f64-1cfccf984251",
   "metadata": {},
   "source": [
    "#### Adversarial Robustness Radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83a04f2b-c74f-426a-b6ca-556fd5f1749c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing Adversarial Robustness Radius (ARR) for selected perturbed graphs...\n",
      "? Done ARR computation. Time elapsed: 14.3s\n",
      "\n",
      "--- Adversarial Robustness Radius (ARR) Stats ---\n",
      "Class      Avg Radius ± Std   Avg Rel. Error ± Std\n",
      "----------------------------------------------------\n",
      "clean      20.0000 ± 0.0000      0.0000e+00 ± 0.0000e+00\n",
      "trojan     20.0000 ± 0.0000      0.0000e+00 ± 0.0000e+00\n",
      "\n",
      "Overall ARR: Avg Radius: 20.0000 ± 0.0000\n",
      "Overall ARR: Avg Relative Error: 0.0000e+00 ± 0.0000e+00\n",
      "\n",
      "============= Robustness Evaluation (Full Test Set: perturbed selected graphs + others clean) =============\n",
      "Accuracy: 42.86%\n",
      "Precision: 0.4940, Recall: 0.4286, F1: 0.4540\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     0.1667    0.2500    0.2000         4\n",
      "      trojan     0.6250    0.5000    0.5556        10\n",
      "\n",
      "    accuracy                         0.4286        14\n",
      "   macro avg     0.3958    0.3750    0.3778        14\n",
      "weighted avg     0.4940    0.4286    0.4540        14\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1 3]\n",
      " [5 5]]\n",
      "\n",
      "Selected graphs: 14. Flipped after perturbation: 7 (50.00%).\n"
     ]
    }
   ],
   "source": [
    "# ----------------------- Adversarial Robustness Radius (Graph-Level) -----------------------\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "# ----- helpers -----\n",
    "def f_for_graph(x_tensor, data):\n",
    "    \"\"\"Return logits for given graph with node features replaced by x_tensor.\"\"\"\n",
    "    x_tensor = x_tensor.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out = graph_model(x_tensor, data.edge_index.to(DEVICE), batch_for(x_tensor))\n",
    "    return out.squeeze(0)\n",
    "\n",
    "def adversarial_radius_for_graph(data, x0, initial_epsilon=1e-3, growth_factor=1.25,\n",
    "                                 max_epsilon=20.0, bs_iters=10, num_trials=6):\n",
    "    \"\"\"Estimate minimal perturbation radius that flips prediction (around perturbed point).\"\"\"\n",
    "    with torch.no_grad():\n",
    "        base_out = graph_model(x0, data.edge_index.to(DEVICE), batch_for(x0))\n",
    "        y0 = int(base_out.argmax().item())\n",
    "\n",
    "    def is_same(x):\n",
    "        out = f_for_graph(x, data)\n",
    "        return int(out.argmax().item()) == y0\n",
    "\n",
    "    radii = []\n",
    "    for _ in range(num_trials):\n",
    "        d = torch.randn_like(x0)\n",
    "        d = d / (d.norm() + 1e-12)\n",
    "\n",
    "        eps = initial_epsilon\n",
    "        while eps < max_epsilon and is_same(x0 + eps * d):\n",
    "            eps *= growth_factor\n",
    "\n",
    "        if eps >= max_epsilon:\n",
    "            candidate = max_epsilon\n",
    "        else:\n",
    "            low, high = eps / growth_factor, eps\n",
    "            for _ in range(bs_iters):\n",
    "                mid = 0.5 * (low + high)\n",
    "                if is_same(x0 + mid * d):\n",
    "                    low = mid\n",
    "                else:\n",
    "                    high = mid\n",
    "            candidate = float(high)\n",
    "        radii.append(candidate)\n",
    "    return float(min(radii))\n",
    "\n",
    "def adversarial_radius_relerr(data, x0):\n",
    "    r1 = adversarial_radius_for_graph(data, x0, growth_factor=2.05, bs_iters=10, num_trials=6)\n",
    "    r2 = adversarial_radius_for_graph(data, x0, growth_factor=1.8, bs_iters=12, num_trials=6)\n",
    "    rel_err = abs(r1 - r2) / (abs(r2) + 1e-12)\n",
    "    return r1, rel_err\n",
    "\n",
    "# ----- ARR computation on selected perturbed graphs -----\n",
    "class_names = ['clean', 'trojan']\n",
    "class_adv_radius = {cn: [] for cn in class_names}\n",
    "class_rel_errors = {cn: [] for cn in class_names}\n",
    "all_radii, all_rel_errs = [], []\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"\\nComputing Adversarial Robustness Radius (ARR) for selected perturbed graphs...\")\n",
    "for i, idx in enumerate(selected):\n",
    "    idx = int(idx)\n",
    "    data = dataset[idx]\n",
    "    label = int(data.y.item())\n",
    "    cn = class_names[label]\n",
    "\n",
    "    x0 = perturbed_map[idx].clone().detach().to(DEVICE)  # start from perturbed features\n",
    "    r, rel = adversarial_radius_relerr(data, x0)\n",
    "\n",
    "    class_adv_radius[cn].append(r)\n",
    "    class_rel_errors[cn].append(rel)\n",
    "    all_radii.append(r)\n",
    "    all_rel_errs.append(rel)\n",
    "\n",
    "    if (i+1) % 20 == 0:\n",
    "        print(f\"  processed {i+1}/{len(selected)} graphs...\")\n",
    "\n",
    "t1 = time.time()\n",
    "print(f\"? Done ARR computation. Time elapsed: {t1-t0:.1f}s\")\n",
    "\n",
    "# ----- Reporting ARR aggregates -----\n",
    "print(\"\\n--- Adversarial Robustness Radius (ARR) Stats ---\")\n",
    "print(\"{:<10s} {:>14s} {:>22s}\".format(\"Class\", \"Avg Radius ± Std\", \"Avg Rel. Error ± Std\"))\n",
    "print(\"-\"*52)\n",
    "for cn in class_names:\n",
    "    if class_adv_radius[cn]:\n",
    "        print(\"{:<10s} {:>7.4f} ± {:<7.4f} {:>14.4e} ± {:<10.4e}\".format(\n",
    "            cn, np.mean(class_adv_radius[cn]), np.std(class_adv_radius[cn]),\n",
    "            np.mean(class_rel_errors[cn]), np.std(class_rel_errors[cn])\n",
    "        ))\n",
    "    else:\n",
    "        print(\"{:<10s} {:>10s}\".format(cn, \"-\"))\n",
    "\n",
    "print(\"\\nOverall ARR: Avg Radius: {:.4f} ± {:.4f}\".format(np.mean(all_radii), np.std(all_radii)))\n",
    "print(\"Overall ARR: Avg Relative Error: {:.4e} ± {:.4e}\".format(np.mean(all_rel_errs), np.std(all_rel_errs)))\n",
    "\n",
    "# ------------------ Evaluate model on full test set (selected perturbed + others clean) ------------------\n",
    "graph_model.eval()\n",
    "with torch.no_grad():\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    for i, data in enumerate(dataset):\n",
    "        x_in = perturbed_map[i].to(DEVICE) if i in perturbed_map else data.x.to(DEVICE)\n",
    "        logits = graph_model(x_in, data.edge_index.to(DEVICE), batch_for(x_in))\n",
    "        all_logits.append(logits.cpu())\n",
    "        all_labels.append(int(data.y.item()))\n",
    "\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    preds_all = all_logits.argmax(dim=1).numpy()\n",
    "    labels_all = np.array(all_labels)\n",
    "\n",
    "acc = (preds_all == labels_all).mean()\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(labels_all, preds_all, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"\\n============= Robustness Evaluation (Full Test Set: perturbed selected graphs + others clean) =============\")\n",
    "print(f\"Accuracy: {acc*100:.2f}%\")\n",
    "print(f\"Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\\n\")\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(labels_all, preds_all, target_names=['clean','trojan'], digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(labels_all, preds_all, labels=[0,1]))\n",
    "\n",
    "# ------------------ Flip statistics for selected graphs ------------------\n",
    "with torch.no_grad():\n",
    "    orig_preds = []\n",
    "    for data in dataset:\n",
    "        x_in = data.x.to(DEVICE)\n",
    "        logits = graph_model(x_in, data.edge_index.to(DEVICE), batch_for(x_in))\n",
    "        orig_preds.append(logits.cpu())\n",
    "    orig_preds = torch.cat(orig_preds, dim=0).argmax(dim=1).numpy()\n",
    "\n",
    "adv_sel_preds = preds_all[selected]\n",
    "num_flips = int((orig_preds[selected] != adv_sel_preds).sum())\n",
    "print(f\"\\nSelected graphs: {len(selected)}. Flipped after perturbation: {num_flips} ({100.0 * num_flips/len(selected):.2f}%).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1af61c-92a2-4229-9e74-1da940bae23a",
   "metadata": {},
   "source": [
    "#### Stability Under Input Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b32afd2-ed09-4595-819f-f2e21d98ad21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stability Under Input Noise (graph-level) ---\n",
      "Selected graphs: clean=4, trojan=10\n",
      "\n",
      "============= Robustness Evaluation (Full Test Set: perturbed selected + others unperturbed) =============\n",
      "Accuracy: 42.86%\n",
      "Precision: 0.4940, Recall: 0.4286, F1: 0.4540\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     0.1667    0.2500    0.2000         4\n",
      "      trojan     0.6250    0.5000    0.5556        10\n",
      "\n",
      "    accuracy                         0.4286        14\n",
      "   macro avg     0.3958    0.3750    0.3778        14\n",
      "weighted avg     0.4940    0.4286    0.4540        14\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1 3]\n",
      " [5 5]]\n",
      "\n",
      "Selected graphs: 14. Flipped after perturbation: 7 (50.00%).\n",
      "\n",
      "Computing Stability Under Input Noise (this may take a while)...\n",
      "  processed 10/14 graphs...\n",
      "? Done SUIN computation. Time elapsed: 60.2s\n",
      "\n",
      "--- Stability Under Input Noise (on perturbed graphs) ---\n",
      " Clean:  avg_stability=0.0304 ± 0.0131, avg_relerr=3.1787e-02 ± 3.9258e-02\n",
      " Trojan: avg_stability=0.0833 ± 0.0260, avg_relerr=1.3912e-02 ± 1.5433e-02\n",
      "\n",
      "Sample preview (first 6): (idx, label, stability, rel_err)\n",
      "(0, 0, 0.01696591931860894, 0.09944163678528219)\n",
      "(1, 0, 0.028194842617958783, 0.015510025793295993)\n",
      "(2, 0, 0.02460160998161882, 0.0048933657473740465)\n",
      "(3, 0, 0.051935511603951455, 0.0073030049956169605)\n",
      "(4, 1, 0.059593073651194574, 0.002063742624077379)\n",
      "(5, 1, 0.048475909009575845, 0.05758116169348127)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Stability Under Input Noise (SUIN) - Graph classification\n",
    "# ============================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "import time\n",
    "\n",
    "# --- Parameters ---\n",
    "NOISE_SIGMA        = 0.8   # Gaussian noise stddev for stability metric\n",
    "NUM_NOISE_SAMPLES  = 50    # Monte Carlo samples per graph\n",
    "RELERR_RESAMPLES   = 10     # repeats to estimate relative error\n",
    "\n",
    "print(\"\\n--- Stability Under Input Noise (graph-level) ---\")\n",
    "print(f\"Selected graphs: clean={(labels_np[selected]==0).sum()}, trojan={(labels_np[selected]==1).sum()}\")\n",
    "\n",
    "# ============================================================\n",
    "# Step 1: Evaluate model on full dataset (perturbed + original)\n",
    "# ============================================================\n",
    "graph_model.eval()\n",
    "with torch.no_grad():\n",
    "    all_logits, all_labels = [], []\n",
    "    for i, data in enumerate(dataset):\n",
    "        # use perturbed features if available\n",
    "        x_in = perturbed_map[i].to(DEVICE) if i in perturbed_map else data.x.to(DEVICE)\n",
    "        logits = graph_model(x_in, data.edge_index.to(DEVICE), batch_for(x_in))\n",
    "        all_logits.append(logits.cpu())\n",
    "        all_labels.append(int(data.y.item()))\n",
    "\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    preds_all  = all_logits.argmax(dim=1).numpy()\n",
    "    labels_all = np.array(all_labels)\n",
    "\n",
    "acc = (preds_all == labels_all).mean()\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(labels_all, preds_all, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"\\n============= Robustness Evaluation (Full Test Set: perturbed selected + others unperturbed) =============\")\n",
    "print(f\"Accuracy: {acc*100:.2f}%\")\n",
    "print(f\"Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\\n\")\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(labels_all, preds_all, target_names=['clean','trojan'], digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(labels_all, preds_all, labels=[0,1]))\n",
    "\n",
    "# Flip statistics for selected graphs\n",
    "with torch.no_grad():\n",
    "    orig_preds = []\n",
    "    for data in dataset:\n",
    "        x_in = data.x.to(DEVICE)\n",
    "        logits = graph_model(x_in, data.edge_index.to(DEVICE), batch_for(x_in))\n",
    "        orig_preds.append(logits.cpu())\n",
    "    orig_preds = torch.cat(orig_preds, dim=0).argmax(dim=1).numpy()\n",
    "\n",
    "adv_sel_preds = preds_all[selected]\n",
    "num_flips = int((orig_preds[selected] != adv_sel_preds).sum())\n",
    "print(f\"\\nSelected graphs: {len(selected)}. Flipped after perturbation: {num_flips} ({100.0*num_flips/len(selected):.2f}%).\")\n",
    "\n",
    "# ============================================================\n",
    "# Step 2: Stability Under Input Noise (on perturbed graphs)\n",
    "# ============================================================\n",
    "def stability_for_graph(idx, sigma, num_samples):\n",
    "    \"\"\"Compute avg L2 change in logits for noisy perturbations around perturbed graph.\"\"\"\n",
    "    data = dataset[idx]\n",
    "    base_x = perturbed_map[idx].to(DEVICE)\n",
    "    edge_index = data.edge_index.to(DEVICE)\n",
    "    batch = batch_for(base_x)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        f_orig = graph_model(base_x, edge_index, batch).squeeze()\n",
    "\n",
    "    diffs = []\n",
    "    for _ in range(num_samples):\n",
    "        noise = sigma * torch.randn_like(base_x).to(DEVICE)\n",
    "        f_noisy = graph_model(base_x + noise, edge_index, batch).squeeze()\n",
    "        diffs.append(torch.norm(f_noisy - f_orig).item())\n",
    "    return float(np.mean(diffs))\n",
    "\n",
    "print(\"\\nComputing Stability Under Input Noise (this may take a while)...\")\n",
    "t0 = time.time()\n",
    "per_sample_info = []  # (idx, label, stability, rel_err)\n",
    "for i, idx in enumerate(selected):\n",
    "    s_val = stability_for_graph(idx, NOISE_SIGMA, NUM_NOISE_SAMPLES)\n",
    "    re_vals = [stability_for_graph(idx, NOISE_SIGMA, NUM_NOISE_SAMPLES) for _ in range(RELERR_RESAMPLES)]\n",
    "    s_ref = float(np.mean(re_vals))\n",
    "    rel_err = abs(s_val - s_ref) / (abs(s_ref) + 1e-12)\n",
    "    per_sample_info.append((int(idx), int(labels_np[idx]), s_val, rel_err))\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(f\"  processed {i+1}/{len(selected)} graphs...\")\n",
    "t1 = time.time()\n",
    "print(f\"? Done SUIN computation. Time elapsed: {t1-t0:.1f}s\")\n",
    "\n",
    "# ============================================================\n",
    "# Step 3: Aggregate and report\n",
    "# ============================================================\n",
    "clean_stats = [(i, s, e) for (i, l, s, e) in per_sample_info if l == 0]\n",
    "troj_stats  = [(i, s, e) for (i, l, s, e) in per_sample_info if l == 1]\n",
    "\n",
    "def aggs(stats):\n",
    "    if not stats:\n",
    "        return (0.0, 0.0, 0.0, 0.0)\n",
    "    Ss = np.array([s for (_, s, _) in stats])\n",
    "    Es = np.array([e for (_, _, e) in stats])\n",
    "    return (Ss.mean(), Ss.std(), Es.mean(), Es.std())\n",
    "\n",
    "cS_mean, cS_std, cE_mean, cE_std = aggs(clean_stats)\n",
    "tS_mean, tS_std, tE_mean, tE_std = aggs(troj_stats)\n",
    "\n",
    "print(\"\\n--- Stability Under Input Noise (on perturbed graphs) ---\")\n",
    "print(f\" Clean:  avg_stability={cS_mean:.4f} ± {cS_std:.4f}, avg_relerr={cE_mean:.4e} ± {cE_std:.4e}\")\n",
    "print(f\" Trojan: avg_stability={tS_mean:.4f} ± {tS_std:.4f}, avg_relerr={tE_mean:.4e} ± {tE_std:.4e}\")\n",
    "\n",
    "print(\"\\nSample preview (first 6): (idx, label, stability, rel_err)\")\n",
    "for p in per_sample_info[:6]:\n",
    "    print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d5299b-b6b0-4319-ab39-9e1d4daf3776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
