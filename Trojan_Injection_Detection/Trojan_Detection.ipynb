{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eb5d540-cb61-42e5-8b5a-942d0ab25031",
   "metadata": {},
   "source": [
    "#### Node Classification: Trojan vs non-trojan. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1ba8c7-82e1-4547-8088-e36ce4f6e7ab",
   "metadata": {},
   "source": [
    "Below is a single, end‑to‑end GCN training script that:\n",
    "\n",
    "reads GNNDatasets/node.csv + GNNDatasets/node_edges.csv\n",
    "\n",
    "auto‑builds a multi‑circuit graph (nodes keyed by circuit_name::node)\n",
    "\n",
    "one‑hot encodes gate_type, standardizes numeric features\n",
    "\n",
    "adds zero‑feature pseudo nodes for edge‑only items (ASSIGN_*, DFF_*, PIs/POs)\n",
    "\n",
    "trains a 2‑layer GCN with class‑weighted loss + early stopping\n",
    "\n",
    "reports accuracy, classification report, confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b5790c1-0a45-4ba9-a072-a5c358305959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss 0.8937 | Val 0.4607 | Test 0.4631\n",
      "Epoch 010 | Loss 0.6115 | Val 0.9505 | Test 0.9496\n",
      "Epoch 020 | Loss 0.4193 | Val 0.9883 | Test 0.9885\n",
      "Epoch 030 | Loss 0.2920 | Val 0.9926 | Test 0.9931\n",
      "Epoch 040 | Loss 0.2113 | Val 0.9981 | Test 0.9981\n",
      "Epoch 050 | Loss 0.1559 | Val 0.9994 | Test 0.9995\n",
      "Epoch 060 | Loss 0.1176 | Val 0.9999 | Test 0.9998\n",
      "Epoch 070 | Loss 0.0936 | Val 0.9999 | Test 0.9998\n",
      "Epoch 080 | Loss 0.0780 | Val 1.0000 | Test 1.0000\n",
      "Epoch 090 | Loss 0.0668 | Val 1.0000 | Test 1.0000\n",
      "Epoch 100 | Loss 0.0553 | Val 1.0000 | Test 1.0000\n",
      "Epoch 110 | Loss 0.0493 | Val 1.0000 | Test 1.0000\n",
      "Epoch 120 | Loss 0.0435 | Val 1.0000 | Test 1.0000\n",
      "Epoch 130 | Loss 0.0407 | Val 1.0000 | Test 1.0000\n",
      "Epoch 140 | Loss 0.0371 | Val 1.0000 | Test 1.0000\n",
      "Epoch 150 | Loss 0.0346 | Val 1.0000 | Test 1.0000\n",
      "Epoch 160 | Loss 0.0329 | Val 1.0000 | Test 1.0000\n",
      "Epoch 170 | Loss 0.0301 | Val 1.0000 | Test 1.0000\n",
      "Epoch 180 | Loss 0.0283 | Val 1.0000 | Test 1.0000\n",
      "Epoch 190 | Loss 0.0273 | Val 1.0000 | Test 1.0000\n",
      "Epoch 200 | Loss 0.0266 | Val 1.0000 | Test 1.0000\n",
      "Epoch 210 | Loss 0.0253 | Val 1.0000 | Test 1.0000\n",
      "Epoch 220 | Loss 0.0241 | Val 1.0000 | Test 1.0000\n",
      "Epoch 230 | Loss 0.0240 | Val 1.0000 | Test 1.0000\n",
      "Epoch 240 | Loss 0.0230 | Val 1.0000 | Test 1.0000\n",
      "Epoch 250 | Loss 0.0210 | Val 1.0000 | Test 1.0000\n",
      "Epoch 260 | Loss 0.0215 | Val 1.0000 | Test 1.0000\n",
      "Epoch 270 | Loss 0.0197 | Val 1.0000 | Test 1.0000\n",
      "Epoch 280 | Loss 0.0196 | Val 1.0000 | Test 1.0000\n",
      "Early stopping.\n",
      "\n",
      "Final Evaluation (Node-Level)\n",
      "=============================\n",
      "Test Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     1.0000    1.0000    1.0000      9159\n",
      "      trojan     1.0000    1.0000    1.0000     27556\n",
      "\n",
      "    accuracy                         1.0000     36715\n",
      "   macro avg     1.0000    1.0000    1.0000     36715\n",
      "weighted avg     1.0000    1.0000    1.0000     36715\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 9159     0]\n",
      " [    0 27556]]\n"
     ]
    }
   ],
   "source": [
    "# train_gcn_node_fixed.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "NODE_CSV  = \"GNNDatasets/node.csv\"\n",
    "EDGE_CSV  = \"GNNDatasets/node_edges.csv\"\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# ----------------------------- Load nodes -----------------------------\n",
    "nodes_df = pd.read_csv(NODE_CSV)\n",
    "\n",
    "label_col = None\n",
    "for cand in [\"label\", \"is_trojan\", \"trojan\", \"target\"]:\n",
    "    if cand in nodes_df.columns:\n",
    "        label_col = cand; break\n",
    "if label_col is None:\n",
    "    nodes_df[\"label\"] = nodes_df[\"circuit_name\"].astype(str).str.contains(\"__trojan_\").astype(int)\n",
    "    label_col = \"label\"\n",
    "\n",
    "nodes_df[\"uid\"] = nodes_df[\"circuit_name\"].astype(str) + \"::\" + nodes_df[\"node\"].astype(str)\n",
    "\n",
    "feat_df = nodes_df.copy()\n",
    "if \"gate_type\" in feat_df.columns:\n",
    "    gate_oh = pd.get_dummies(feat_df[\"gate_type\"], prefix=\"gt\")\n",
    "    feat_df = pd.concat([feat_df.drop(columns=[\"gate_type\"]), gate_oh], axis=1)\n",
    "\n",
    "exclude = {\"uid\",\"node\",\"circuit_name\",label_col}\n",
    "num_cols = [c for c in feat_df.columns if c not in exclude and pd.api.types.is_numeric_dtype(feat_df[c])]\n",
    "X = feat_df[num_cols].fillna(0.0).values.astype(np.float32)\n",
    "y = nodes_df[label_col].values.astype(np.int64)\n",
    "\n",
    "# ----------------------------- Load edges; add missing nodes -----------------------------\n",
    "edges_df = pd.read_csv(EDGE_CSV)\n",
    "edges_df[\"src_uid\"] = edges_df[\"circuit_name\"].astype(str) + \"::\" + edges_df[\"src\"].astype(str)\n",
    "edges_df[\"dst_uid\"] = edges_df[\"circuit_name\"].astype(str) + \"::\" + edges_df[\"dst\"].astype(str)\n",
    "\n",
    "known_uids = set(nodes_df[\"uid\"])\n",
    "edge_uids = set(edges_df[\"src_uid\"]).union(set(edges_df[\"dst_uid\"]))\n",
    "missing = list(edge_uids - known_uids)\n",
    "\n",
    "if missing:\n",
    "    zero_row = np.zeros((1, X.shape[1]), dtype=np.float32)\n",
    "    addX = np.repeat(zero_row, len(missing), axis=0)\n",
    "    addY = -1*np.ones(len(missing), dtype=np.int64)\n",
    "    add_df = pd.DataFrame({\n",
    "        \"uid\": missing,\n",
    "        \"circuit_name\": [u.split(\"::\",1)[0] for u in missing],\n",
    "        \"node\": [u.split(\"::\",1)[1] for u in missing],\n",
    "        label_col: addY\n",
    "    })\n",
    "    X = np.vstack([X, addX])\n",
    "    y = np.concatenate([y, addY])\n",
    "    nodes_df = pd.concat([nodes_df, add_df], ignore_index=True)\n",
    "\n",
    "uid_to_idx = {u:i for i,u in enumerate(nodes_df[\"uid\"].tolist())}\n",
    "src_idx = edges_df[\"src_uid\"].map(uid_to_idx).dropna().astype(int).values\n",
    "dst_idx = edges_df[\"dst_uid\"].map(uid_to_idx).dropna().astype(int).values\n",
    "edge_index = np.stack([np.concatenate([src_idx, dst_idx]),\n",
    "                       np.concatenate([dst_idx, src_idx])], axis=0)\n",
    "\n",
    "# ----------------------------- Scale features -----------------------------\n",
    "labeled_mask_np = (y >= 0)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "X_scaled[labeled_mask_np] = scaler.fit_transform(X_scaled[labeled_mask_np])\n",
    "if (~labeled_mask_np).any():\n",
    "    X_scaled[~labeled_mask_np] = (X_scaled[~labeled_mask_np] - scaler.mean_) / np.sqrt(scaler.var_ + 1e-8)\n",
    "\n",
    "# ----------------------------- Splits -----------------------------\n",
    "idx_all = np.where(labeled_mask_np)[0]\n",
    "y_all = y[labeled_mask_np]\n",
    "\n",
    "idx_train, idx_tmp, y_train, y_tmp = train_test_split(\n",
    "    idx_all, y_all, test_size=0.30, random_state=SEED, stratify=y_all\n",
    ")\n",
    "idx_val, idx_test, y_val, y_test = train_test_split(\n",
    "    idx_tmp, y_tmp, test_size=0.50, random_state=SEED, stratify=y_tmp\n",
    ")\n",
    "\n",
    "# ----------------------------- Torch tensors (FIX: masks as torch.bool) -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_t = torch.from_numpy(X_scaled).to(device)\n",
    "y_t = torch.from_numpy(y).to(device)\n",
    "\n",
    "edge_index_t = torch.from_numpy(edge_index).long().to(device)\n",
    "\n",
    "train_mask_t = torch.zeros(len(y), dtype=torch.bool, device=device); train_mask_t[idx_train] = True\n",
    "val_mask_t   = torch.zeros(len(y), dtype=torch.bool, device=device); val_mask_t[idx_val]   = True\n",
    "test_mask_t  = torch.zeros(len(y), dtype=torch.bool, device=device); test_mask_t[idx_test]  = True\n",
    "labeled_mask_t = torch.from_numpy(labeled_mask_np).to(device)\n",
    "\n",
    "# ----------------------------- Build GCN adjacency -----------------------------\n",
    "def build_adj(num_nodes, edge_index):\n",
    "    self_loops = torch.arange(num_nodes, device=edge_index.device)\n",
    "    ei = torch.cat([edge_index, torch.stack([self_loops, self_loops])], dim=1)\n",
    "    deg = torch.bincount(ei[0], minlength=num_nodes).float()\n",
    "    deg_inv_sqrt = deg.clamp(min=1).pow(-0.5)\n",
    "    w = deg_inv_sqrt[ei[0]] * deg_inv_sqrt[ei[1]]\n",
    "    A = torch.sparse_coo_tensor(ei, w, (num_nodes, num_nodes))\n",
    "    return A.coalesce()\n",
    "\n",
    "A_t = build_adj(X_t.size(0), edge_index_t)\n",
    "\n",
    "# ----------------------------- Model -----------------------------\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        nn.init.xavier_uniform_(self.lin.weight)\n",
    "    def forward(self, x, adj):\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sparse.mm(adj, x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=96, out_dim=2, dropout=0.35):\n",
    "        super().__init__()\n",
    "        self.g1 = GCNLayer(in_dim, hid_dim, dropout)\n",
    "        self.g2 = GCNLayer(hid_dim, out_dim, dropout)\n",
    "        self.do = nn.Dropout(dropout)\n",
    "    def forward(self, x, adj):\n",
    "        x = self.g1(x, adj); x = F.relu(x); x = self.do(x)\n",
    "        x = self.g2(x, adj)\n",
    "        return x\n",
    "\n",
    "model = GCN(in_dim=X_t.size(1), hid_dim=96, out_dim=2, dropout=0.35).to(device)\n",
    "\n",
    "# ----------------------------- Loss, optimizer -----------------------------\n",
    "train_labels = y_t[train_mask_t]\n",
    "classes, counts = torch.unique(train_labels, return_counts=True)\n",
    "num_pos = counts[classes==1].item() if (classes==1).any() else 1\n",
    "num_neg = counts[classes==0].item() if (classes==0).any() else 1\n",
    "weight_pos = (num_neg + num_pos) / (2.0 * num_pos)\n",
    "weight_neg = (num_neg + num_pos) / (2.0 * num_neg)\n",
    "class_weights = torch.tensor([weight_neg, weight_pos], dtype=torch.float32, device=device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3, weight_decay=5e-4)\n",
    "\n",
    "# ----------------------------- Training -----------------------------\n",
    "def evaluate(mask_t):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_t, A_t)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        msk = mask_t & (y_t >= 0)\n",
    "        if msk.sum() == 0: return 0.0\n",
    "        return (pred[msk] == y_t[msk]).float().mean().item()\n",
    "\n",
    "best_val, best_state = -1.0, None\n",
    "patience, patience_cnt = 20, 0\n",
    "EPOCHS = 300\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(X_t, A_t)\n",
    "    loss = criterion(logits[train_mask_t], y_t[train_mask_t])\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        val_acc = evaluate(val_mask_t)\n",
    "        test_acc = evaluate(test_mask_t)\n",
    "        print(f\"Epoch {epoch:03d} | Loss {loss.item():.4f} | Val {val_acc:.4f} | Test {test_acc:.4f}\")\n",
    "        if val_acc > best_val + 1e-4:\n",
    "            best_val = val_acc\n",
    "            best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "            patience_cnt = 0\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            if patience_cnt >= patience:\n",
    "                print(\"Early stopping.\"); break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "# ----------------------------- Final eval -----------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X_t, A_t)\n",
    "    preds = logits.argmax(dim=1)\n",
    "\n",
    "msk = (test_mask_t & (y_t >= 0)).cpu().numpy()\n",
    "y_true = y_t.cpu().numpy()[msk]\n",
    "y_pred = preds.cpu().numpy()[msk]\n",
    "\n",
    "acc = (y_true == y_pred).mean()\n",
    "print(\"\\nFinal Evaluation (Node-Level)\")\n",
    "print(\"=============================\")\n",
    "print(f\"Test Accuracy: {acc:.4f}\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, labels=[0,1], target_names=[\"clean\",\"trojan\"], digits=4))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=[0,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f180284c-98e0-49d9-9ff9-cef614e6a1ff",
   "metadata": {},
   "source": [
    "#### Subgraph Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c246e2-abee-45de-8107-c8f3b6d71d75",
   "metadata": {},
   "source": [
    "Below is a complete, end‑to‑end script for subgraph‑level classification. It:\n",
    "\n",
    "Loads GNNDatasets/subgraph.csv, GNNDatasets/node.csv, and GNNDatasets/node_edges.csv.\n",
    "\n",
    "Reconstructs per‑circuit graphs (NetworkX) and extracts each subgraph as the K‑hop ego graph around center_node (uses K=2, same as before).\n",
    "\n",
    "Builds a PyTorch‑Geometric Data object per subgraph with node features taken from node.csv (numeric features + one‑hot gate type); missing nodes get zero features.\n",
    "\n",
    "Trains a GraphSAGE model (two layers) with global mean pooling for graph classification (label_subgraph: 0/1).\n",
    "\n",
    "Prints accuracy, classification report, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7416ae58-7249-4b03-a918-6665a4fb18b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building subgraphs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 674/674 [00:19<00:00, 34.49it/s]\n",
      "/home/rrk307/anaconda3/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 674 subgraphs (usable)\n",
      "Train: 471, Val: 101, Test: 102\n",
      "Epoch 001 | Loss 0.7464 | Val 0.2673\n",
      "Epoch 005 | Loss 0.6193 | Val 0.6238\n",
      "Epoch 010 | Loss 0.5896 | Val 0.5941\n",
      "Epoch 015 | Loss 0.5101 | Val 0.6832\n",
      "Epoch 020 | Loss 0.3784 | Val 0.8020\n",
      "Epoch 025 | Loss 0.2963 | Val 0.8317\n",
      "Epoch 030 | Loss 0.2182 | Val 0.8812\n",
      "Epoch 035 | Loss 0.1549 | Val 0.9208\n",
      "Epoch 040 | Loss 0.1265 | Val 0.9208\n",
      "Epoch 045 | Loss 0.1028 | Val 0.9208\n",
      "Epoch 050 | Loss 0.0897 | Val 0.9208\n",
      "Epoch 055 | Loss 0.0840 | Val 0.9208\n",
      "Epoch 060 | Loss 0.0851 | Val 0.9307\n",
      "Epoch 065 | Loss 0.0641 | Val 0.9307\n",
      "Epoch 070 | Loss 0.0713 | Val 0.9604\n",
      "Epoch 075 | Loss 0.0604 | Val 0.9604\n",
      "Epoch 080 | Loss 0.0645 | Val 0.9604\n",
      "\n",
      "Final Evaluation (Subgraph-Level)\n",
      "=================================\n",
      "Test Accuracy: 0.9902\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9167    1.0000    0.9565        11\n",
      "           1     1.0000    0.9890    0.9945        91\n",
      "\n",
      "    accuracy                         0.9902       102\n",
      "   macro avg     0.9583    0.9945    0.9755       102\n",
      "weighted avg     0.9910    0.9902    0.9904       102\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11  0]\n",
      " [ 1 90]]\n"
     ]
    }
   ],
   "source": [
    "# train_subgraph_gnn_fixed.py\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 80\n",
    "LR = 1e-3\n",
    "HID_DIM = 64\n",
    "\n",
    "# -----------------------\n",
    "# Load node features\n",
    "# -----------------------\n",
    "nodes_df = pd.read_csv(\"GNNDatasets/node.csv\")\n",
    "nodes_df[\"uid\"] = nodes_df[\"circuit_name\"].astype(str) + \"::\" + nodes_df[\"node\"].astype(str)\n",
    "\n",
    "# one-hot gate_type\n",
    "if \"gate_type\" in nodes_df.columns:\n",
    "    gate_ohe = pd.get_dummies(nodes_df[\"gate_type\"], prefix=\"gt\")\n",
    "    nodes_feat_df = pd.concat([nodes_df.drop(columns=[\"gate_type\"]), gate_ohe], axis=1)\n",
    "else:\n",
    "    nodes_feat_df = nodes_df.copy()\n",
    "\n",
    "meta_cols = {\"uid\",\"node\",\"circuit_name\",\"label\",\"label_node\",\"label_graph\",\"label_subgraph\",\"folder\"}\n",
    "num_cols = [c for c in nodes_feat_df.columns if c not in meta_cols and pd.api.types.is_numeric_dtype(nodes_feat_df[c])]\n",
    "\n",
    "uid_to_feat = {}\n",
    "for _, r in nodes_feat_df.iterrows():\n",
    "    uid_to_feat[r[\"uid\"]] = r[num_cols].astype(float).values\n",
    "\n",
    "scaler = StandardScaler().fit(np.stack(list(uid_to_feat.values())))\n",
    "for k in list(uid_to_feat.keys()):\n",
    "    uid_to_feat[k] = scaler.transform(uid_to_feat[k].reshape(1,-1)).reshape(-1)\n",
    "\n",
    "feat_dim = len(uid_to_feat[list(uid_to_feat.keys())[0]])\n",
    "\n",
    "# -----------------------\n",
    "# Merge edge CSVs\n",
    "# -----------------------\n",
    "edge_files = [\n",
    "    \"GNNDatasets/node_edges.csv\",\n",
    "    \"GNNDatasets/subgraph_edges_andxor.csv\",\n",
    "    \"GNNDatasets/subgraph_edges_countermux.csv\",\n",
    "    \"GNNDatasets/subgraph_edges_fsmor.csv\",\n",
    "]\n",
    "\n",
    "edges_by_circuit = defaultdict(list)\n",
    "for ef in edge_files:\n",
    "    df = pd.read_csv(ef)\n",
    "    for _, r in df.iterrows():\n",
    "        edges_by_circuit[r[\"circuit_name\"]].append((r[\"src\"], r[\"dst\"]))\n",
    "\n",
    "# -----------------------\n",
    "# Build dataset\n",
    "# -----------------------\n",
    "sub_df = pd.read_csv(\"GNNDatasets/subgraph.csv\")\n",
    "data_list, labels = [], []\n",
    "\n",
    "for idx, row in tqdm(sub_df.iterrows(), total=len(sub_df), desc=\"Building subgraphs\"):\n",
    "    ckt = row[\"circuit_name\"]\n",
    "    lbl = int(row.get(\"label_subgraph\", row.get(\"label\", 0)))\n",
    "    \n",
    "    if ckt not in edges_by_circuit:\n",
    "        continue\n",
    "    \n",
    "    # collect node uids from node.csv that belong to this circuit\n",
    "    sub_nodes = nodes_df[nodes_df[\"circuit_name\"]==ckt][\"node\"].tolist()\n",
    "    if not sub_nodes: \n",
    "        continue\n",
    "    \n",
    "    uid_map = {n:i for i,n in enumerate(sub_nodes)}\n",
    "    x_list = []\n",
    "    for n in sub_nodes:\n",
    "        uid = f\"{ckt}::{n}\"\n",
    "        if uid in uid_to_feat:\n",
    "            x_list.append(uid_to_feat[uid])\n",
    "        else:\n",
    "            x_list.append(np.zeros(feat_dim))\n",
    "    x = torch.tensor(np.vstack(x_list), dtype=torch.float)\n",
    "    \n",
    "    # build edge_index\n",
    "    edge_idx = [[], []]\n",
    "    for u,v in edges_by_circuit[ckt]:\n",
    "        if u in uid_map and v in uid_map:\n",
    "            edge_idx[0].append(uid_map[u]); edge_idx[1].append(uid_map[v])\n",
    "            edge_idx[0].append(uid_map[v]); edge_idx[1].append(uid_map[u])\n",
    "    if not edge_idx[0]:\n",
    "        continue\n",
    "    edge_index = torch.tensor(edge_idx, dtype=torch.long)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index, y=torch.tensor([lbl], dtype=torch.long))\n",
    "    data.circuit_name = ckt\n",
    "    data_list.append(data)\n",
    "    labels.append(lbl)\n",
    "\n",
    "print(f\"Built {len(data_list)} subgraphs (usable)\")\n",
    "\n",
    "# -----------------------\n",
    "# Split\n",
    "# -----------------------\n",
    "labels = np.array(labels)\n",
    "idxs = np.arange(len(data_list))\n",
    "train_idx, temp_idx, y_train, y_temp = train_test_split(idxs, labels, test_size=0.3, \n",
    "                                                        stratify=labels, random_state=RANDOM_SEED)\n",
    "val_idx, test_idx, y_val, y_test = train_test_split(temp_idx, y_temp, test_size=0.5,\n",
    "                                                    stratify=y_temp, random_state=RANDOM_SEED)\n",
    "\n",
    "train_loader = DataLoader([data_list[i] for i in train_idx], batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader([data_list[i] for i in val_idx], batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader([data_list[i] for i in test_idx], batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")\n",
    "\n",
    "# -----------------------\n",
    "# Model\n",
    "# -----------------------\n",
    "class SubgraphClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=HID_DIM, out_dim=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim, hid_dim)\n",
    "        self.conv2 = SAGEConv(hid_dim, hid_dim)\n",
    "        self.lin = nn.Linear(hid_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.4, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.lin(x)\n",
    "\n",
    "model = SubgraphClassifier(feat_dim).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=5e-4)\n",
    "\n",
    "# class weights\n",
    "cls_counts = np.bincount(labels)\n",
    "w = torch.tensor([cls_counts.sum()/c for c in cls_counts], dtype=torch.float32).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(weight=w)\n",
    "\n",
    "# -----------------------\n",
    "# Training loop\n",
    "# -----------------------\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    ys, preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for b in loader:\n",
    "            b = b.to(DEVICE)\n",
    "            out = model(b.x, b.edge_index, b.batch)\n",
    "            p = out.argmax(dim=1).cpu().numpy()\n",
    "            ys.extend(b.y.cpu().numpy())\n",
    "            preds.extend(p)\n",
    "    return np.array(ys), np.array(preds)\n",
    "\n",
    "best_val, best_state = -1, None\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    for b in train_loader:\n",
    "        b = b.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(b.x, b.edge_index, b.batch)\n",
    "        loss = criterion(out, b.y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if epoch%5==0 or epoch==1:\n",
    "        yv,pv = evaluate(val_loader)\n",
    "        acc = accuracy_score(yv,pv)\n",
    "        print(f\"Epoch {epoch:03d} | Loss {total_loss/len(train_loader):.4f} | Val {acc:.4f}\")\n",
    "        if acc>best_val: \n",
    "            best_val=acc; best_state=model.state_dict().copy()\n",
    "\n",
    "# load best\n",
    "if best_state: model.load_state_dict(best_state)\n",
    "\n",
    "# -----------------------\n",
    "# Final test\n",
    "# -----------------------\n",
    "yt,pt = evaluate(test_loader)\n",
    "print(\"\\nFinal Evaluation (Subgraph-Level)\")\n",
    "print(\"=================================\")\n",
    "print(f\"Test Accuracy: {accuracy_score(yt,pt):.4f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(yt,pt,digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(yt,pt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee5b92c-1233-4e96-9950-c2bdc414cdc8",
   "metadata": {},
   "source": [
    "#### Graph Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f73a355-9556-41ed-8e20-213c45b054e8",
   "metadata": {},
   "source": [
    "At this level we want:\n",
    "- Input: GNNDatasets/graph.csv (circuit labels)\n",
    "- Edges: use GNNDatasets/graph_edges.csv (and optionally node_edges/subgraph_edges, but graph_edges.csv should already be the merged top-level edge file).\n",
    "- Nodes: GNNDatasets/node.csv still provides features.\n",
    "\n",
    "We’ll build each circuit as one graph, then classify whether it is clean or trojaned.\n",
    "- Uses GINConv (better for graph classification than vanilla GCN/GraphSAGE).\n",
    "- Loads graph-level edges (graph_edges.csv).\n",
    "- Uses graph.csv for circuit labels.\n",
    "- Includes class-weighted loss (handles class imbalance).\n",
    "- Prints classification report + confusion matrix just like node/subgraph levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e32792be-7e35-4be5-b478-8aca843852f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 92 graphs (usable)\n",
      "Train: 64, Val: 14, Test: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rrk307/anaconda3/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss 0.7626 | Val 0.7143\n",
      "Epoch 005 | Loss 0.6486 | Val 0.2857\n",
      "Epoch 010 | Loss 0.6627 | Val 0.5000\n",
      "Epoch 015 | Loss 0.6499 | Val 0.2857\n",
      "Epoch 020 | Loss 0.6382 | Val 0.2143\n",
      "Epoch 025 | Loss 0.6565 | Val 0.3571\n",
      "Epoch 030 | Loss 0.6186 | Val 0.2143\n",
      "Epoch 035 | Loss 0.6348 | Val 0.1429\n",
      "Epoch 040 | Loss 0.5834 | Val 0.2143\n",
      "Epoch 045 | Loss 0.5532 | Val 0.2143\n",
      "Epoch 050 | Loss 0.5080 | Val 0.2143\n",
      "Epoch 055 | Loss 0.4923 | Val 0.2143\n",
      "Epoch 060 | Loss 0.4683 | Val 0.3571\n",
      "Epoch 065 | Loss 0.4470 | Val 0.3571\n",
      "Epoch 070 | Loss 0.4055 | Val 0.4286\n",
      "Epoch 075 | Loss 0.3791 | Val 0.6429\n",
      "Epoch 080 | Loss 0.3507 | Val 0.7143\n",
      "Epoch 085 | Loss 0.3147 | Val 0.7857\n",
      "Epoch 090 | Loss 0.2813 | Val 0.7857\n",
      "Epoch 095 | Loss 0.2523 | Val 0.7857\n",
      "Epoch 100 | Loss 0.2401 | Val 0.7857\n",
      "\n",
      "Final Evaluation (Graph-Level)\n",
      "=================================\n",
      "Test Accuracy: 0.5714\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3333    0.5000    0.4000         4\n",
      "           1     0.7500    0.6000    0.6667        10\n",
      "\n",
      "    accuracy                         0.5714        14\n",
      "   macro avg     0.5417    0.5500    0.5333        14\n",
      "weighted avg     0.6310    0.5714    0.5905        14\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2 2]\n",
      " [4 6]]\n"
     ]
    }
   ],
   "source": [
    "# train_graph_gnn.py\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GINConv, global_mean_pool\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "LR = 1e-3\n",
    "HID_DIM = 64\n",
    "\n",
    "# -----------------------\n",
    "# Load node features\n",
    "# -----------------------\n",
    "nodes_df = pd.read_csv(\"GNNDatasets/node.csv\")\n",
    "nodes_df[\"uid\"] = nodes_df[\"circuit_name\"].astype(str) + \"::\" + nodes_df[\"node\"].astype(str)\n",
    "\n",
    "# one-hot gate_type\n",
    "if \"gate_type\" in nodes_df.columns:\n",
    "    gate_ohe = pd.get_dummies(nodes_df[\"gate_type\"], prefix=\"gt\")\n",
    "    nodes_feat_df = pd.concat([nodes_df.drop(columns=[\"gate_type\"]), gate_ohe], axis=1)\n",
    "else:\n",
    "    nodes_feat_df = nodes_df.copy()\n",
    "\n",
    "meta_cols = {\"uid\",\"node\",\"circuit_name\",\"label\",\"label_node\",\"label_graph\",\"label_subgraph\",\"folder\"}\n",
    "num_cols = [c for c in nodes_feat_df.columns if c not in meta_cols and pd.api.types.is_numeric_dtype(nodes_feat_df[c])]\n",
    "\n",
    "uid_to_feat = {}\n",
    "for _, r in nodes_feat_df.iterrows():\n",
    "    uid_to_feat[r[\"uid\"]] = r[num_cols].astype(float).values\n",
    "\n",
    "scaler = StandardScaler().fit(np.stack(list(uid_to_feat.values())))\n",
    "for k in list(uid_to_feat.keys()):\n",
    "    uid_to_feat[k] = scaler.transform(uid_to_feat[k].reshape(1,-1)).reshape(-1)\n",
    "\n",
    "feat_dim = len(uid_to_feat[list(uid_to_feat.keys())[0]])\n",
    "\n",
    "# -----------------------\n",
    "# Load graph-level labels\n",
    "# -----------------------\n",
    "graph_df = pd.read_csv(\"GNNDatasets/graph.csv\")\n",
    "graph_labels = {r[\"circuit_name\"]: int(r.get(\"label_graph\", r.get(\"label\", 0))) for _, r in graph_df.iterrows()}\n",
    "\n",
    "# -----------------------\n",
    "# Load graph-level edges\n",
    "# -----------------------\n",
    "edges_df = pd.read_csv(\"GNNDatasets/graph_edges.csv\")\n",
    "edges_by_circuit = defaultdict(list)\n",
    "for _, r in edges_df.iterrows():\n",
    "    edges_by_circuit[r[\"circuit_name\"]].append((r[\"src\"], r[\"dst\"]))\n",
    "\n",
    "# -----------------------\n",
    "# Build dataset\n",
    "# -----------------------\n",
    "data_list, labels = [], []\n",
    "\n",
    "for ckt, lbl in graph_labels.items():\n",
    "    if ckt not in edges_by_circuit:\n",
    "        continue\n",
    "    \n",
    "    # collect node list\n",
    "    sub_nodes = nodes_df[nodes_df[\"circuit_name\"]==ckt][\"node\"].tolist()\n",
    "    if not sub_nodes: \n",
    "        continue\n",
    "    \n",
    "    uid_map = {n:i for i,n in enumerate(sub_nodes)}\n",
    "    x_list = []\n",
    "    for n in sub_nodes:\n",
    "        uid = f\"{ckt}::{n}\"\n",
    "        if uid in uid_to_feat:\n",
    "            x_list.append(uid_to_feat[uid])\n",
    "        else:\n",
    "            x_list.append(np.zeros(feat_dim))\n",
    "    x = torch.tensor(np.vstack(x_list), dtype=torch.float)\n",
    "    \n",
    "    # build edge_index\n",
    "    edge_idx = [[], []]\n",
    "    for u,v in edges_by_circuit[ckt]:\n",
    "        if u in uid_map and v in uid_map:\n",
    "            edge_idx[0].append(uid_map[u]); edge_idx[1].append(uid_map[v])\n",
    "            edge_idx[0].append(uid_map[v]); edge_idx[1].append(uid_map[u])\n",
    "    if not edge_idx[0]:\n",
    "        continue\n",
    "    edge_index = torch.tensor(edge_idx, dtype=torch.long)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index, y=torch.tensor([lbl], dtype=torch.long))\n",
    "    data.circuit_name = ckt\n",
    "    data_list.append(data)\n",
    "    labels.append(lbl)\n",
    "\n",
    "print(f\"Built {len(data_list)} graphs (usable)\")\n",
    "\n",
    "# -----------------------\n",
    "# Split\n",
    "# -----------------------\n",
    "labels = np.array(labels)\n",
    "idxs = np.arange(len(data_list))\n",
    "train_idx, temp_idx, y_train, y_temp = train_test_split(idxs, labels, test_size=0.3, \n",
    "                                                        stratify=labels, random_state=RANDOM_SEED)\n",
    "val_idx, test_idx, y_val, y_test = train_test_split(temp_idx, y_temp, test_size=0.5,\n",
    "                                                    stratify=y_temp, random_state=RANDOM_SEED)\n",
    "\n",
    "train_loader = DataLoader([data_list[i] for i in train_idx], batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader([data_list[i] for i in val_idx], batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader([data_list[i] for i in test_idx], batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")\n",
    "\n",
    "# -----------------------\n",
    "# Model (GIN for graphs)\n",
    "# -----------------------\n",
    "class GraphClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=HID_DIM, out_dim=2):\n",
    "        super().__init__()\n",
    "        nn1 = nn.Sequential(nn.Linear(in_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, hid_dim))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        nn2 = nn.Sequential(nn.Linear(hid_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, hid_dim))\n",
    "        self.conv2 = GINConv(nn2)\n",
    "        self.lin = nn.Linear(hid_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.4, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.lin(x)\n",
    "\n",
    "model = GraphClassifier(feat_dim).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=5e-4)\n",
    "\n",
    "# class weights\n",
    "cls_counts = np.bincount(labels)\n",
    "w = torch.tensor([cls_counts.sum()/c for c in cls_counts], dtype=torch.float32).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(weight=w)\n",
    "\n",
    "# -----------------------\n",
    "# Training loop\n",
    "# -----------------------\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    ys, preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for b in loader:\n",
    "            b = b.to(DEVICE)\n",
    "            out = model(b.x, b.edge_index, b.batch)\n",
    "            p = out.argmax(dim=1).cpu().numpy()\n",
    "            ys.extend(b.y.cpu().numpy())\n",
    "            preds.extend(p)\n",
    "    return np.array(ys), np.array(preds)\n",
    "\n",
    "best_val, best_state = -1, None\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    for b in train_loader:\n",
    "        b = b.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(b.x, b.edge_index, b.batch)\n",
    "        loss = criterion(out, b.y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if epoch%5==0 or epoch==1:\n",
    "        yv,pv = evaluate(val_loader)\n",
    "        acc = accuracy_score(yv,pv)\n",
    "        print(f\"Epoch {epoch:03d} | Loss {total_loss/len(train_loader):.4f} | Val {acc:.4f}\")\n",
    "        if acc>best_val: \n",
    "            best_val=acc; best_state=model.state_dict().copy()\n",
    "\n",
    "# load best\n",
    "if best_state: model.load_state_dict(best_state)\n",
    "\n",
    "# -----------------------\n",
    "# Final test\n",
    "# -----------------------\n",
    "yt,pt = evaluate(test_loader)\n",
    "print(\"\\nFinal Evaluation (Graph-Level)\")\n",
    "print(\"=================================\")\n",
    "print(f\"Test Accuracy: {accuracy_score(yt,pt):.4f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(yt,pt,digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(yt,pt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a3967d-982a-4cfa-bca7-748f0d68f4b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
