{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eb5d540-cb61-42e5-8b5a-942d0ab25031",
   "metadata": {},
   "source": [
    "#### Node Classification: Trojan vs non-trojan. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1ba8c7-82e1-4547-8088-e36ce4f6e7ab",
   "metadata": {},
   "source": [
    "Below is a single, end‑to‑end GCN training script that:\n",
    "\n",
    "reads GNNDatasets/node.csv + GNNDatasets/node_edges.csv\n",
    "\n",
    "auto‑builds a multi‑circuit graph (nodes keyed by circuit_name::node)\n",
    "\n",
    "one‑hot encodes gate_type, standardizes numeric features\n",
    "\n",
    "adds zero‑feature pseudo nodes for edge‑only items (ASSIGN_*, DFF_*, PIs/POs)\n",
    "\n",
    "trains a 2‑layer GCN with class‑weighted loss + early stopping\n",
    "\n",
    "reports accuracy, classification report, confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b5790c1-0a45-4ba9-a072-a5c358305959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss 0.8937 | Val 0.4607 | Test 0.4631\n",
      "Epoch 010 | Loss 0.6115 | Val 0.9505 | Test 0.9496\n",
      "Epoch 020 | Loss 0.4193 | Val 0.9883 | Test 0.9885\n",
      "Epoch 030 | Loss 0.2920 | Val 0.9926 | Test 0.9931\n",
      "Epoch 040 | Loss 0.2113 | Val 0.9981 | Test 0.9981\n",
      "Epoch 050 | Loss 0.1559 | Val 0.9994 | Test 0.9995\n",
      "Epoch 060 | Loss 0.1176 | Val 0.9999 | Test 0.9998\n",
      "Epoch 070 | Loss 0.0936 | Val 0.9999 | Test 0.9998\n",
      "Epoch 080 | Loss 0.0780 | Val 1.0000 | Test 1.0000\n",
      "Epoch 090 | Loss 0.0668 | Val 1.0000 | Test 1.0000\n",
      "Epoch 100 | Loss 0.0553 | Val 1.0000 | Test 1.0000\n",
      "Epoch 110 | Loss 0.0493 | Val 1.0000 | Test 1.0000\n",
      "Epoch 120 | Loss 0.0435 | Val 1.0000 | Test 1.0000\n",
      "Epoch 130 | Loss 0.0407 | Val 1.0000 | Test 1.0000\n",
      "Epoch 140 | Loss 0.0371 | Val 1.0000 | Test 1.0000\n",
      "Epoch 150 | Loss 0.0346 | Val 1.0000 | Test 1.0000\n",
      "Epoch 160 | Loss 0.0329 | Val 1.0000 | Test 1.0000\n",
      "Epoch 170 | Loss 0.0301 | Val 1.0000 | Test 1.0000\n",
      "Epoch 180 | Loss 0.0283 | Val 1.0000 | Test 1.0000\n",
      "Epoch 190 | Loss 0.0273 | Val 1.0000 | Test 1.0000\n",
      "Epoch 200 | Loss 0.0266 | Val 1.0000 | Test 1.0000\n",
      "Epoch 210 | Loss 0.0253 | Val 1.0000 | Test 1.0000\n",
      "Epoch 220 | Loss 0.0241 | Val 1.0000 | Test 1.0000\n",
      "Epoch 230 | Loss 0.0240 | Val 1.0000 | Test 1.0000\n",
      "Epoch 240 | Loss 0.0230 | Val 1.0000 | Test 1.0000\n",
      "Epoch 250 | Loss 0.0210 | Val 1.0000 | Test 1.0000\n",
      "Epoch 260 | Loss 0.0215 | Val 1.0000 | Test 1.0000\n",
      "Epoch 270 | Loss 0.0197 | Val 1.0000 | Test 1.0000\n",
      "Epoch 280 | Loss 0.0196 | Val 1.0000 | Test 1.0000\n",
      "Early stopping.\n",
      "\n",
      "Final Evaluation (Node-Level)\n",
      "=============================\n",
      "Test Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean     1.0000    1.0000    1.0000      9159\n",
      "      trojan     1.0000    1.0000    1.0000     27556\n",
      "\n",
      "    accuracy                         1.0000     36715\n",
      "   macro avg     1.0000    1.0000    1.0000     36715\n",
      "weighted avg     1.0000    1.0000    1.0000     36715\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 9159     0]\n",
      " [    0 27556]]\n"
     ]
    }
   ],
   "source": [
    "# train_gcn_node_fixed.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "NODE_CSV  = \"GNNDatasets/node.csv\"\n",
    "EDGE_CSV  = \"GNNDatasets/node_edges.csv\"\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# ----------------------------- Load nodes -----------------------------\n",
    "nodes_df = pd.read_csv(NODE_CSV)\n",
    "\n",
    "label_col = None\n",
    "for cand in [\"label\", \"is_trojan\", \"trojan\", \"target\"]:\n",
    "    if cand in nodes_df.columns:\n",
    "        label_col = cand; break\n",
    "if label_col is None:\n",
    "    nodes_df[\"label\"] = nodes_df[\"circuit_name\"].astype(str).str.contains(\"__trojan_\").astype(int)\n",
    "    label_col = \"label\"\n",
    "\n",
    "nodes_df[\"uid\"] = nodes_df[\"circuit_name\"].astype(str) + \"::\" + nodes_df[\"node\"].astype(str)\n",
    "\n",
    "feat_df = nodes_df.copy()\n",
    "if \"gate_type\" in feat_df.columns:\n",
    "    gate_oh = pd.get_dummies(feat_df[\"gate_type\"], prefix=\"gt\")\n",
    "    feat_df = pd.concat([feat_df.drop(columns=[\"gate_type\"]), gate_oh], axis=1)\n",
    "\n",
    "exclude = {\"uid\",\"node\",\"circuit_name\",label_col}\n",
    "num_cols = [c for c in feat_df.columns if c not in exclude and pd.api.types.is_numeric_dtype(feat_df[c])]\n",
    "X = feat_df[num_cols].fillna(0.0).values.astype(np.float32)\n",
    "y = nodes_df[label_col].values.astype(np.int64)\n",
    "\n",
    "# ----------------------------- Load edges; add missing nodes -----------------------------\n",
    "edges_df = pd.read_csv(EDGE_CSV)\n",
    "edges_df[\"src_uid\"] = edges_df[\"circuit_name\"].astype(str) + \"::\" + edges_df[\"src\"].astype(str)\n",
    "edges_df[\"dst_uid\"] = edges_df[\"circuit_name\"].astype(str) + \"::\" + edges_df[\"dst\"].astype(str)\n",
    "\n",
    "known_uids = set(nodes_df[\"uid\"])\n",
    "edge_uids = set(edges_df[\"src_uid\"]).union(set(edges_df[\"dst_uid\"]))\n",
    "missing = list(edge_uids - known_uids)\n",
    "\n",
    "if missing:\n",
    "    zero_row = np.zeros((1, X.shape[1]), dtype=np.float32)\n",
    "    addX = np.repeat(zero_row, len(missing), axis=0)\n",
    "    addY = -1*np.ones(len(missing), dtype=np.int64)\n",
    "    add_df = pd.DataFrame({\n",
    "        \"uid\": missing,\n",
    "        \"circuit_name\": [u.split(\"::\",1)[0] for u in missing],\n",
    "        \"node\": [u.split(\"::\",1)[1] for u in missing],\n",
    "        label_col: addY\n",
    "    })\n",
    "    X = np.vstack([X, addX])\n",
    "    y = np.concatenate([y, addY])\n",
    "    nodes_df = pd.concat([nodes_df, add_df], ignore_index=True)\n",
    "\n",
    "uid_to_idx = {u:i for i,u in enumerate(nodes_df[\"uid\"].tolist())}\n",
    "src_idx = edges_df[\"src_uid\"].map(uid_to_idx).dropna().astype(int).values\n",
    "dst_idx = edges_df[\"dst_uid\"].map(uid_to_idx).dropna().astype(int).values\n",
    "edge_index = np.stack([np.concatenate([src_idx, dst_idx]),\n",
    "                       np.concatenate([dst_idx, src_idx])], axis=0)\n",
    "\n",
    "# ----------------------------- Scale features -----------------------------\n",
    "labeled_mask_np = (y >= 0)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "X_scaled[labeled_mask_np] = scaler.fit_transform(X_scaled[labeled_mask_np])\n",
    "if (~labeled_mask_np).any():\n",
    "    X_scaled[~labeled_mask_np] = (X_scaled[~labeled_mask_np] - scaler.mean_) / np.sqrt(scaler.var_ + 1e-8)\n",
    "\n",
    "# ----------------------------- Splits -----------------------------\n",
    "idx_all = np.where(labeled_mask_np)[0]\n",
    "y_all = y[labeled_mask_np]\n",
    "\n",
    "idx_train, idx_tmp, y_train, y_tmp = train_test_split(\n",
    "    idx_all, y_all, test_size=0.30, random_state=SEED, stratify=y_all\n",
    ")\n",
    "idx_val, idx_test, y_val, y_test = train_test_split(\n",
    "    idx_tmp, y_tmp, test_size=0.50, random_state=SEED, stratify=y_tmp\n",
    ")\n",
    "\n",
    "# ----------------------------- Torch tensors (FIX: masks as torch.bool) -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_t = torch.from_numpy(X_scaled).to(device)\n",
    "y_t = torch.from_numpy(y).to(device)\n",
    "\n",
    "edge_index_t = torch.from_numpy(edge_index).long().to(device)\n",
    "\n",
    "train_mask_t = torch.zeros(len(y), dtype=torch.bool, device=device); train_mask_t[idx_train] = True\n",
    "val_mask_t   = torch.zeros(len(y), dtype=torch.bool, device=device); val_mask_t[idx_val]   = True\n",
    "test_mask_t  = torch.zeros(len(y), dtype=torch.bool, device=device); test_mask_t[idx_test]  = True\n",
    "labeled_mask_t = torch.from_numpy(labeled_mask_np).to(device)\n",
    "\n",
    "# ----------------------------- Build GCN adjacency -----------------------------\n",
    "def build_adj(num_nodes, edge_index):\n",
    "    self_loops = torch.arange(num_nodes, device=edge_index.device)\n",
    "    ei = torch.cat([edge_index, torch.stack([self_loops, self_loops])], dim=1)\n",
    "    deg = torch.bincount(ei[0], minlength=num_nodes).float()\n",
    "    deg_inv_sqrt = deg.clamp(min=1).pow(-0.5)\n",
    "    w = deg_inv_sqrt[ei[0]] * deg_inv_sqrt[ei[1]]\n",
    "    A = torch.sparse_coo_tensor(ei, w, (num_nodes, num_nodes))\n",
    "    return A.coalesce()\n",
    "\n",
    "A_t = build_adj(X_t.size(0), edge_index_t)\n",
    "\n",
    "# ----------------------------- Model -----------------------------\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        nn.init.xavier_uniform_(self.lin.weight)\n",
    "    def forward(self, x, adj):\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sparse.mm(adj, x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=96, out_dim=2, dropout=0.35):\n",
    "        super().__init__()\n",
    "        self.g1 = GCNLayer(in_dim, hid_dim, dropout)\n",
    "        self.g2 = GCNLayer(hid_dim, out_dim, dropout)\n",
    "        self.do = nn.Dropout(dropout)\n",
    "    def forward(self, x, adj):\n",
    "        x = self.g1(x, adj); x = F.relu(x); x = self.do(x)\n",
    "        x = self.g2(x, adj)\n",
    "        return x\n",
    "\n",
    "model = GCN(in_dim=X_t.size(1), hid_dim=96, out_dim=2, dropout=0.35).to(device)\n",
    "\n",
    "# ----------------------------- Loss, optimizer -----------------------------\n",
    "train_labels = y_t[train_mask_t]\n",
    "classes, counts = torch.unique(train_labels, return_counts=True)\n",
    "num_pos = counts[classes==1].item() if (classes==1).any() else 1\n",
    "num_neg = counts[classes==0].item() if (classes==0).any() else 1\n",
    "weight_pos = (num_neg + num_pos) / (2.0 * num_pos)\n",
    "weight_neg = (num_neg + num_pos) / (2.0 * num_neg)\n",
    "class_weights = torch.tensor([weight_neg, weight_pos], dtype=torch.float32, device=device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3, weight_decay=5e-4)\n",
    "\n",
    "# ----------------------------- Training -----------------------------\n",
    "def evaluate(mask_t):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_t, A_t)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        msk = mask_t & (y_t >= 0)\n",
    "        if msk.sum() == 0: return 0.0\n",
    "        return (pred[msk] == y_t[msk]).float().mean().item()\n",
    "\n",
    "best_val, best_state = -1.0, None\n",
    "patience, patience_cnt = 20, 0\n",
    "EPOCHS = 300\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(X_t, A_t)\n",
    "    loss = criterion(logits[train_mask_t], y_t[train_mask_t])\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        val_acc = evaluate(val_mask_t)\n",
    "        test_acc = evaluate(test_mask_t)\n",
    "        print(f\"Epoch {epoch:03d} | Loss {loss.item():.4f} | Val {val_acc:.4f} | Test {test_acc:.4f}\")\n",
    "        if val_acc > best_val + 1e-4:\n",
    "            best_val = val_acc\n",
    "            best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "            patience_cnt = 0\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            if patience_cnt >= patience:\n",
    "                print(\"Early stopping.\"); break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "# ----------------------------- Final eval -----------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X_t, A_t)\n",
    "    preds = logits.argmax(dim=1)\n",
    "\n",
    "msk = (test_mask_t & (y_t >= 0)).cpu().numpy()\n",
    "y_true = y_t.cpu().numpy()[msk]\n",
    "y_pred = preds.cpu().numpy()[msk]\n",
    "\n",
    "acc = (y_true == y_pred).mean()\n",
    "print(\"\\nFinal Evaluation (Node-Level)\")\n",
    "print(\"=============================\")\n",
    "print(f\"Test Accuracy: {acc:.4f}\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, labels=[0,1], target_names=[\"clean\",\"trojan\"], digits=4))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=[0,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f180284c-98e0-49d9-9ff9-cef614e6a1ff",
   "metadata": {},
   "source": [
    "#### Subgraph Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c246e2-abee-45de-8107-c8f3b6d71d75",
   "metadata": {},
   "source": [
    "Below is a complete, end‑to‑end script for subgraph‑level classification. It:\n",
    "\n",
    "Loads GNNDatasets/subgraph.csv, GNNDatasets/node.csv, and GNNDatasets/node_edges.csv.\n",
    "\n",
    "Reconstructs per‑circuit graphs (NetworkX) and extracts each subgraph as the K‑hop ego graph around center_node (uses K=2, same as before).\n",
    "\n",
    "Builds a PyTorch‑Geometric Data object per subgraph with node features taken from node.csv (numeric features + one‑hot gate type); missing nodes get zero features.\n",
    "\n",
    "Trains a GraphSAGE model (two layers) with global mean pooling for graph classification (label_subgraph: 0/1).\n",
    "\n",
    "Prints accuracy, classification report, and confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c909024c-ac10-4af1-bdc5-c74ff0e7e2d7",
   "metadata": {},
   "source": [
    "#### Subgraph Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7416ae58-7249-4b03-a918-6665a4fb18b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building subgraphs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 674/674 [00:19<00:00, 34.49it/s]\n",
      "/home/rrk307/anaconda3/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 674 subgraphs (usable)\n",
      "Train: 471, Val: 101, Test: 102\n",
      "Epoch 001 | Loss 0.7464 | Val 0.2673\n",
      "Epoch 005 | Loss 0.6193 | Val 0.6238\n",
      "Epoch 010 | Loss 0.5896 | Val 0.5941\n",
      "Epoch 015 | Loss 0.5101 | Val 0.6832\n",
      "Epoch 020 | Loss 0.3784 | Val 0.8020\n",
      "Epoch 025 | Loss 0.2963 | Val 0.8317\n",
      "Epoch 030 | Loss 0.2182 | Val 0.8812\n",
      "Epoch 035 | Loss 0.1549 | Val 0.9208\n",
      "Epoch 040 | Loss 0.1265 | Val 0.9208\n",
      "Epoch 045 | Loss 0.1028 | Val 0.9208\n",
      "Epoch 050 | Loss 0.0897 | Val 0.9208\n",
      "Epoch 055 | Loss 0.0840 | Val 0.9208\n",
      "Epoch 060 | Loss 0.0851 | Val 0.9307\n",
      "Epoch 065 | Loss 0.0641 | Val 0.9307\n",
      "Epoch 070 | Loss 0.0713 | Val 0.9604\n",
      "Epoch 075 | Loss 0.0604 | Val 0.9604\n",
      "Epoch 080 | Loss 0.0645 | Val 0.9604\n",
      "\n",
      "Final Evaluation (Subgraph-Level)\n",
      "=================================\n",
      "Test Accuracy: 0.9902\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9167    1.0000    0.9565        11\n",
      "           1     1.0000    0.9890    0.9945        91\n",
      "\n",
      "    accuracy                         0.9902       102\n",
      "   macro avg     0.9583    0.9945    0.9755       102\n",
      "weighted avg     0.9910    0.9902    0.9904       102\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11  0]\n",
      " [ 1 90]]\n"
     ]
    }
   ],
   "source": [
    "# train_subgraph_gnn_fixed.py\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 80\n",
    "LR = 1e-3\n",
    "HID_DIM = 64\n",
    "\n",
    "# -----------------------\n",
    "# Load node features\n",
    "# -----------------------\n",
    "nodes_df = pd.read_csv(\"GNNDatasets/node.csv\")\n",
    "nodes_df[\"uid\"] = nodes_df[\"circuit_name\"].astype(str) + \"::\" + nodes_df[\"node\"].astype(str)\n",
    "\n",
    "# one-hot gate_type\n",
    "if \"gate_type\" in nodes_df.columns:\n",
    "    gate_ohe = pd.get_dummies(nodes_df[\"gate_type\"], prefix=\"gt\")\n",
    "    nodes_feat_df = pd.concat([nodes_df.drop(columns=[\"gate_type\"]), gate_ohe], axis=1)\n",
    "else:\n",
    "    nodes_feat_df = nodes_df.copy()\n",
    "\n",
    "meta_cols = {\"uid\",\"node\",\"circuit_name\",\"label\",\"label_node\",\"label_graph\",\"label_subgraph\",\"folder\"}\n",
    "num_cols = [c for c in nodes_feat_df.columns if c not in meta_cols and pd.api.types.is_numeric_dtype(nodes_feat_df[c])]\n",
    "\n",
    "uid_to_feat = {}\n",
    "for _, r in nodes_feat_df.iterrows():\n",
    "    uid_to_feat[r[\"uid\"]] = r[num_cols].astype(float).values\n",
    "\n",
    "scaler = StandardScaler().fit(np.stack(list(uid_to_feat.values())))\n",
    "for k in list(uid_to_feat.keys()):\n",
    "    uid_to_feat[k] = scaler.transform(uid_to_feat[k].reshape(1,-1)).reshape(-1)\n",
    "\n",
    "feat_dim = len(uid_to_feat[list(uid_to_feat.keys())[0]])\n",
    "\n",
    "# -----------------------\n",
    "# Merge edge CSVs\n",
    "# -----------------------\n",
    "edge_files = [\n",
    "    \"GNNDatasets/node_edges.csv\",\n",
    "    \"GNNDatasets/subgraph_edges_andxor.csv\",\n",
    "    \"GNNDatasets/subgraph_edges_countermux.csv\",\n",
    "    \"GNNDatasets/subgraph_edges_fsmor.csv\",\n",
    "]\n",
    "\n",
    "edges_by_circuit = defaultdict(list)\n",
    "for ef in edge_files:\n",
    "    df = pd.read_csv(ef)\n",
    "    for _, r in df.iterrows():\n",
    "        edges_by_circuit[r[\"circuit_name\"]].append((r[\"src\"], r[\"dst\"]))\n",
    "\n",
    "# -----------------------\n",
    "# Build dataset\n",
    "# -----------------------\n",
    "sub_df = pd.read_csv(\"GNNDatasets/subgraph.csv\")\n",
    "data_list, labels = [], []\n",
    "\n",
    "for idx, row in tqdm(sub_df.iterrows(), total=len(sub_df), desc=\"Building subgraphs\"):\n",
    "    ckt = row[\"circuit_name\"]\n",
    "    lbl = int(row.get(\"label_subgraph\", row.get(\"label\", 0)))\n",
    "    \n",
    "    if ckt not in edges_by_circuit:\n",
    "        continue\n",
    "    \n",
    "    # collect node uids from node.csv that belong to this circuit\n",
    "    sub_nodes = nodes_df[nodes_df[\"circuit_name\"]==ckt][\"node\"].tolist()\n",
    "    if not sub_nodes: \n",
    "        continue\n",
    "    \n",
    "    uid_map = {n:i for i,n in enumerate(sub_nodes)}\n",
    "    x_list = []\n",
    "    for n in sub_nodes:\n",
    "        uid = f\"{ckt}::{n}\"\n",
    "        if uid in uid_to_feat:\n",
    "            x_list.append(uid_to_feat[uid])\n",
    "        else:\n",
    "            x_list.append(np.zeros(feat_dim))\n",
    "    x = torch.tensor(np.vstack(x_list), dtype=torch.float)\n",
    "    \n",
    "    # build edge_index\n",
    "    edge_idx = [[], []]\n",
    "    for u,v in edges_by_circuit[ckt]:\n",
    "        if u in uid_map and v in uid_map:\n",
    "            edge_idx[0].append(uid_map[u]); edge_idx[1].append(uid_map[v])\n",
    "            edge_idx[0].append(uid_map[v]); edge_idx[1].append(uid_map[u])\n",
    "    if not edge_idx[0]:\n",
    "        continue\n",
    "    edge_index = torch.tensor(edge_idx, dtype=torch.long)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index, y=torch.tensor([lbl], dtype=torch.long))\n",
    "    data.circuit_name = ckt\n",
    "    data_list.append(data)\n",
    "    labels.append(lbl)\n",
    "\n",
    "print(f\"Built {len(data_list)} subgraphs (usable)\")\n",
    "\n",
    "# -----------------------\n",
    "# Split\n",
    "# -----------------------\n",
    "labels = np.array(labels)\n",
    "idxs = np.arange(len(data_list))\n",
    "train_idx, temp_idx, y_train, y_temp = train_test_split(idxs, labels, test_size=0.3, \n",
    "                                                        stratify=labels, random_state=RANDOM_SEED)\n",
    "val_idx, test_idx, y_val, y_test = train_test_split(temp_idx, y_temp, test_size=0.5,\n",
    "                                                    stratify=y_temp, random_state=RANDOM_SEED)\n",
    "\n",
    "train_loader = DataLoader([data_list[i] for i in train_idx], batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader([data_list[i] for i in val_idx], batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader([data_list[i] for i in test_idx], batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")\n",
    "\n",
    "# -----------------------\n",
    "# Model\n",
    "# -----------------------\n",
    "class SubgraphClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=HID_DIM, out_dim=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim, hid_dim)\n",
    "        self.conv2 = SAGEConv(hid_dim, hid_dim)\n",
    "        self.lin = nn.Linear(hid_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.4, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.lin(x)\n",
    "\n",
    "model = SubgraphClassifier(feat_dim).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=5e-4)\n",
    "\n",
    "# class weights\n",
    "cls_counts = np.bincount(labels)\n",
    "w = torch.tensor([cls_counts.sum()/c for c in cls_counts], dtype=torch.float32).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(weight=w)\n",
    "\n",
    "# -----------------------\n",
    "# Training loop\n",
    "# -----------------------\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    ys, preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for b in loader:\n",
    "            b = b.to(DEVICE)\n",
    "            out = model(b.x, b.edge_index, b.batch)\n",
    "            p = out.argmax(dim=1).cpu().numpy()\n",
    "            ys.extend(b.y.cpu().numpy())\n",
    "            preds.extend(p)\n",
    "    return np.array(ys), np.array(preds)\n",
    "\n",
    "best_val, best_state = -1, None\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    for b in train_loader:\n",
    "        b = b.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(b.x, b.edge_index, b.batch)\n",
    "        loss = criterion(out, b.y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if epoch%5==0 or epoch==1:\n",
    "        yv,pv = evaluate(val_loader)\n",
    "        acc = accuracy_score(yv,pv)\n",
    "        print(f\"Epoch {epoch:03d} | Loss {total_loss/len(train_loader):.4f} | Val {acc:.4f}\")\n",
    "        if acc>best_val: \n",
    "            best_val=acc; best_state=model.state_dict().copy()\n",
    "\n",
    "# load best\n",
    "if best_state: model.load_state_dict(best_state)\n",
    "\n",
    "# -----------------------\n",
    "# Final test\n",
    "# -----------------------\n",
    "yt,pt = evaluate(test_loader)\n",
    "print(\"\\nFinal Evaluation (Subgraph-Level)\")\n",
    "print(\"=================================\")\n",
    "print(f\"Test Accuracy: {accuracy_score(yt,pt):.4f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(yt,pt,digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(yt,pt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee5b92c-1233-4e96-9950-c2bdc414cdc8",
   "metadata": {},
   "source": [
    "#### Graph Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f73a355-9556-41ed-8e20-213c45b054e8",
   "metadata": {},
   "source": [
    "At this level we want:\n",
    "- Input: GNNDatasets/graph.csv (circuit labels)\n",
    "- Edges: use GNNDatasets/graph_edges.csv (and optionally node_edges/subgraph_edges, but graph_edges.csv should already be the merged top-level edge file).\n",
    "- Nodes: GNNDatasets/node.csv still provides features.\n",
    "\n",
    "We’ll build each circuit as one graph, then classify whether it is clean or trojaned.\n",
    "- Uses GINConv (better for graph classification than vanilla GCN/GraphSAGE).\n",
    "- Loads graph-level edges (graph_edges.csv).\n",
    "- Uses graph.csv for circuit labels.\n",
    "- Includes class-weighted loss (handles class imbalance).\n",
    "- Prints classification report + confusion matrix just like node/subgraph levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e32792be-7e35-4be5-b478-8aca843852f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 92 graphs (usable)\n",
      "Train: 64, Val: 14, Test: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rrk307/anaconda3/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss 0.7626 | Val 0.7143\n",
      "Epoch 005 | Loss 0.6486 | Val 0.2857\n",
      "Epoch 010 | Loss 0.6627 | Val 0.5000\n",
      "Epoch 015 | Loss 0.6499 | Val 0.2857\n",
      "Epoch 020 | Loss 0.6382 | Val 0.2143\n",
      "Epoch 025 | Loss 0.6565 | Val 0.3571\n",
      "Epoch 030 | Loss 0.6186 | Val 0.2143\n",
      "Epoch 035 | Loss 0.6348 | Val 0.1429\n",
      "Epoch 040 | Loss 0.5834 | Val 0.2143\n",
      "Epoch 045 | Loss 0.5532 | Val 0.2143\n",
      "Epoch 050 | Loss 0.5080 | Val 0.2143\n",
      "Epoch 055 | Loss 0.4923 | Val 0.2143\n",
      "Epoch 060 | Loss 0.4683 | Val 0.3571\n",
      "Epoch 065 | Loss 0.4470 | Val 0.3571\n",
      "Epoch 070 | Loss 0.4055 | Val 0.4286\n",
      "Epoch 075 | Loss 0.3791 | Val 0.6429\n",
      "Epoch 080 | Loss 0.3507 | Val 0.7143\n",
      "Epoch 085 | Loss 0.3147 | Val 0.7857\n",
      "Epoch 090 | Loss 0.2813 | Val 0.7857\n",
      "Epoch 095 | Loss 0.2523 | Val 0.7857\n",
      "Epoch 100 | Loss 0.2401 | Val 0.7857\n",
      "\n",
      "Final Evaluation (Graph-Level)\n",
      "=================================\n",
      "Test Accuracy: 0.5714\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3333    0.5000    0.4000         4\n",
      "           1     0.7500    0.6000    0.6667        10\n",
      "\n",
      "    accuracy                         0.5714        14\n",
      "   macro avg     0.5417    0.5500    0.5333        14\n",
      "weighted avg     0.6310    0.5714    0.5905        14\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2 2]\n",
      " [4 6]]\n"
     ]
    }
   ],
   "source": [
    "# train_graph_gnn.py\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GINConv, global_mean_pool\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "LR = 1e-3\n",
    "HID_DIM = 64\n",
    "\n",
    "# -----------------------\n",
    "# Load node features\n",
    "# -----------------------\n",
    "nodes_df = pd.read_csv(\"GNNDatasets/node.csv\")\n",
    "nodes_df[\"uid\"] = nodes_df[\"circuit_name\"].astype(str) + \"::\" + nodes_df[\"node\"].astype(str)\n",
    "\n",
    "# one-hot gate_type\n",
    "if \"gate_type\" in nodes_df.columns:\n",
    "    gate_ohe = pd.get_dummies(nodes_df[\"gate_type\"], prefix=\"gt\")\n",
    "    nodes_feat_df = pd.concat([nodes_df.drop(columns=[\"gate_type\"]), gate_ohe], axis=1)\n",
    "else:\n",
    "    nodes_feat_df = nodes_df.copy()\n",
    "\n",
    "meta_cols = {\"uid\",\"node\",\"circuit_name\",\"label\",\"label_node\",\"label_graph\",\"label_subgraph\",\"folder\"}\n",
    "num_cols = [c for c in nodes_feat_df.columns if c not in meta_cols and pd.api.types.is_numeric_dtype(nodes_feat_df[c])]\n",
    "\n",
    "uid_to_feat = {}\n",
    "for _, r in nodes_feat_df.iterrows():\n",
    "    uid_to_feat[r[\"uid\"]] = r[num_cols].astype(float).values\n",
    "\n",
    "scaler = StandardScaler().fit(np.stack(list(uid_to_feat.values())))\n",
    "for k in list(uid_to_feat.keys()):\n",
    "    uid_to_feat[k] = scaler.transform(uid_to_feat[k].reshape(1,-1)).reshape(-1)\n",
    "\n",
    "feat_dim = len(uid_to_feat[list(uid_to_feat.keys())[0]])\n",
    "\n",
    "# -----------------------\n",
    "# Load graph-level labels\n",
    "# -----------------------\n",
    "graph_df = pd.read_csv(\"GNNDatasets/graph.csv\")\n",
    "graph_labels = {r[\"circuit_name\"]: int(r.get(\"label_graph\", r.get(\"label\", 0))) for _, r in graph_df.iterrows()}\n",
    "\n",
    "# -----------------------\n",
    "# Load graph-level edges\n",
    "# -----------------------\n",
    "edges_df = pd.read_csv(\"GNNDatasets/graph_edges.csv\")\n",
    "edges_by_circuit = defaultdict(list)\n",
    "for _, r in edges_df.iterrows():\n",
    "    edges_by_circuit[r[\"circuit_name\"]].append((r[\"src\"], r[\"dst\"]))\n",
    "\n",
    "# -----------------------\n",
    "# Build dataset\n",
    "# -----------------------\n",
    "data_list, labels = [], []\n",
    "\n",
    "for ckt, lbl in graph_labels.items():\n",
    "    if ckt not in edges_by_circuit:\n",
    "        continue\n",
    "    \n",
    "    # collect node list\n",
    "    sub_nodes = nodes_df[nodes_df[\"circuit_name\"]==ckt][\"node\"].tolist()\n",
    "    if not sub_nodes: \n",
    "        continue\n",
    "    \n",
    "    uid_map = {n:i for i,n in enumerate(sub_nodes)}\n",
    "    x_list = []\n",
    "    for n in sub_nodes:\n",
    "        uid = f\"{ckt}::{n}\"\n",
    "        if uid in uid_to_feat:\n",
    "            x_list.append(uid_to_feat[uid])\n",
    "        else:\n",
    "            x_list.append(np.zeros(feat_dim))\n",
    "    x = torch.tensor(np.vstack(x_list), dtype=torch.float)\n",
    "    \n",
    "    # build edge_index\n",
    "    edge_idx = [[], []]\n",
    "    for u,v in edges_by_circuit[ckt]:\n",
    "        if u in uid_map and v in uid_map:\n",
    "            edge_idx[0].append(uid_map[u]); edge_idx[1].append(uid_map[v])\n",
    "            edge_idx[0].append(uid_map[v]); edge_idx[1].append(uid_map[u])\n",
    "    if not edge_idx[0]:\n",
    "        continue\n",
    "    edge_index = torch.tensor(edge_idx, dtype=torch.long)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index, y=torch.tensor([lbl], dtype=torch.long))\n",
    "    data.circuit_name = ckt\n",
    "    data_list.append(data)\n",
    "    labels.append(lbl)\n",
    "\n",
    "print(f\"Built {len(data_list)} graphs (usable)\")\n",
    "\n",
    "# -----------------------\n",
    "# Split\n",
    "# -----------------------\n",
    "labels = np.array(labels)\n",
    "idxs = np.arange(len(data_list))\n",
    "train_idx, temp_idx, y_train, y_temp = train_test_split(idxs, labels, test_size=0.3, \n",
    "                                                        stratify=labels, random_state=RANDOM_SEED)\n",
    "val_idx, test_idx, y_val, y_test = train_test_split(temp_idx, y_temp, test_size=0.5,\n",
    "                                                    stratify=y_temp, random_state=RANDOM_SEED)\n",
    "\n",
    "train_loader = DataLoader([data_list[i] for i in train_idx], batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader([data_list[i] for i in val_idx], batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader([data_list[i] for i in test_idx], batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")\n",
    "\n",
    "# -----------------------\n",
    "# Model (GIN for graphs)\n",
    "# -----------------------\n",
    "class GraphClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=HID_DIM, out_dim=2):\n",
    "        super().__init__()\n",
    "        nn1 = nn.Sequential(nn.Linear(in_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, hid_dim))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        nn2 = nn.Sequential(nn.Linear(hid_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, hid_dim))\n",
    "        self.conv2 = GINConv(nn2)\n",
    "        self.lin = nn.Linear(hid_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.4, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.lin(x)\n",
    "\n",
    "model = GraphClassifier(feat_dim).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=5e-4)\n",
    "\n",
    "# class weights\n",
    "cls_counts = np.bincount(labels)\n",
    "w = torch.tensor([cls_counts.sum()/c for c in cls_counts], dtype=torch.float32).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(weight=w)\n",
    "\n",
    "# -----------------------\n",
    "# Training loop\n",
    "# -----------------------\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    ys, preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for b in loader:\n",
    "            b = b.to(DEVICE)\n",
    "            out = model(b.x, b.edge_index, b.batch)\n",
    "            p = out.argmax(dim=1).cpu().numpy()\n",
    "            ys.extend(b.y.cpu().numpy())\n",
    "            preds.extend(p)\n",
    "    return np.array(ys), np.array(preds)\n",
    "\n",
    "best_val, best_state = -1, None\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    for b in train_loader:\n",
    "        b = b.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(b.x, b.edge_index, b.batch)\n",
    "        loss = criterion(out, b.y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if epoch%5==0 or epoch==1:\n",
    "        yv,pv = evaluate(val_loader)\n",
    "        acc = accuracy_score(yv,pv)\n",
    "        print(f\"Epoch {epoch:03d} | Loss {total_loss/len(train_loader):.4f} | Val {acc:.4f}\")\n",
    "        if acc>best_val: \n",
    "            best_val=acc; best_state=model.state_dict().copy()\n",
    "\n",
    "# load best\n",
    "if best_state: model.load_state_dict(best_state)\n",
    "\n",
    "# -----------------------\n",
    "# Final test\n",
    "# -----------------------\n",
    "yt,pt = evaluate(test_loader)\n",
    "print(\"\\nFinal Evaluation (Graph-Level)\")\n",
    "print(\"=================================\")\n",
    "print(f\"Test Accuracy: {accuracy_score(yt,pt):.4f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(yt,pt,digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(yt,pt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d1bf0d-925c-43a5-a87b-a113e12fe7df",
   "metadata": {},
   "source": [
    "An improved graph‑level training pipeline that combines two tactics to boost real performance on a small dataset:\n",
    "- Stronger regularization + data balancing: oversample the minority class in the training split and use DropEdge augmentation during training.\n",
    "- Model ensemble: train several models (different seeds and architectures — GIN, GraphSAGE, GCN) and average their predicted probabilities on the same test set.\n",
    "\n",
    "This approach reduces variance on small datasets and makes decisions more robust than a single model. Save as train_graph_gnn_ensemble.py and run in the same folder as your GNNDatasets/* CSVs.\n",
    "\n",
    "Quick notes on what this script does and why it should help: \n",
    "- Oversampling balances the small training set so the model sees enough positive (trojan) circuits during training.\n",
    "- DropEdge is a light graph augmentation which acts like dropout for edges — it regularizes models and helps generalization on small graphs.\n",
    "- Ensemble of different architectures & seeds reduces model variance and smooths out model-specific biases that were likely causing your earlier instability.\n",
    "- Early stopping avoids overfitting on a very small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e97f3690-bd9e-4a04-bfd0-c4e07097b891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch_cluster/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "/home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch_spline_conv/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(\n",
      "/home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 92 graphs (usable)\n",
      "Train: 64, Val: 14, Test: 14\n",
      "After oversampling train size: 96\n",
      "[Model 1/5] arch=GIN seed=42 test_acc=0.5714\n",
      "[Model 2/5] arch=SAGE seed=43 test_acc=0.4286\n",
      "[Model 3/5] arch=GCN seed=44 test_acc=0.5714\n",
      "[Model 4/5] arch=GIN seed=45 test_acc=0.7143\n",
      "[Model 5/5] arch=SAGE seed=46 test_acc=0.5000\n",
      "\n",
      "Per-model test accuracies: [0.5714285714285714, 0.42857142857142855, 0.5714285714285714, 0.7142857142857143, 0.5]\n",
      "\n",
      "Ensembled Test Results\n",
      "=======================\n",
      "Accuracy: 0.6428571428571429\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4000    0.5000    0.4444         4\n",
      "           1     0.7778    0.7000    0.7368        10\n",
      "\n",
      "    accuracy                         0.6429        14\n",
      "   macro avg     0.5889    0.6000    0.5906        14\n",
      "weighted avg     0.6698    0.6429    0.6533        14\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2 2]\n",
      " [3 7]]\n"
     ]
    }
   ],
   "source": [
    "# train_graph_gnn_ensemble.py\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GINConv, SAGEConv, GCNConv, global_mean_pool\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED); np.random.seed(RANDOM_SEED); random.seed(RANDOM_SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "N_ENSEMBLE = 5\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 200\n",
    "LR = 1e-3\n",
    "HID_DIM = 64\n",
    "DROPOUT_PROB = 0.4\n",
    "EDGE_DROPOUT = 0.15  # DropEdge probability during training\n",
    "PATIENCE = 30\n",
    "\n",
    "# -----------------------\n",
    "# Load node features (same scheme as before)\n",
    "# -----------------------\n",
    "nodes_df = pd.read_csv(\"GNNDatasets/node.csv\")\n",
    "nodes_df[\"uid\"] = nodes_df[\"circuit_name\"].astype(str) + \"::\" + nodes_df[\"node\"].astype(str)\n",
    "\n",
    "# one-hot gate_type (if present)\n",
    "if \"gate_type\" in nodes_df.columns:\n",
    "    gate_ohe = pd.get_dummies(nodes_df[\"gate_type\"], prefix=\"gt\")\n",
    "    nodes_feat_df = pd.concat([nodes_df.drop(columns=[\"gate_type\"]), gate_ohe], axis=1)\n",
    "else:\n",
    "    nodes_feat_df = nodes_df.copy()\n",
    "\n",
    "meta_cols = {\"uid\",\"node\",\"circuit_name\",\"label\",\"label_node\",\"label_graph\",\"label_subgraph\",\"folder\"}\n",
    "num_cols = [c for c in nodes_feat_df.columns if c not in meta_cols and pd.api.types.is_numeric_dtype(nodes_feat_df[c])]\n",
    "\n",
    "uid_to_feat = {}\n",
    "for _, r in nodes_feat_df.iterrows():\n",
    "    uid_to_feat[r[\"uid\"]] = r[num_cols].astype(float).values\n",
    "\n",
    "# Fit scaler on all nodes\n",
    "all_feats = np.stack(list(uid_to_feat.values()))\n",
    "scaler = StandardScaler().fit(all_feats)\n",
    "for k in list(uid_to_feat.keys()):\n",
    "    uid_to_feat[k] = scaler.transform(uid_to_feat[k].reshape(1,-1)).reshape(-1)\n",
    "feat_dim = all_feats.shape[1]\n",
    "\n",
    "# -----------------------\n",
    "# Load graph labels\n",
    "# -----------------------\n",
    "graph_df = pd.read_csv(\"GNNDatasets/graph.csv\")\n",
    "graph_labels = {r[\"circuit_name\"]: int(r.get(\"label_graph\", r.get(\"label\", 0))) for _, r in graph_df.iterrows()}\n",
    "\n",
    "# -----------------------\n",
    "# Load merged graph edges\n",
    "# -----------------------\n",
    "edges_df = pd.read_csv(\"GNNDatasets/graph_edges.csv\")\n",
    "edges_by_circuit = defaultdict(list)\n",
    "for _, r in edges_df.iterrows():\n",
    "    edges_by_circuit[r[\"circuit_name\"]].append((r[\"src\"], r[\"dst\"]))\n",
    "\n",
    "# -----------------------\n",
    "# Build dataset (Data objects per circuit)\n",
    "# -----------------------\n",
    "data_list = []\n",
    "labels = []\n",
    "for ckt, lbl in graph_labels.items():\n",
    "    if ckt not in edges_by_circuit:\n",
    "        continue\n",
    "    nodes_in_ckt = nodes_df[nodes_df[\"circuit_name\"]==ckt][\"node\"].tolist()\n",
    "    if not nodes_in_ckt:\n",
    "        continue\n",
    "    uid_map = {n:i for i,n in enumerate(nodes_in_ckt)}\n",
    "    x_list = []\n",
    "    for n in nodes_in_ckt:\n",
    "        uid = f\"{ckt}::{n}\"\n",
    "        if uid in uid_to_feat:\n",
    "            x_list.append(uid_to_feat[uid])\n",
    "        else:\n",
    "            x_list.append(np.zeros(feat_dim))\n",
    "    x = torch.tensor(np.vstack(x_list), dtype=torch.float)\n",
    "\n",
    "    # build edge_index (undirected)\n",
    "    srcs, dsts = [], []\n",
    "    for u,v in edges_by_circuit[ckt]:\n",
    "        if u in uid_map and v in uid_map:\n",
    "            srcs.extend([uid_map[u], uid_map[v]])\n",
    "            dsts.extend([uid_map[v], uid_map[u]])\n",
    "    if len(srcs) == 0:\n",
    "        continue\n",
    "    edge_index = torch.tensor([srcs, dsts], dtype=torch.long)\n",
    "    data = Data(x=x, edge_index=edge_index, y=torch.tensor([lbl], dtype=torch.long))\n",
    "    data.circuit_name = ckt\n",
    "    data_list.append(data)\n",
    "    labels.append(lbl)\n",
    "\n",
    "print(f\"Built {len(data_list)} graphs (usable)\")\n",
    "\n",
    "if len(data_list) < 10:\n",
    "    raise SystemExit(\"Too few graphs to train; consider expanding dataset or lowering filters.\")\n",
    "\n",
    "labels = np.array(labels)\n",
    "\n",
    "# -----------------------\n",
    "# Create one fixed stratified split (user wanted reproducible test set)\n",
    "# -----------------------\n",
    "idxs = np.arange(len(data_list))\n",
    "train_idx, temp_idx, y_train, y_temp = train_test_split(idxs, labels, test_size=0.3,\n",
    "                                                        stratify=labels, random_state=RANDOM_SEED)\n",
    "val_idx, test_idx, y_val, y_test = train_test_split(temp_idx, y_temp, test_size=0.5,\n",
    "                                                    stratify=y_temp, random_state=RANDOM_SEED)\n",
    "\n",
    "train_dataset = [data_list[i] for i in train_idx]\n",
    "val_dataset   = [data_list[i] for i in val_idx]\n",
    "test_dataset  = [data_list[i] for i in test_idx]\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "# -----------------------\n",
    "# Oversample minority class in training set by simple replicate-with-replacement\n",
    "# -----------------------\n",
    "def oversample_dataset(dataset):\n",
    "    ys = np.array([int(d.y.item()) for d in dataset])\n",
    "    unique, counts = np.unique(ys, return_counts=True)\n",
    "    if len(unique) == 1:\n",
    "        return dataset  # nothing to do\n",
    "    maxc = counts.max()\n",
    "    new_list = []\n",
    "    for cls in unique:\n",
    "        cls_inds = [i for i,y in enumerate(ys) if y==cls]\n",
    "        times = maxc // len(cls_inds)\n",
    "        rem = maxc % len(cls_inds)\n",
    "        for _ in range(times):\n",
    "            for i in cls_inds:\n",
    "                new_list.append(dataset[i])\n",
    "        sel = np.random.choice(cls_inds, size=rem, replace=False)\n",
    "        for i in sel:\n",
    "            new_list.append(dataset[i])\n",
    "    random.shuffle(new_list)\n",
    "    return new_list\n",
    "\n",
    "train_dataset_bal = oversample_dataset(train_dataset)\n",
    "print(f\"After oversampling train size: {len(train_dataset_bal)}\")\n",
    "\n",
    "# -----------------------\n",
    "# Utility: edge dropout\n",
    "# -----------------------\n",
    "def drop_edges(edge_index, p):\n",
    "    if p <= 0.0:\n",
    "        return edge_index\n",
    "    E = edge_index.size(1)\n",
    "    mask = (torch.rand(E, device=edge_index.device) > p)\n",
    "    # if mask is all false, keep at least one\n",
    "    if mask.sum() == 0:\n",
    "        idx = torch.randint(0, E, (1,), device=edge_index.device)\n",
    "        mask[idx] = True\n",
    "    return edge_index[:, mask]\n",
    "\n",
    "# -----------------------\n",
    "# Models (three types)\n",
    "# -----------------------\n",
    "class GINClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=HID_DIM, out_dim=2, dropout=DROPOUT_PROB):\n",
    "        super().__init__()\n",
    "        nn1 = nn.Sequential(nn.Linear(in_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, hid_dim))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        nn2 = nn.Sequential(nn.Linear(hid_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, hid_dim))\n",
    "        self.conv2 = GINConv(nn2)\n",
    "        self.lin = nn.Linear(hid_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.lin(x)\n",
    "\n",
    "class SAGEClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=HID_DIM, out_dim=2, dropout=DROPOUT_PROB):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim, hid_dim)\n",
    "        self.conv2 = SAGEConv(hid_dim, hid_dim)\n",
    "        self.lin = nn.Linear(hid_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index)); x = self.dropout(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.lin(x)\n",
    "\n",
    "class GCNClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=HID_DIM, out_dim=2, dropout=DROPOUT_PROB):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hid_dim)\n",
    "        self.conv2 = GCNConv(hid_dim, hid_dim)\n",
    "        self.lin = nn.Linear(hid_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index)); x = self.dropout(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.lin(x)\n",
    "\n",
    "archs = [\"GIN\", \"SAGE\", \"GCN\", \"GIN\", \"SAGE\"]  # cycle through\n",
    "\n",
    "# -----------------------\n",
    "# Training helpers\n",
    "# -----------------------\n",
    "def train_single_model(model, train_dataset, val_dataset, seed, edge_drop=EDGE_DROPOUT):\n",
    "    torch.manual_seed(seed); np.random.seed(seed); random.seed(seed)\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=5e-4)\n",
    "    # class weights from train_dataset\n",
    "    ytrain = np.array([int(d.y.item()) for d in train_dataset])\n",
    "    unique, counts = np.unique(ytrain, return_counts=True)\n",
    "    weights = None\n",
    "    if len(unique) == 2:\n",
    "        w_pos = (counts.sum() / counts[1]) / 2.0\n",
    "        w_neg = (counts.sum() / counts[0]) / 2.0\n",
    "        weights = torch.tensor([w_neg, w_pos], dtype=torch.float32, device=DEVICE)\n",
    "    else:\n",
    "        weights = torch.tensor([1.0, 1.0], device=DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    best_val = -1.0; best_state = None; patience_cnt = 0\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            # apply edge dropout on per-batch edge_index\n",
    "            ei = drop_edges(batch.edge_index, edge_drop)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, ei, batch.batch)\n",
    "            loss = criterion(out, batch.y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        avg_loss = total_loss / len(train_dataset)\n",
    "        # validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            ys=[]; ps=[]\n",
    "            for b in val_loader:\n",
    "                b = b.to(DEVICE)\n",
    "                out = model(b.x, b.edge_index, b.batch)\n",
    "                p = out.argmax(dim=1).cpu().numpy()\n",
    "                ys.extend(b.y.cpu().numpy()); ps.extend(p)\n",
    "            if len(ys)>0:\n",
    "                val_acc = accuracy_score(ys, ps)\n",
    "            else:\n",
    "                val_acc = 0.0\n",
    "        # early stopping\n",
    "        if val_acc > best_val + 1e-4:\n",
    "            best_val = val_acc\n",
    "            best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "            patience_cnt = 0\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            if patience_cnt >= PATIENCE:\n",
    "                break\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def predict_probs(model, dataset):\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    model.eval()\n",
    "    probs_all = []\n",
    "    ys = []\n",
    "    with torch.no_grad():\n",
    "        for b in loader:\n",
    "            b = b.to(DEVICE)\n",
    "            logits = model(b.x, b.edge_index, b.batch)\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "            probs_all.append(probs)\n",
    "            ys.extend(b.y.cpu().numpy())\n",
    "    probs_all = np.vstack(probs_all)\n",
    "    return np.array(ys), probs_all\n",
    "\n",
    "# -----------------------\n",
    "# Ensemble training\n",
    "# -----------------------\n",
    "test_probs_accum = None\n",
    "true_test_y = None\n",
    "per_model_test_accs = []\n",
    "\n",
    "for m in range(N_ENSEMBLE):\n",
    "    arch = archs[m % len(archs)]\n",
    "    if arch == \"GIN\":\n",
    "        model = GINClassifier(feat_dim, hid_dim=HID_DIM, out_dim=2)\n",
    "    elif arch == \"SAGE\":\n",
    "        model = SAGEClassifier(feat_dim, hid_dim=HID_DIM, out_dim=2)\n",
    "    else:\n",
    "        model = GCNClassifier(feat_dim, hid_dim=HID_DIM, out_dim=2)\n",
    "\n",
    "    # use the balanced training dataset (oversampled) for every member\n",
    "    trained = train_single_model(model, train_dataset_bal, val_dataset, seed=RANDOM_SEED + m, edge_drop=EDGE_DROPOUT)\n",
    "    ys_test, probs_test = predict_probs(trained, test_dataset)\n",
    "    preds = probs_test.argmax(axis=1)\n",
    "    acc = accuracy_score(ys_test, preds)\n",
    "    per_model_test_accs.append(acc)\n",
    "    print(f\"[Model {m+1}/{N_ENSEMBLE}] arch={arch} seed={RANDOM_SEED+m} test_acc={acc:.4f}\")\n",
    "\n",
    "    if test_probs_accum is None:\n",
    "        test_probs_accum = probs_test.copy()\n",
    "        true_test_y = ys_test.copy()\n",
    "    else:\n",
    "        # ensure test ordering aligns (it should: same test_dataset order)\n",
    "        test_probs_accum += probs_test\n",
    "\n",
    "# Average probabilities\n",
    "test_probs_avg = test_probs_accum / float(N_ENSEMBLE)\n",
    "test_preds_avg = test_probs_avg.argmax(axis=1)\n",
    "\n",
    "# -----------------------\n",
    "# Final report\n",
    "# -----------------------\n",
    "print(\"\\nPer-model test accuracies:\", per_model_test_accs)\n",
    "print(\"\\nEnsembled Test Results\")\n",
    "print(\"=======================\")\n",
    "print(\"Accuracy:\", accuracy_score(true_test_y, test_preds_avg))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_test_y, test_preds_avg, digits=4))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(true_test_y, test_preds_avg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa0e197-7706-4369-a990-943f90d7b2bb",
   "metadata": {},
   "source": [
    "New Strategy — Pretraining + Fine-tuning: \n",
    "- Pretrain GNN encoders at node-level (where you already reached ~99% accuracy).\n",
    "- Then use that encoder as initialization for graph classification — instead of starting from scratch.\n",
    "- This leverages the fact that node embeddings already learned discriminative trojan vs. non-trojan features.\n",
    "\n",
    "Here’s the end-to-end pipeline:\n",
    "- Load node-level pretrained GCN weights\n",
    "- Reuse encoder layers (dropout + convolution stack)\n",
    "- Add a graph pooling head (e.g., global mean pooling) + classifier\n",
    "- Fine-tune on graph-level dataset (GNNDatasets/graph.csv, GNNDatasets/graph_edges.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be792df3-8464-4456-97b3-86bb9ffd0b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circuits split -> train:64 val:14 test:14\n",
      "Pretraining will use nodes from 78 circuits (train+val) and exclude 14 test circuits.\n",
      "Node pretraining sizes (labeled): train=130706 val=28008\n",
      "Pretrain Epoch 001 | Loss 0.7781 | Val 0.6488\n",
      "Pretrain Epoch 005 | Loss 0.6883 | Val 0.6977\n",
      "Pretrain Epoch 010 | Loss 0.5939 | Val 0.9208\n",
      "Pretrain Epoch 015 | Loss 0.5102 | Val 0.9453\n",
      "Pretrain Epoch 020 | Loss 0.4307 | Val 0.9594\n",
      "Pretrain Epoch 025 | Loss 0.3524 | Val 0.9863\n",
      "Pretrain Epoch 030 | Loss 0.2770 | Val 0.9936\n",
      "Pretrain Epoch 035 | Loss 0.2106 | Val 0.9944\n",
      "Pretrain Epoch 040 | Loss 0.1548 | Val 0.9961\n",
      "Pretrain Epoch 045 | Loss 0.1138 | Val 0.9991\n",
      "Pretrain Epoch 050 | Loss 0.0838 | Val 0.9996\n",
      "Pretrain Epoch 055 | Loss 0.0618 | Val 0.9997\n",
      "Pretrain Epoch 060 | Loss 0.0478 | Val 0.9998\n",
      "Pretrain Epoch 065 | Loss 0.0380 | Val 0.9998\n",
      "Pretrain Epoch 070 | Loss 0.0316 | Val 0.9998\n",
      "Pretrain Epoch 075 | Loss 0.0256 | Val 0.9998\n",
      "Pretrain Epoch 080 | Loss 0.0221 | Val 0.9998\n",
      "Pretrain Epoch 085 | Loss 0.0190 | Val 0.9999\n",
      "Pretrain Epoch 090 | Loss 0.0159 | Val 0.9999\n",
      "Pretrain Epoch 095 | Loss 0.0145 | Val 0.9999\n",
      "Pretrain Epoch 100 | Loss 0.0127 | Val 0.9999\n",
      "Saved node-level model checkpoint: node_gcn_pretrained.pth\n",
      "\n",
      "Building graph-level Data objects for fine-tuning (train/val/test circuits) ...\n",
      "Built 92 graphs.\n",
      "Graph counts -> train: 64, val: 14, test: 14\n",
      "\n",
      "Stage 1: freeze encoder and train classifier head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18406/2805370865.py:312: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  node_state = torch.load(PRETRAIN_CHECKPOINT, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune (frozen) Epoch 001 | Val Acc 1.0000 | AvgLoss 0.5945\n",
      "Fine-tune (frozen) Epoch 005 | Val Acc 1.0000 | AvgLoss 0.4577\n",
      "Fine-tune (frozen) Epoch 010 | Val Acc 1.0000 | AvgLoss 0.2530\n",
      "Fine-tune (frozen) Epoch 015 | Val Acc 1.0000 | AvgLoss 0.1850\n",
      "Fine-tune (frozen) Epoch 020 | Val Acc 1.0000 | AvgLoss 0.1366\n",
      "Fine-tune (frozen) Epoch 025 | Val Acc 1.0000 | AvgLoss 0.0988\n",
      "Fine-tune (frozen) Epoch 030 | Val Acc 1.0000 | AvgLoss 0.1019\n",
      "Early stopping fine-tune stage.\n",
      "After head training -> Test Acc: 0.9286\n",
      "\n",
      "Stage 2: unfreeze encoder and fine-tune entire model\n",
      "Fine-tune (all) Epoch 001 | Val Acc 1.0000 | AvgLoss 0.5179\n",
      "Fine-tune (all) Epoch 005 | Val Acc 1.0000 | AvgLoss 0.3262\n",
      "Fine-tune (all) Epoch 010 | Val Acc 1.0000 | AvgLoss 0.2033\n",
      "Fine-tune (all) Epoch 015 | Val Acc 1.0000 | AvgLoss 0.1076\n",
      "Fine-tune (all) Epoch 020 | Val Acc 1.0000 | AvgLoss 0.0639\n",
      "Fine-tune (all) Epoch 025 | Val Acc 1.0000 | AvgLoss 0.0353\n",
      "Fine-tune (all) Epoch 030 | Val Acc 1.0000 | AvgLoss 0.0246\n",
      "Early stopping fine-tune stage.\n",
      "After full fine-tune -> Test Acc: 0.9286\n",
      "\n",
      "Final Evaluation (Graph-level after transfer)\n",
      "=============================================\n",
      "Test Accuracy: 0.9286\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.7500    0.8571         4\n",
      "           1     0.9091    1.0000    0.9524        10\n",
      "\n",
      "    accuracy                         0.9286        14\n",
      "   macro avg     0.9545    0.8750    0.9048        14\n",
      "weighted avg     0.9351    0.9286    0.9252        14\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 3  1]\n",
      " [ 0 10]]\n"
     ]
    }
   ],
   "source": [
    "# transfer_pretrain_node_finetune_graph.py\n",
    "# Node pretraining (GCNConv) -> Graph fine-tune (reuse exact conv weights; no mapping)\n",
    "# Safe: circuits in graph test set are excluded from node pretraining (prevents leakage).\n",
    "\n",
    "import os, random, math\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "# ----------------- Config -----------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NODE_CSV = \"GNNDatasets/node.csv\"\n",
    "NODE_EDGE_CSV = \"GNNDatasets/node_edges.csv\"\n",
    "GRAPH_CSV = \"GNNDatasets/graph.csv\"\n",
    "GRAPH_EDGE_CSV = \"GNNDatasets/graph_edges.csv\"\n",
    "\n",
    "PRETRAIN_CHECKPOINT = \"node_gcn_pretrained.pth\"\n",
    "\n",
    "# Hyperparams (tweakable)\n",
    "HID_DIM = 64\n",
    "PRETRAIN_LR = 1e-3\n",
    "PRETRAIN_EPOCHS = 100\n",
    "PRETRAIN_BATCH = None   # full-graph pretraining uses adjacency; we do whole-graph training (no batch)\n",
    "\n",
    "FT_HEAD_LR = 1e-3\n",
    "FT_FINETUNE_LR = 5e-4\n",
    "FT_EPOCHS_HEAD = 60\n",
    "FT_EPOCHS_FINETUNE = 120\n",
    "BATCH_SIZE = 16\n",
    "EARLY_STOPPING = 30\n",
    "DROPOUT = 0.35\n",
    "\n",
    "# ----------------- Load graph-level labels and split circuits -----------------\n",
    "graph_df = pd.read_csv(GRAPH_CSV)\n",
    "# find graph label column\n",
    "graph_label_col = None\n",
    "for cand in [\"label_graph\", \"label\", \"is_trojan\", \"trojan\"]:\n",
    "    if cand in graph_df.columns:\n",
    "        graph_label_col = cand; break\n",
    "if graph_label_col is None:\n",
    "    graph_df[\"label_graph\"] = graph_df[\"circuit_name\"].astype(str).str.contains(\"__trojan_\").astype(int)\n",
    "    graph_label_col = \"label_graph\"\n",
    "\n",
    "circuits = graph_df[\"circuit_name\"].tolist()\n",
    "graph_labels = [int(x) for x in graph_df[graph_label_col].tolist()]\n",
    "\n",
    "# stratified split of circuits for final graph-level evaluation\n",
    "train_circuits, temp_circuits, y_train_c, y_temp_c = train_test_split(\n",
    "    circuits, graph_labels, test_size=0.30, random_state=SEED, stratify=graph_labels\n",
    ")\n",
    "val_circuits, test_circuits, y_val_c, y_test_c = train_test_split(\n",
    "    temp_circuits, y_temp_c, test_size=0.50, random_state=SEED, stratify=y_temp_c\n",
    ")\n",
    "\n",
    "print(f\"Circuits split -> train:{len(train_circuits)} val:{len(val_circuits)} test:{len(test_circuits)}\")\n",
    "\n",
    "# ----------------- Prepare node data for pretraining (exclude test circuits!) -----------------\n",
    "nodes_df = pd.read_csv(NODE_CSV)\n",
    "edges_df = pd.read_csv(NODE_EDGE_CSV)\n",
    "\n",
    "# identify node label column\n",
    "node_label_col = None\n",
    "for cand in [\"label\", \"is_trojan\", \"trojan\", \"target\"]:\n",
    "    if cand in nodes_df.columns:\n",
    "        node_label_col = cand; break\n",
    "if node_label_col is None:\n",
    "    nodes_df[\"label\"] = nodes_df[\"circuit_name\"].astype(str).str.contains(\"__trojan_\").astype(int)\n",
    "    node_label_col = \"label\"\n",
    "\n",
    "nodes_df[\"uid\"] = nodes_df[\"circuit_name\"].astype(str) + \"::\" + nodes_df[\"node\"].astype(str)\n",
    "\n",
    "# Pretrain set circuits: combine train + val circuits (we exclude graph test circuits)\n",
    "pretrain_circuits = set(train_circuits + val_circuits)\n",
    "print(f\"Pretraining will use nodes from {len(pretrain_circuits)} circuits (train+val) and exclude {len(test_circuits)} test circuits.\")\n",
    "\n",
    "# Filter nodes/edges for pretraining graph\n",
    "nodes_pretrain_df = nodes_df[nodes_df[\"circuit_name\"].isin(pretrain_circuits)].reset_index(drop=True)\n",
    "edges_pretrain_df = edges_df[edges_df[\"circuit_name\"].isin(pretrain_circuits)].reset_index(drop=True)\n",
    "\n",
    "# feature columns (numeric) — exclude metadata\n",
    "feat_df = nodes_pretrain_df.copy()\n",
    "if \"gate_type\" in feat_df.columns:\n",
    "    # one-hot encode gate_type\n",
    "    gate_oh = pd.get_dummies(feat_df[\"gate_type\"], prefix=\"gt\")\n",
    "    feat_df = pd.concat([feat_df.drop(columns=[\"gate_type\"]), gate_oh], axis=1)\n",
    "\n",
    "exclude = {\"uid\",\"node\",\"circuit_name\", node_label_col}\n",
    "feature_cols = [c for c in feat_df.columns if c not in exclude and pd.api.types.is_numeric_dtype(feat_df[c])]\n",
    "if len(feature_cols) == 0:\n",
    "    raise SystemExit(\"No numeric feature columns found in node CSV. Please include features.\")\n",
    "\n",
    "# Build X_all for pretraining nodes and mapping\n",
    "X_pre = feat_df[feature_cols].fillna(0.0).to_numpy(dtype=np.float32)\n",
    "y_pre = nodes_pretrain_df[node_label_col].to_numpy(dtype=np.int64)\n",
    "uids_pre = nodes_pretrain_df[\"uid\"].tolist()\n",
    "uid_to_idx_pre = {u: i for i,u in enumerate(uids_pre)}\n",
    "\n",
    "# Some edges may include nodes not present in nodes_pretrain_df (rare) — filter edges\n",
    "def map_uid(signal, circuit):\n",
    "    return f\"{circuit}::{signal}\"\n",
    "\n",
    "edge_src_uids = edges_pretrain_df.apply(lambda r: map_uid(r[\"src\"], r[\"circuit_name\"]), axis=1)\n",
    "edge_dst_uids = edges_pretrain_df.apply(lambda r: map_uid(r[\"dst\"], r[\"circuit_name\"]), axis=1)\n",
    "\n",
    "edge_src_idx = edge_src_uids.map(uid_to_idx_pre).dropna().astype(int).values\n",
    "edge_dst_idx = edge_dst_uids.map(uid_to_idx_pre).dropna().astype(int).values\n",
    "\n",
    "if len(edge_src_idx) == 0:\n",
    "    raise SystemExit(\"No edges left after filtering to pretrain circuits; check your GNNDatasets files.\")\n",
    "\n",
    "edge_index_pre = np.stack([np.concatenate([edge_src_idx, edge_dst_idx]),\n",
    "                           np.concatenate([edge_dst_idx, edge_src_idx])], axis=0)  # undirected\n",
    "\n",
    "# Scale features using labeled nodes only (within pretrain set)\n",
    "# ensure labels are available\n",
    "labeled_mask_pre = (y_pre >= 0)\n",
    "scaler = StandardScaler()\n",
    "X_pre_scaled = X_pre.copy()\n",
    "if labeled_mask_pre.sum() == 0:\n",
    "    raise SystemExit(\"No labeled nodes in pretraining set.\")\n",
    "X_pre_scaled[labeled_mask_pre] = scaler.fit_transform(X_pre_scaled[labeled_mask_pre])\n",
    "X_pre_scaled[~labeled_mask_pre] = (X_pre_scaled[~labeled_mask_pre] - scaler.mean_) / np.sqrt(scaler.var_ + 1e-8)\n",
    "\n",
    "# Convert to torch\n",
    "X_pre_t = torch.from_numpy(X_pre_scaled).to(DEVICE)\n",
    "y_pre_t = torch.from_numpy(y_pre).to(DEVICE)\n",
    "edge_index_pre_t = torch.from_numpy(edge_index_pre).long().to(DEVICE)\n",
    "\n",
    "# Create train/val split (node-level) for early stopping on pretraining\n",
    "idx_nodes = np.where(labeled_mask_pre)[0]\n",
    "y_nodes = y_pre[labeled_mask_pre]\n",
    "n_train_nodes, n_tmp_nodes = train_test_split(idx_nodes, test_size=0.30, random_state=SEED, stratify=y_nodes)\n",
    "n_val_nodes, n_test_nodes = train_test_split(n_tmp_nodes, test_size=0.50, random_state=SEED,\n",
    "                                             stratify=y_pre[n_tmp_nodes])\n",
    "\n",
    "train_mask_nodes = torch.zeros(len(y_pre), dtype=torch.bool, device=DEVICE); train_mask_nodes[n_train_nodes] = True\n",
    "val_mask_nodes   = torch.zeros(len(y_pre), dtype=torch.bool, device=DEVICE);   val_mask_nodes[n_val_nodes] = True\n",
    "# note: we won't use node_test_nodes later\n",
    "\n",
    "# ----------------- Node GCN pretraining (PyG-style conv) -----------------\n",
    "class NodeGCN(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=HID_DIM, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hid_dim)\n",
    "        self.conv2 = GCNConv(hid_dim, hid_dim)\n",
    "        self.head = nn.Linear(hid_dim, 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return self.head(x)  # per-node logits\n",
    "\n",
    "node_model = NodeGCN(in_dim=X_pre_t.shape[1], hid_dim=HID_DIM).to(DEVICE)\n",
    "\n",
    "# class weights on node-level training labels\n",
    "train_labels_nodes = y_pre_t[train_mask_nodes]\n",
    "classes, counts = torch.unique(train_labels_nodes, return_counts=True)\n",
    "num_pos = int((train_labels_nodes==1).sum().item()) if (train_labels_nodes==1).any() else 1\n",
    "num_neg = int((train_labels_nodes==0).sum().item()) if (train_labels_nodes==0).any() else 1\n",
    "w_pos = (num_neg + num_pos) / (2.0 * num_pos)\n",
    "w_neg = (num_neg + num_pos) / (2.0 * num_neg)\n",
    "class_weights_nodes = torch.tensor([w_neg, w_pos], dtype=torch.float32, device=DEVICE)\n",
    "crit_node = nn.CrossEntropyLoss(weight=class_weights_nodes)\n",
    "opt_node = torch.optim.Adam(node_model.parameters(), lr=PRETRAIN_LR, weight_decay=5e-4)\n",
    "\n",
    "# Pretraining loop\n",
    "best_val = -1.0; best_state = None; patience_cnt = 0\n",
    "print(\"Node pretraining sizes (labeled): train=%d val=%d\" % (train_mask_nodes.sum().item(), val_mask_nodes.sum().item()))\n",
    "for epoch in range(1, PRETRAIN_EPOCHS+1):\n",
    "    node_model.train()\n",
    "    opt_node.zero_grad()\n",
    "    logits_nodes = node_model(X_pre_t, edge_index_pre_t)\n",
    "    loss = crit_node(logits_nodes[train_mask_nodes], y_pre_t[train_mask_nodes])\n",
    "    loss.backward()\n",
    "    opt_node.step()\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        node_model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits_val = node_model(X_pre_t, edge_index_pre_t)\n",
    "            preds_val = logits_val.argmax(dim=1)\n",
    "            if val_mask_nodes.sum() > 0:\n",
    "                val_acc = (preds_val[val_mask_nodes] == y_pre_t[val_mask_nodes]).float().mean().item()\n",
    "            else:\n",
    "                val_acc = 0.0\n",
    "        print(f\"Pretrain Epoch {epoch:03d} | Loss {loss.item():.4f} | Val {val_acc:.4f}\")\n",
    "        if val_acc > best_val + 1e-5:\n",
    "            best_val = val_acc\n",
    "            best_state = {k:v.detach().cpu().clone() for k,v in node_model.state_dict().items()}\n",
    "            patience_cnt = 0\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            if patience_cnt >= EARLY_STOPPING:\n",
    "                print(\"Early stopping node pretrain.\")\n",
    "                break\n",
    "\n",
    "if best_state is not None:\n",
    "    node_model.load_state_dict(best_state)\n",
    "torch.save(node_model.state_dict(), PRETRAIN_CHECKPOINT)\n",
    "print(\"Saved node-level model checkpoint:\", PRETRAIN_CHECKPOINT)\n",
    "\n",
    "# ----------------- Build graph-level dataset (per-circuit Data objects) -----------------\n",
    "print(\"\\nBuilding graph-level Data objects for fine-tuning (train/val/test circuits) ...\")\n",
    "# load full nodes/edges for graphs (use nodes_df, edges_df from earlier)\n",
    "nodes_full_df = pd.read_csv(NODE_CSV)\n",
    "edges_full_df = pd.read_csv(GRAPH_EDGE_CSV)\n",
    "\n",
    "# Prepare feature scaler: use the scaler fitted during pretraining (we already have StandardScaler scaler)\n",
    "# For nodes not seen in pretrain set, we'll apply same scaler transform using scaler.mean_/var_\n",
    "def node_uid(circuit, node):\n",
    "    return f\"{circuit}::{node}\"\n",
    "\n",
    "# build uid->feature map for all nodes (apply scaler to full dataset)\n",
    "feat_full_df = nodes_full_df.copy()\n",
    "if \"gate_type\" in feat_full_df.columns:\n",
    "    gate_oh = pd.get_dummies(feat_full_df[\"gate_type\"], prefix=\"gt\")\n",
    "    feat_full_df = pd.concat([feat_full_df.drop(columns=[\"gate_type\"]), gate_oh], axis=1)\n",
    "# ensure feature columns compatible: if full has extra gate dummies or missing ones compared to pretrain, align\n",
    "for col in feature_cols:\n",
    "    if col not in feat_full_df.columns:\n",
    "        feat_full_df[col] = 0.0\n",
    "feat_full_df = feat_full_df[[\"circuit_name\",\"node\"] + feature_cols]\n",
    "\n",
    "# scale using pretrain scaler (note: scaler was fitted on pretrain labeled nodes' features)\n",
    "full_X = feat_full_df[feature_cols].fillna(0.0).to_numpy(dtype=np.float32)\n",
    "full_X_scaled = (full_X - scaler.mean_) / np.sqrt(scaler.var_ + 1e-8)  # consistent transform\n",
    "feat_full_df[\"scaled_feat\"] = list(full_X_scaled.tolist())\n",
    "\n",
    "# build edges by circuit\n",
    "edges_full_df[\"src_uid\"] = edges_full_df[\"circuit_name\"].astype(str) + \"::\" + edges_full_df[\"src\"].astype(str)\n",
    "edges_full_df[\"dst_uid\"] = edges_full_df[\"circuit_name\"].astype(str) + \"::\" + edges_full_df[\"dst\"].astype(str)\n",
    "edges_by_circuit = defaultdict(list)\n",
    "for _, r in edges_full_df.iterrows():\n",
    "    edges_by_circuit[r[\"circuit_name\"]].append((r[\"src\"], r[\"dst\"]))\n",
    "\n",
    "# build per-circuit Data (only circuits present in graph_labels)\n",
    "graph_data_list = []\n",
    "graph_names = []\n",
    "graph_target = []\n",
    "for _, row in graph_df.iterrows():\n",
    "    ckt = row[\"circuit_name\"]\n",
    "    lbl = int(row[graph_label_col])\n",
    "    # nodes of this circuit\n",
    "    sub_nodes = feat_full_df[feat_full_df[\"circuit_name\"]==ckt]\n",
    "    if sub_nodes.shape[0] == 0: \n",
    "        continue\n",
    "    node_names = sub_nodes[\"node\"].tolist()\n",
    "    uid_map = {n:i for i,n in enumerate(node_names)}\n",
    "    X_nodes = np.vstack(sub_nodes[\"scaled_feat\"].values).astype(np.float32)\n",
    "    # build edge_index\n",
    "    srcs, dsts = [], []\n",
    "    if ckt in edges_by_circuit:\n",
    "        for u,v in edges_by_circuit[ckt]:\n",
    "            if u in uid_map and v in uid_map:\n",
    "                srcs.extend([uid_map[u], uid_map[v]])\n",
    "                dsts.extend([uid_map[v], uid_map[u]])\n",
    "    if len(srcs) == 0:\n",
    "        # skip graphs without edges (unlikely)\n",
    "        continue\n",
    "    edge_index = torch.tensor([srcs, dsts], dtype=torch.long)\n",
    "    data = Data(x=torch.tensor(X_nodes, dtype=torch.float), edge_index=edge_index, y=torch.tensor([lbl], dtype=torch.long))\n",
    "    data.circuit_name = ckt\n",
    "    graph_data_list.append(data)\n",
    "    graph_names.append(ckt)\n",
    "    graph_target.append(lbl)\n",
    "\n",
    "print(f\"Built {len(graph_data_list)} graphs.\")\n",
    "\n",
    "# Create train/val/test lists by circuit split we made earlier\n",
    "def filter_by_circuit(list_data, circuits_set):\n",
    "    idxs = [i for i,d in enumerate(list_data) if d.circuit_name in circuits_set]\n",
    "    return [list_data[i] for i in idxs]\n",
    "\n",
    "train_graphs = filter_by_circuit(graph_data_list, set(train_circuits))\n",
    "val_graphs   = filter_by_circuit(graph_data_list, set(val_circuits))\n",
    "test_graphs  = filter_by_circuit(graph_data_list, set(test_circuits))\n",
    "\n",
    "print(f\"Graph counts -> train: {len(train_graphs)}, val: {len(val_graphs)}, test: {len(test_graphs)}\")\n",
    "\n",
    "# ----------------- Graph classifier reusing conv layers from node_model -----------------\n",
    "class GraphClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=HID_DIM, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hid_dim)\n",
    "        self.conv2 = GCNConv(hid_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hid_dim, 2)\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = F.relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h, edge_index)\n",
    "        g = global_mean_pool(h, batch)\n",
    "        return self.classifier(g)\n",
    "\n",
    "graph_model = GraphClassifier(in_dim=X_pre_t.shape[1], hid_dim=HID_DIM).to(DEVICE)\n",
    "\n",
    "# Copy conv weights from node_model -> graph_model (shapes align because both use GCNConv)\n",
    "node_state = torch.load(PRETRAIN_CHECKPOINT, map_location=\"cpu\")\n",
    "# node_state contains keys: conv1.lin.weight, conv1.lin.bias? Check keys\n",
    "for k_src, v_src in node_state.items():\n",
    "    if \"conv1\" in k_src and \"weight\" in k_src:\n",
    "        # copy to graph_model conv1 weight if exists\n",
    "        if k_src in graph_model.state_dict() and graph_model.state_dict()[k_src].shape == v_src.shape:\n",
    "            graph_model.state_dict()[k_src].copy_(v_src)\n",
    "    if \"conv2\" in k_src and \"weight\" in k_src:\n",
    "        if k_src in graph_model.state_dict() and graph_model.state_dict()[k_src].shape == v_src.shape:\n",
    "            graph_model.state_dict()[k_src].copy_(v_src)\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# --- replace the function definition (rename it) ---\n",
    "def train_graphs_fn(model, train_set, val_set, test_set, freeze_encoder=True):\n",
    "    # dataloaders\n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    if freeze_encoder:\n",
    "        for p in model.conv1.parameters(): p.requires_grad = False\n",
    "        for p in model.conv2.parameters(): p.requires_grad = False\n",
    "    else:\n",
    "        for p in model.parameters(): p.requires_grad = True\n",
    "\n",
    "    # class weights\n",
    "    ytrain = np.array([int(d.y.item()) for d in train_set])\n",
    "    if len(np.unique(ytrain)) == 2:\n",
    "        counts = np.bincount(ytrain); w = torch.tensor([ (counts.sum()/counts[0]), (counts.sum()/counts[1]) ], dtype=torch.float32).to(DEVICE)\n",
    "        criterion = nn.CrossEntropyLoss(weight=w)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=FT_HEAD_LR if freeze_encoder else FT_FINETUNE_LR, weight_decay=5e-4)\n",
    "    best_val = -1.0; best_state = None; pcount = 0\n",
    "\n",
    "    for epoch in range(1, FT_EPOCHS_HEAD + 1 if freeze_encoder else FT_EPOCHS_FINETUNE + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch.x, batch.edge_index, batch.batch)\n",
    "            loss = criterion(logits, batch.y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        ys, ps = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(DEVICE)\n",
    "                out = model(batch.x, batch.edge_index, batch.batch)\n",
    "                preds = out.argmax(dim=1)\n",
    "                ys.extend(batch.y.cpu().numpy()); ps.extend(preds.cpu().numpy())\n",
    "        val_acc = accuracy_score(ys, ps) if len(ys)>0 else 0.0\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(f\"Fine-tune ({'frozen' if freeze_encoder else 'all'}) Epoch {epoch:03d} | Val Acc {val_acc:.4f} | AvgLoss {total_loss / max(1,len(train_set)):.4f}\")\n",
    "        if val_acc > best_val + 1e-4:\n",
    "            best_val = val_acc\n",
    "            best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "            pcount = 0\n",
    "        else:\n",
    "            pcount += 1\n",
    "            if pcount >= EARLY_STOPPING:\n",
    "                print(\"Early stopping fine-tune stage.\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # test eval\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            out = model(batch.x, batch.edge_index, batch.batch)\n",
    "            preds = out.argmax(dim=1)\n",
    "            ys.extend(batch.y.cpu().numpy()); ps.extend(preds.cpu().numpy())\n",
    "    test_acc = accuracy_score(ys, ps) if len(ys)>0 else 0.0\n",
    "    return test_acc, ys, ps\n",
    "\n",
    "# Stage 1: freeze encoder, train head\n",
    "print(\"\\nStage 1: freeze encoder and train classifier head\")\n",
    "for p in graph_model.conv1.parameters(): p.requires_grad = False\n",
    "for p in graph_model.conv2.parameters(): p.requires_grad = False\n",
    "# Stage 1 call:\n",
    "test_acc_1, ys1, ps1 = train_graphs_fn(graph_model, train_graphs, val_graphs, test_graphs, freeze_encoder=True)\n",
    "print(\"After head training -> Test Acc: {:.4f}\".format(test_acc_1))\n",
    "\n",
    "# Stage 2: unfreeze and fine-tune all (lower lr)\n",
    "print(\"\\nStage 2: unfreeze encoder and fine-tune entire model\")\n",
    "for p in graph_model.parameters(): p.requires_grad = True\n",
    "# update global lr for finetune\n",
    "FT_HEAD_LR = FT_FINETUNE_LR\n",
    "# Stage 2 call:\n",
    "test_acc_2, ys2, ps2 = train_graphs_fn(graph_model, train_graphs, val_graphs, test_graphs, freeze_encoder=False)\n",
    "print(\"After full fine-tune -> Test Acc: {:.4f}\".format(test_acc_2))\n",
    "\n",
    "# Final report\n",
    "final_ys, final_ps = (ys2, ps2) if len(ys2)>0 else (ys1, ps1)\n",
    "print(\"\\nFinal Evaluation (Graph-level after transfer)\")\n",
    "print(\"=============================================\")\n",
    "print(f\"Test Accuracy: {accuracy_score(final_ys, final_ps):.4f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(final_ys, final_ps, digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(final_ys, final_ps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2785901-d496-4567-8ec3-3890d2d6afb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
