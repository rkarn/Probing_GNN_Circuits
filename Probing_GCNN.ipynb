{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d2f0719",
   "metadata": {},
   "source": [
    "#### Training GCN\n",
    "\n",
    "Model using the graph dataset extracted from the csv file. TO generate csv file, please run the parsing notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52bbdb56",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Œ Dataset Overview:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60882 entries, 0 to 60881\n",
      "Data columns (total 18 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   circuit_name            60882 non-null  object \n",
      " 1   node                    60882 non-null  object \n",
      " 2   gate_type               60882 non-null  object \n",
      " 3   fan_in                  60882 non-null  int64  \n",
      " 4   fan_out                 60882 non-null  int64  \n",
      " 5   depth                   60882 non-null  object \n",
      " 6   dist_to_output          60882 non-null  int64  \n",
      " 7   is_primary_input        60882 non-null  int64  \n",
      " 8   is_primary_output       60882 non-null  int64  \n",
      " 9   is_internal             60882 non-null  int64  \n",
      " 10  is_key_gate             60882 non-null  int64  \n",
      " 11  key_dependency          122 non-null    object \n",
      " 12  degree_centrality       60882 non-null  float64\n",
      " 13  betweenness_centrality  60882 non-null  float64\n",
      " 14  closeness_centrality    60882 non-null  float64\n",
      " 15  clustering_coefficient  60882 non-null  float64\n",
      " 16  avg_fan_in_neighbors    60882 non-null  float64\n",
      " 17  avg_fan_out_neighbors   60882 non-null  float64\n",
      "dtypes: float64(6), int64(7), object(5)\n",
      "memory usage: 8.4+ MB\n",
      "None\n",
      "\n",
      "ðŸ” Extracted 6172 edges.\n",
      "Epoch 0/50, Loss: 2.3544\n",
      "Epoch 10/50, Loss: 1.0805\n",
      "Epoch 20/50, Loss: 0.5065\n",
      "Epoch 30/50, Loss: 0.3541\n",
      "Epoch 40/50, Loss: 0.2935\n",
      "\n",
      "âœ… Test Accuracy on Original Graph: 92.53%\n",
      "\n",
      "ðŸ“ Confusion Matrix (Original Graph):\n",
      "[[5555    0   23   53    6    0    0    0]\n",
      " [   0  473    0    0    2    0    0    0]\n",
      " [ 427    0   61   38    1    0    0    0]\n",
      " [ 153    0    2  412    0    0    0    0]\n",
      " [   4    0    1    3 4470    0    0    0]\n",
      " [ 197    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0  266    0]\n",
      " [   0    0    0    0    0    0    0   30]]\n",
      "\n",
      "Classification Report (Original Graph):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         and       0.88      0.99      0.93      5637\n",
      "       input       1.00      1.00      1.00       475\n",
      "        nand       0.70      0.12      0.20       527\n",
      "         nor       0.81      0.73      0.77       567\n",
      "         not       1.00      1.00      1.00      4478\n",
      "          or       0.00      0.00      0.00       197\n",
      "      output       1.00      1.00      1.00       266\n",
      "         xor       1.00      1.00      1.00        30\n",
      "\n",
      "    accuracy                           0.93     12177\n",
      "   macro avg       0.80      0.73      0.74     12177\n",
      "weighted avg       0.90      0.93      0.90     12177\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rrk307\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\rrk307\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\rrk307\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Load Dataset and Prepare Data\n",
    "# ------------------------------\n",
    "df = pd.read_csv(\"all_circuits_features.csv\")\n",
    "print(\"\\nðŸ“Œ Dataset Overview:\")\n",
    "print(df.info())\n",
    "\n",
    "gate_types = ['and', 'or', 'nand', 'nor', 'xor', 'xnor', 'buf', 'not']\n",
    "\n",
    "# Convert 'gate_type' to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"gate_label\"] = label_encoder.fit_transform(df[\"gate_type\"])\n",
    "\n",
    "# Feature columns\n",
    "feature_columns = [\n",
    "    \"fan_in\", \"fan_out\", \"dist_to_output\", \"is_primary_input\", \"is_primary_output\",\n",
    "    \"is_internal\", \"is_key_gate\", \"degree_centrality\", \"betweenness_centrality\",\n",
    "    \"closeness_centrality\", \"clustering_coefficient\", \"avg_fan_in_neighbors\", \"avg_fan_out_neighbors\"\n",
    "]\n",
    "\n",
    "# Drop NaN values and convert features to float\n",
    "df = df.dropna(subset=feature_columns)\n",
    "df[feature_columns] = df[feature_columns].astype(float)\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "df[feature_columns] = scaler.fit_transform(df[feature_columns])\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Build Node and Edge Lists\n",
    "# ---------------------------\n",
    "nodes = df[\"node\"].tolist()\n",
    "node_to_id = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "# Build edge list based on fan-in relationships (directed: from source to current node)\n",
    "edges = []\n",
    "for idx, row in df.iterrows():\n",
    "    node_id = node_to_id[row[\"node\"]]\n",
    "    # Select potential driving nodes: those with fan_out > 0\n",
    "    potential_sources = df[df[\"fan_out\"] > 0][\"node\"].tolist()\n",
    "    num_fan_in = int(row[\"fan_in\"])\n",
    "    if num_fan_in > 0:\n",
    "        sources = potential_sources[:num_fan_in]\n",
    "        for src in sources:\n",
    "            if src in node_to_id:\n",
    "                edges.append((node_to_id[src], node_id))\n",
    "\n",
    "print(\"\\nðŸ” Extracted\", len(edges), \"edges.\")\n",
    "if len(edges) == 0:\n",
    "    raise ValueError(\"No edges found! Check your fan_in values.\")\n",
    "\n",
    "# Convert edges into tensors\n",
    "src_nodes, dst_nodes = zip(*edges) if edges else ([], [])\n",
    "src_tensor = torch.tensor(src_nodes, dtype=torch.int64)\n",
    "dst_tensor = torch.tensor(dst_nodes, dtype=torch.int64)\n",
    "valid_edges = (src_tensor >= 0) & (dst_tensor >= 0) & (src_tensor < len(nodes)) & (dst_tensor < len(nodes))\n",
    "src_tensor = src_tensor[valid_edges]\n",
    "dst_tensor = dst_tensor[valid_edges]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Create the Original DGL Graph\n",
    "# ---------------------------\n",
    "graph = dgl.graph((src_tensor, dst_tensor), num_nodes=len(nodes))\n",
    "graph = dgl.add_self_loop(graph)  # add self-loops to avoid zero in-degree issues\n",
    "\n",
    "# Assign node features and labels\n",
    "graph.ndata['features'] = torch.tensor(df[feature_columns].values, dtype=torch.float32)\n",
    "graph.ndata['labels'] = torch.tensor(df[\"gate_label\"].values, dtype=torch.long)\n",
    "\n",
    "# Create train-test split using node indices\n",
    "nodes_idx = np.arange(len(nodes))\n",
    "train_idx, test_idx = train_test_split(nodes_idx, test_size=0.2, random_state=42)\n",
    "train_mask = torch.tensor(train_idx, dtype=torch.long)\n",
    "test_mask = torch.tensor(test_idx, dtype=torch.long)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Define the GCNN Model\n",
    "# ---------------------------\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, out_feats):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = dglnn.GraphConv(in_feats, hidden_feats, allow_zero_in_degree=True)\n",
    "        self.conv2 = dglnn.GraphConv(hidden_feats, out_feats, allow_zero_in_degree=True)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        x = self.conv1(g, inputs)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(g, x)\n",
    "        return x\n",
    "\n",
    "in_feats = len(feature_columns)\n",
    "hidden_feats = 32\n",
    "out_feats = len(label_encoder.classes_)\n",
    "model = GCN(in_feats, hidden_feats, out_feats)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Training Loop\n",
    "# ---------------------------\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    logits = model(graph, graph.ndata['features'])\n",
    "    loss = loss_fn(logits[train_mask], graph.ndata['labels'][train_mask])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Evaluation\n",
    "# ---------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(graph, graph.ndata['features'])\n",
    "    test_logits = logits[test_mask]\n",
    "    test_predictions = test_logits.argmax(dim=1)\n",
    "    orig_accuracy = (test_predictions == graph.ndata['labels'][test_mask]).float().mean().item()\n",
    "    \n",
    "    print(f\"\\nâœ… Test Accuracy on Original Graph: {orig_accuracy * 100:.2f}%\")\n",
    "    \n",
    "    true_labels_orig = graph.ndata['labels'][test_mask].cpu().numpy()\n",
    "    pred_labels_orig = test_predictions.cpu().numpy()\n",
    "    conf_mat_orig = confusion_matrix(true_labels_orig, pred_labels_orig)\n",
    "    \n",
    "    print(\"\\nðŸ“ Confusion Matrix (Original Graph):\")\n",
    "    print(conf_mat_orig)\n",
    "    \n",
    "    print(\"\\nClassification Report (Original Graph):\")\n",
    "    print(classification_report(true_labels_orig, pred_labels_orig, target_names=label_encoder.classes_))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018700d",
   "metadata": {},
   "source": [
    "#### Jacobian \n",
    "Calculation for one sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30f42bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Jacobian Computation and Verification ---\n",
      "\n",
      "Selected test node index: 54063\n",
      "Original feature vector for test node (x0): tensor([ 0.5852,  3.3096, -0.2307, -0.2018, -0.1490,  0.2546, -0.0448,  0.2462,\n",
      "         0.4904, -0.4170, -0.2078,  0.2185,  0.1667], requires_grad=True)\n",
      "\n",
      "Jacobian shape: torch.Size([8, 13])\n",
      "Jacobian matrix:\n",
      " tensor([[ 1.1751,  0.1264,  0.3150,  0.0690, -0.1401, -0.2460, -0.9988, -0.4929,\n",
      "         -0.4157, -1.2658, -0.9800,  0.3919, -1.0841],\n",
      "        [-0.3199, -0.1870,  0.1485,  1.1556,  0.6583, -1.2490,  0.1671,  0.7602,\n",
      "         -0.3451,  0.0817,  0.0779,  0.4733, -0.0856],\n",
      "        [ 0.1512, -0.4985, -1.3084, -0.3012, -0.1043,  0.5629, -0.2641,  0.9349,\n",
      "         -0.4111,  0.1425,  0.4399,  0.8889, -0.6913],\n",
      "        [ 0.2978, -0.1672, -0.3140,  0.2385, -0.0732,  0.3313, -0.1602, -0.1774,\n",
      "          0.4853,  0.2406,  1.1527, -0.2559,  0.2984],\n",
      "        [-2.5618, -0.0189,  0.2247, -0.6293,  0.4530,  1.2460, -0.2286, -0.2451,\n",
      "         -0.2844, -0.6528, -0.1483,  0.5295,  0.0229],\n",
      "        [ 0.3291, -0.2021, -0.0362,  0.1762,  0.1658, -0.3737,  0.5645,  0.5776,\n",
      "         -0.0203,  0.8282, -0.0561, -0.2138,  0.0264],\n",
      "        [ 0.0544, -0.6380,  0.0396,  0.1436,  0.9209, -0.1495,  0.7336,  0.6157,\n",
      "         -0.2194,  0.7213,  0.0718,  0.4602,  0.4369],\n",
      "        [-0.3397, -0.6830, -0.0871,  0.1537,  0.6558, -0.2772,  1.5989,  0.9222,\n",
      "          0.1117,  1.4755,  0.3377,  0.2514,  0.5097]])\n",
      "\n",
      "Perturbation delta vector: tensor([-0.0004,  0.0003, -0.0007, -0.0003,  0.0005,  0.0013,  0.0016,  0.0002,\n",
      "         0.0018, -0.0010,  0.0005,  0.0013,  0.0002])\n",
      "\n",
      "Predicted change in output (Jacobian * delta): tensor([-0.0024, -0.0014,  0.0017,  0.0011,  0.0032, -0.0008,  0.0008,  0.0019])\n",
      "Actual change in output after perturbation: tensor([-0.0024, -0.0014,  0.0017,  0.0011,  0.0032, -0.0008,  0.0008,  0.0019],\n",
      "       grad_fn=<SubBackward0>)\n",
      "\n",
      "Relative error between predicted and actual change: 9.8014e-05\n",
      "\n",
      "Outcome: The Jacobian accurately captures the local sensitivity of the model's output relative to small perturbations in the input.\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# 7. Jacobian Computation and Verification\n",
    "#########################\n",
    "\n",
    "print(\"\\n--- Jacobian Computation and Verification ---\\n\")\n",
    "\n",
    "# Select one test node. Here, we simply choose the first node in the test set.\n",
    "test_idx = test_mask[0].item()\n",
    "print(f\"Selected test node index: {test_idx}\")\n",
    "\n",
    "# Get the feature vector for this test node and set it to require gradients.\n",
    "x0 = graph.ndata['features'][test_idx].clone().detach().requires_grad_(True)\n",
    "print(\"Original feature vector for test node (x0):\", x0)\n",
    "\n",
    "# Define a helper function that takes a feature vector for test_idx and returns the model's output (logit vector)\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Given an input feature vector x for the selected test node (test_idx),\n",
    "    create a new features tensor by replacing the test node's features and compute the model's output.\n",
    "    \"\"\"\n",
    "    # Clone the original features from the graph.\n",
    "    new_features = graph.ndata['features'].clone().detach()\n",
    "    # Replace the test node's feature vector with x.\n",
    "    new_features[test_idx] = x\n",
    "    # Forward propagate through the model (the graph structure remains unchanged).\n",
    "    output = model(graph, new_features)\n",
    "    # Return the output (logit vector) for the test node.\n",
    "    return output[test_idx]\n",
    "\n",
    "# Compute the Jacobian of f with respect to x0.\n",
    "# The resulting Jacobian has shape (num_classes, feature_dim)\n",
    "jacobian = torch.autograd.functional.jacobian(f, x0)\n",
    "\n",
    "print(\"\\nJacobian shape:\", jacobian.shape)\n",
    "print(\"Jacobian matrix:\\n\", jacobian)\n",
    "\n",
    "#########################\n",
    "# 8. Jacobian Verification via Finite Differences\n",
    "#########################\n",
    "\n",
    "# Choose a small random perturbation delta.\n",
    "epsilon = 1e-3\n",
    "delta = epsilon * torch.randn_like(x0)\n",
    "print(\"\\nPerturbation delta vector:\", delta)\n",
    "\n",
    "# Compute the predicted change in output using the linear approximation: J * delta.\n",
    "predicted_change = jacobian.mv(delta)  # matrix-vector product\n",
    "print(\"\\nPredicted change in output (Jacobian * delta):\", predicted_change)\n",
    "\n",
    "# Now compute the actual change in output by explicitly perturbing x0.\n",
    "f_x0 = f(x0)\n",
    "f_x0_perturbed = f(x0 + delta)\n",
    "actual_change = f_x0_perturbed - f_x0\n",
    "print(\"Actual change in output after perturbation:\", actual_change)\n",
    "\n",
    "# Calculate the relative error between the predicted change and the actual change.\n",
    "relative_error = (predicted_change - actual_change).norm() / (actual_change.norm() + 1e-8)\n",
    "print(\"\\nRelative error between predicted and actual change: {:.4e}\".format(relative_error.item()))\n",
    "\n",
    "#########################\n",
    "# 9. Outcome Interpretation\n",
    "#########################\n",
    "\n",
    "if relative_error < 1e-2:\n",
    "    print(\"\\nOutcome: The Jacobian accurately captures the local sensitivity of the model's output relative to small perturbations in the input.\")\n",
    "else:\n",
    "    print(\"\\nOutcome: The relatively high error indicates that higher-order effects or estimation noise may be present, suggesting that the local model behavior is less linear.\")\n",
    "\n",
    "#############################################\n",
    "# 10. Discussion: Verifying Jacobian Information\n",
    "#############################################\n",
    "#\n",
    "# The procedure above gives us two key pieces of information:\n",
    "#\n",
    "# 1. The Jacobian matrix tells us how significantly each output logit is affected by changes in each feature component.\n",
    "#    - For example, large entries in the Jacobian indicate that a small change in that particular input dimension produces a large change in the output.\n",
    "#\n",
    "# 2. By comparing the linear prediction (via the Jacobian) to the actual output change for a small input alteration,\n",
    "#    we verify (via low relative error) that the Jacobian is a valid local approximation.\n",
    "#\n",
    "# In the context of circuit analysis:\n",
    "# - High local sensitivity (i.e. large norms of the Jacobian) might indicate that the GCNN's decision function is \"sharp\" or brittle in that region.\n",
    "# - An adversary could use such information to target the most sensitive input directions, while a defender might regularize the model to minimize the Jacobian norm.\n",
    "#\n",
    "# Verification:\n",
    "# - The relative error is used as a measure: If it is low (e.g., < 1%), then the linear approximation (Jacobian) is a reliable indicator of local sensitivity.\n",
    "# - This provides confidence that the model's predictions are indeed highly sensitive to specific segments of the input space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81e433b",
   "metadata": {},
   "source": [
    "##### Several Samples per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3969940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class-wise Jacobian Analysis Results:\n",
      "Class            Avg. Jacobian Norm Â± Std           Avg. Relative Error Â± Std\n",
      "-----------------------------------------------------------------------------\n",
      "and                 7.8267 Â± 1.8320            1.6930e-03 Â± 1.5638e-02\n",
      "or                  3.7826 Â± 0.1303            3.0280e-04 Â± 1.4921e-04\n",
      "nand                6.4220 Â± 2.1018            3.0992e-03 Â± 2.9427e-02\n",
      "nor                 4.7018 Â± 1.1635            1.8139e-04 Â± 1.3186e-04\n",
      "xor                 8.0677 Â± 1.0923            1.1946e-03 Â± 6.7491e-03\n",
      "xnor                6.4301 Â± 2.6591            5.6770e-03 Â± 5.3800e-02\n",
      "buf                 3.4224 Â± 0.3137            2.2229e-03 Â± 1.3015e-02\n",
      "not                 3.7574 Â± 0.2653            5.2594e-04 Â± 3.4820e-04\n",
      "\n",
      "Overall Aggregated Jacobian Analysis Results:\n",
      "Average Jacobian Norm: 5.7233 Â± 2.3320\n",
      "Average Relative Error: 1.9902e-03 Â± 2.4107e-02\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define gate types corresponding to the labels\n",
    "gate_types = ['and', 'or', 'nand', 'nor', 'xor', 'xnor', 'buf', 'not']\n",
    "\n",
    "# Dictionary to store metrics for each class\n",
    "class_metrics = {gt: {'jacobian_norms': [], 'relative_errors': []} for gt in gate_types}\n",
    "overall_jacobian_norms = []\n",
    "overall_relative_errors = []\n",
    "\n",
    "# Function to compute the Jacobian for a given test index.\n",
    "def compute_jacobian_for_sample(test_idx):\n",
    "    # Get the feature vector for the selected test node and require gradients.\n",
    "    x0 = graph.ndata['features'][test_idx].clone().detach().requires_grad_(True)\n",
    "    def f(x):\n",
    "        new_features = graph.ndata['features'].clone().detach()\n",
    "        new_features[test_idx] = x\n",
    "        output = model(graph, new_features)\n",
    "        return output[test_idx]\n",
    "    # Compute the Jacobian of f with respect to x0.\n",
    "    jacobian = torch.autograd.functional.jacobian(f, x0)\n",
    "    return jacobian, x0, f\n",
    "\n",
    "# Loop over each selected test node (100 per class)\n",
    "for test_idx in selected_test_nodes:\n",
    "    # Determine the ground-truth label and corresponding gate type.\n",
    "    label_idx = graph.ndata['labels'][test_idx].item()\n",
    "    class_name = gate_types[label_idx]\n",
    "    \n",
    "    # Compute the Jacobian for the selected test node.\n",
    "    jacobian, x0, f = compute_jacobian_for_sample(test_idx)\n",
    "    # Compute the Frobenius norm of the Jacobian.\n",
    "    jacobian_norm = torch.norm(jacobian, p='fro').item()\n",
    "    \n",
    "    # Finite-difference verification:\n",
    "    epsilon = 1e-3\n",
    "    delta = epsilon * torch.randn_like(x0)\n",
    "    predicted_change = jacobian.mv(delta)\n",
    "    f_x0 = f(x0)\n",
    "    f_x0_perturbed = f(x0 + delta)\n",
    "    actual_change = f_x0_perturbed - f_x0\n",
    "    relative_error = (torch.norm(predicted_change - actual_change) /\n",
    "                      (torch.norm(actual_change) + 1e-8)).item()\n",
    "    \n",
    "    # Store values for the current sample.\n",
    "    class_metrics[class_name]['jacobian_norms'].append(jacobian_norm)\n",
    "    class_metrics[class_name]['relative_errors'].append(relative_error)\n",
    "    overall_jacobian_norms.append(jacobian_norm)\n",
    "    overall_relative_errors.append(relative_error)\n",
    "\n",
    "# Display per-class results in a tabular format.\n",
    "print(\"\\nClass-wise Jacobian Analysis Results:\")\n",
    "header = \"{:<10s} {:>30s} {:>35s}\".format(\"Class\", \"Avg. Jacobian Norm Â± Std\", \"Avg. Relative Error Â± Std\")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for cls in gate_types:\n",
    "    norms = class_metrics[cls]['jacobian_norms']\n",
    "    errors = class_metrics[cls]['relative_errors']\n",
    "    avg_norm = np.mean(norms) if norms else 0\n",
    "    std_norm = np.std(norms) if norms else 0\n",
    "    avg_rel_error = np.mean(errors) if errors else 0\n",
    "    std_rel_error = np.std(errors) if errors else 0\n",
    "    print(\"{:<10s} {:>15.4f} Â± {:<12.4f} {:>15.4e} Â± {:<10.4e}\".format(\n",
    "        cls, avg_norm, std_norm, avg_rel_error, std_rel_error))\n",
    "\n",
    "# Compute overall aggregated metrics.\n",
    "overall_avg_norm = np.mean(overall_jacobian_norms)\n",
    "overall_std_norm = np.std(overall_jacobian_norms)\n",
    "overall_avg_rel_error = np.mean(overall_relative_errors)\n",
    "overall_std_rel_error = np.std(overall_relative_errors)\n",
    "\n",
    "print(\"\\nOverall Aggregated Jacobian Analysis Results:\")\n",
    "print(\"Average Jacobian Norm: {:.4f} Â± {:.4f}\".format(overall_avg_norm, overall_std_norm))\n",
    "print(\"Average Relative Error: {:.4e} Â± {:.4e}\".format(overall_avg_rel_error, overall_std_rel_error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46efec1",
   "metadata": {},
   "source": [
    "#### Local Lipschitz constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf46d7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Local Lipschitz Constant Computation and Verification (100 Samples Per Class) ---\n",
      "\n",
      "\n",
      "Class-wise Local Lipschitz Constant Results:\n",
      "Class        Avg Lipschitz Constant Â± Std\n",
      "-----------------------------------------\n",
      "and                 5.8381 Â± 1.3819      \n",
      "or                  2.6307 Â± 0.0743      \n",
      "nand                4.7173 Â± 1.5420      \n",
      "nor                 3.2976 Â± 0.9815      \n",
      "xor                 6.7301 Â± 1.0445      \n",
      "xnor                4.6441 Â± 1.8426      \n",
      "buf                 2.3263 Â± 0.1241      \n",
      "not                 2.7390 Â± 0.2534      \n",
      "\n",
      "Overall Aggregated Local Lipschitz Constant:\n",
      "Average Lipschitz Constant: 4.2474 Â± 1.9138\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Local Lipschitz Constant Computation and Verification (100 Samples Per Class) ---\\n\")\n",
    "\n",
    "# Initialize dictionaries to store the local Lipschitz constants (L_i) for each class.\n",
    "class_lipschitz = {gt: [] for gt in gate_types}\n",
    "overall_lipschitz = []\n",
    "\n",
    "def compute_jacobian_for_sample(test_idx):\n",
    "    \"\"\"\n",
    "    Compute the Jacobian matrix for a given test index.\n",
    "    Returns the Jacobian, the original input x0, and the output function f.\n",
    "    \"\"\"\n",
    "    x0 = graph.ndata['features'][test_idx].clone().detach().requires_grad_(True)\n",
    "    \n",
    "    def f(x):\n",
    "        new_features = graph.ndata['features'].clone().detach()\n",
    "        new_features[test_idx] = x\n",
    "        output = model(graph, new_features)\n",
    "        return output[test_idx]\n",
    "    \n",
    "    jacobian = torch.autograd.functional.jacobian(f, x0)\n",
    "    return jacobian, x0, f\n",
    "\n",
    "# Iterate over each selected test node from the chosen 100 samples per class.\n",
    "for test_idx in selected_test_nodes:\n",
    "    label_idx = graph.ndata['labels'][test_idx].item()\n",
    "    class_name = gate_types[label_idx]\n",
    "    \n",
    "    # Compute the Jacobian for this sample.\n",
    "    jacobian, x0, f = compute_jacobian_for_sample(test_idx)\n",
    "    \n",
    "    # The local Lipschitz constant is approximated as the spectral norm (i.e., the largest singular value)\n",
    "    # of the Jacobian matrix.\n",
    "    L_local = torch.linalg.norm(jacobian, ord=2).item()\n",
    "    \n",
    "    # Store the value per class and overall.\n",
    "    class_lipschitz[class_name].append(L_local)\n",
    "    overall_lipschitz.append(L_local)\n",
    "\n",
    "# Print the per-class results in a tabular format.\n",
    "print(\"\\nClass-wise Local Lipschitz Constant Results:\")\n",
    "header = \"{:<10s} {:>30s}\".format(\"Class\", \"Avg Lipschitz Constant Â± Std\")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for cls in gate_types:\n",
    "    values = class_lipschitz[cls]\n",
    "    if len(values) > 0:\n",
    "        avg_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "    else:\n",
    "        avg_val = 0\n",
    "        std_val = 0\n",
    "    print(\"{:<10s} {:>15.4f} Â± {:<12.4f}\".format(cls, avg_val, std_val))\n",
    "    \n",
    "# Compute and display the overall aggregated metrics.\n",
    "overall_avg = np.mean(overall_lipschitz)\n",
    "overall_std = np.std(overall_lipschitz)\n",
    "print(\"\\nOverall Aggregated Local Lipschitz Constant:\")\n",
    "print(\"Average Lipschitz Constant: {:.4f} Â± {:.4f}\".format(overall_avg, overall_std))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b5349e",
   "metadata": {},
   "source": [
    "#### Hessian-Based Curvature Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2de8353f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hessian-Based Curvature Measure Computation (100 Samples Per Class) ---\n",
      "\n",
      "\n",
      "Class-wise Hessian-Based Curvature Results:\n",
      "Class                Avg. Max Eigenvalue Â± Std\n",
      "----------------------------------------------\n",
      "and                 0.0000 Â± 0.0000         \n",
      "or                  0.0000 Â± 0.0000         \n",
      "nand                0.0000 Â± 0.0000         \n",
      "nor                 0.0000 Â± 0.0000         \n",
      "xor                 0.0000 Â± 0.0000         \n",
      "xnor                0.0000 Â± 0.0000         \n",
      "buf                 0.0000 Â± 0.0000         \n",
      "not                 0.0000 Â± 0.0000         \n",
      "\n",
      "Overall Aggregated Hessian-Based Curvature Measure:\n",
      "Average Max Eigenvalue: 0.0000 Â± 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Hessian-Based Curvature Measure Computation (100 Samples Per Class) ---\\n\")\n",
    "\n",
    "# Define gate types corresponding to your labels.\n",
    "gate_types = ['and', 'or', 'nand', 'nor', 'xor', 'xnor', 'buf', 'not']\n",
    "\n",
    "# Dictionary to store the maximum eigenvalue (lambda_max) for each gate type.\n",
    "class_hessian_eig = {gt: [] for gt in gate_types}\n",
    "overall_hessian_eig = []\n",
    "\n",
    "def compute_hessian_for_sample(test_idx):\n",
    "    \"\"\"\n",
    "    Compute the Hessian-based curvature measure for a given test sample.\n",
    "    We define a scalar function h(x) as the log probability (via log-softmax) \n",
    "    of the predicted class (determined on the unperturbed input). This helps\n",
    "    introduce nonlinearity, thereby yielding nonzero second-order derivatives.\n",
    "    \"\"\"\n",
    "    # Retrieve the feature vector for the test node and require gradients.\n",
    "    x0 = graph.ndata['features'][test_idx].clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # Determine the predicted class for the test node.\n",
    "    with torch.no_grad():\n",
    "        logits = model(graph, graph.ndata['features'])\n",
    "        pred_class = torch.argmax(logits[test_idx])\n",
    "    \n",
    "    # Define a scalar function h(x) that returns log probability for the predicted class.\n",
    "    def h(x):\n",
    "        new_features = graph.ndata['features'].clone().detach()\n",
    "        new_features[test_idx] = x\n",
    "        logits = model(graph, new_features)[test_idx]\n",
    "        log_probs = torch.log_softmax(logits, dim=0)\n",
    "        return log_probs[pred_class]\n",
    "    \n",
    "    # Compute the Hessian of h(x) at x0.\n",
    "    H = torch.autograd.functional.hessian(h, x0, create_graph=False, vectorize=False)\n",
    "    H = H.detach()\n",
    "    \n",
    "    # Compute eigenvalues using a symmetric eigendecomposition.\n",
    "    eigvals = torch.linalg.eigvalsh(H)\n",
    "    lambda_max = torch.max(eigvals).item()\n",
    "    return lambda_max, x0, h\n",
    "\n",
    "# Iterate over each selected test node (assumed to be 100 samples per class).\n",
    "for test_idx in selected_test_nodes:\n",
    "    label_idx = graph.ndata['labels'][test_idx].item()\n",
    "    class_name = gate_types[label_idx]\n",
    "    \n",
    "    lambda_max, x0, h_func = compute_hessian_for_sample(test_idx)\n",
    "    class_hessian_eig[class_name].append(lambda_max)\n",
    "    overall_hessian_eig.append(lambda_max)\n",
    "\n",
    "# Display per-class results in a tabular format.\n",
    "print(\"\\nClass-wise Hessian-Based Curvature Results:\")\n",
    "header = \"{:<10s} {:>35s}\".format(\"Class\", \"Avg. Max Eigenvalue Â± Std\")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for cls in gate_types:\n",
    "    values = class_hessian_eig[cls]\n",
    "    if len(values) > 0:\n",
    "        avg_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "    else:\n",
    "        avg_val, std_val = 0, 0\n",
    "    print(\"{:<10s} {:>15.4f} Â± {:<15.4f}\".format(cls, avg_val, std_val))\n",
    "\n",
    "# Compute and display the overall aggregated result.\n",
    "overall_avg = np.mean(overall_hessian_eig)\n",
    "overall_std = np.std(overall_hessian_eig)\n",
    "print(\"\\nOverall Aggregated Hessian-Based Curvature Measure:\")\n",
    "print(\"Average Max Eigenvalue: {:.4f} Â± {:.4f}\".format(overall_avg, overall_std))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4debe1da",
   "metadata": {},
   "source": [
    "#### Prediction Margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd7ab3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction Margin Computation (100 Samples Per Class) ---\n",
      "\n",
      "\n",
      "Class-wise Prediction Margin Results:\n",
      "Class                             Avg. Margin Â± Std\n",
      "---------------------------------------------------\n",
      "and                 3.4587 Â± 1.4896         \n",
      "or                  5.7445 Â± 0.8630         \n",
      "nand                1.2225 Â± 0.6649         \n",
      "nor                 1.4505 Â± 0.7046         \n",
      "xor                 4.8346 Â± 1.0507         \n",
      "xnor                2.0727 Â± 1.1462         \n",
      "buf                 6.7341 Â± 0.5587         \n",
      "not                 4.5673 Â± 0.4763         \n",
      "\n",
      "Overall Aggregated Prediction Margin:\n",
      "Average Margin: 3.6833 Â± 2.2053\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Prediction Margin Computation (100 Samples Per Class) ---\\n\")\n",
    "\n",
    "# Define gate types corresponding to your labels.\n",
    "gate_types = ['and', 'or', 'nand', 'nor', 'xor', 'xnor', 'buf', 'not']\n",
    "\n",
    "# Dictionary to store prediction margins for each gate type.\n",
    "class_margin = {gt: [] for gt in gate_types}\n",
    "overall_margin = []\n",
    "\n",
    "# Iterate over each selected test node.\n",
    "for test_idx in selected_test_nodes:\n",
    "    # Obtain model output (logit vector) for the test sample.\n",
    "    with torch.no_grad():\n",
    "        logits = model(graph, graph.ndata['features'])[test_idx]  # f(x_i)\n",
    "    \n",
    "    # Predicted class: argmax_j f_j(x_i)\n",
    "    pred_class = int(torch.argmax(logits))\n",
    "    pred_logit = logits[pred_class].item()\n",
    "    \n",
    "    # Obtain the second highest value: set pred_class index to a very low number,\n",
    "    # then take the max of the rest.\n",
    "    other_logits = logits.clone()\n",
    "    other_logits[pred_class] = -float('inf')\n",
    "    second_max = other_logits.max().item()\n",
    "    \n",
    "    # Prediction margin: f_{y_pred}(x_i) - max_{j \\neq y_pred} f_j(x_i)\n",
    "    margin = pred_logit - second_max\n",
    "    \n",
    "    # Group the margin by the actual gate type label.\n",
    "    label_idx = graph.ndata['labels'][test_idx].item()\n",
    "    class_name = gate_types[label_idx]\n",
    "    class_margin[class_name].append(margin)\n",
    "    overall_margin.append(margin)\n",
    "\n",
    "# Display per-class results in a tabular format.\n",
    "print(\"\\nClass-wise Prediction Margin Results:\")\n",
    "header = \"{:<10s} {:>40s}\".format(\"Class\", \"Avg. Margin Â± Std\")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "for cls in gate_types:\n",
    "    values = class_margin[cls]\n",
    "    if values:\n",
    "        avg_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "    else:\n",
    "        avg_val, std_val = 0, 0\n",
    "    print(\"{:<10s} {:>15.4f} Â± {:<15.4f}\".format(cls, avg_val, std_val))\n",
    "\n",
    "# Compute and display the overall aggregated prediction margin.\n",
    "overall_avg_margin = np.mean(overall_margin)\n",
    "overall_std_margin = np.std(overall_margin)\n",
    "print(\"\\nOverall Aggregated Prediction Margin:\")\n",
    "print(\"Average Margin: {:.4f} Â± {:.4f}\".format(overall_avg_margin, overall_std_margin))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a96ce7",
   "metadata": {},
   "source": [
    "#### Adversarial Robustness Radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2175c8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Adversarial Robustness Radius Computation (100 Samples Per Class) ---\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_55232\\2544620608.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mclass_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgate_types\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mradius\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madversarial_radius_for_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m     \u001b[0mclass_adv_radius\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mradius\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0moverall_adv_radius\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mradius\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_55232\\2544620608.py\u001b[0m in \u001b[0;36madversarial_radius_for_sample\u001b[1;34m(test_idx, initial_epsilon, growth_factor, max_epsilon, bs_iters, num_trials)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# Increase epsilon until the classification changes or max_epsilon is reached.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[1;32mwhile\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmax_epsilon\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_same\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[0mepsilon\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mgrowth_factor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_55232\\2544620608.py\u001b[0m in \u001b[0;36mis_same\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m# Helper function: returns True if prediction at x remains unchanged.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mis_same\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf_for_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_55232\\2544620608.py\u001b[0m in \u001b[0;36mf_for_sample\u001b[1;34m(x, test_idx)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mnew_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_55232\\3990547334.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, g, inputs)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, graph, feat, weight, edge_weight)\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[0mfeat_src\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeat_dst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpand_as_pair\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_norm\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'both'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m                 \u001b[0mdegs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_degrees\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeat_src\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_norm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'both'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m                     \u001b[0mnorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdegs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dgl\\heterograph.py\u001b[0m in \u001b[0;36mout_degrees\u001b[1;34m(self, u, etype)\u001b[0m\n\u001b[0;32m   3509\u001b[0m             \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrcnodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrctype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3510\u001b[0m         \u001b[0mu_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'u'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3511\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_nodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mntype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msrctype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3512\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mDGLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'u contains invalid node IDs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3513\u001b[0m         \u001b[0mdeg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_degrees\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'u'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dgl\\heterograph.py\u001b[0m in \u001b[0;36mhas_nodes\u001b[1;34m(self, vid, ntype)\u001b[0m\n\u001b[0;32m   2711\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvid_tensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvid_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvid_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2712\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mDGLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'All IDs must be non-negative integers.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2713\u001b[1;33m         ret = self._graph.has_nodes(\n\u001b[0m\u001b[0;32m   2714\u001b[0m             self.get_ntype_id(ntype), vid_tensor)\n\u001b[0;32m   2715\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dgl\\heterograph_index.py\u001b[0m in \u001b[0;36mhas_nodes\u001b[1;34m(self, ntype, vids)\u001b[0m\n\u001b[0;32m    406\u001b[0m             \u001b[1;36m0\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[0marray\u001b[0m \u001b[0mindicating\u001b[0m \u001b[0mexistence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m         \"\"\"\n\u001b[1;32m--> 408\u001b[1;33m         return F.from_dgl_nd(\n\u001b[0m\u001b[0;32m    409\u001b[0m             \u001b[0m_CAPI_DGLHeteroHasVertices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mntype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dgl_nd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m         )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dgl\\backend\\__init__.py\u001b[0m in \u001b[0;36mfrom_dgl_nd\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfrom_dgl_nd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mzerocopy_from_dgl_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py\u001b[0m in \u001b[0;36mzerocopy_from_dgl_ndarray\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    468\u001b[0m         )\n\u001b[0;32m    469\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 470\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdlpack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dlpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dlpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    471\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dgl\\_ffi\\_ctypes\\ndarray.py\u001b[0m in \u001b[0;36mto_dlpack\u001b[1;34m(self, alignment)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         check_call(\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDGLArrayToDLPack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malignment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         )\n\u001b[0;32m    108\u001b[0m         return ctypes.pythonapi.PyCapsule_New(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Adversarial Robustness Radius Computation (100 Samples Per Class) ---\\n\")\n",
    "\n",
    "# Dictionary to store the adversarial robustness radius for each gate type.\n",
    "class_adv_radius = {gt: [] for gt in gate_types}\n",
    "overall_adv_radius = []\n",
    "\n",
    "def f_for_sample(x, test_idx):\n",
    "    \"\"\"\n",
    "    Given an input x (modified feature vector for the test node),\n",
    "    returns the output (logit vector) for that node.\n",
    "    \"\"\"\n",
    "    new_features = graph.ndata['features'].clone().detach()\n",
    "    new_features[test_idx] = x\n",
    "    with torch.no_grad():\n",
    "        out = model(graph, new_features)\n",
    "    return out[test_idx]\n",
    "\n",
    "def adversarial_radius_for_sample(test_idx, initial_epsilon=1e-3, growth_factor=1.2, max_epsilon=10.0, bs_iters=10, num_trials=10):\n",
    "    \"\"\"\n",
    "    For a given test node (identified by test_idx), this function computes the smallest\n",
    "    perturbation norm (adversarial robustness radius) required to change the model's prediction.\n",
    "    We sample a number of random directions and, for each, increase the perturbation until\n",
    "    the predicted class changes; a binary search is then performed to refine the minimal epsilon.\n",
    "    The minimal epsilon over all trials is returned.\n",
    "    \"\"\"\n",
    "    # Retrieve the original feature vector.\n",
    "    x0 = graph.ndata['features'][test_idx].clone().detach()\n",
    "    \n",
    "    # Determine the predicted class at x0.\n",
    "    with torch.no_grad():\n",
    "        out = model(graph, graph.ndata['features'])\n",
    "        y0 = torch.argmax(out[test_idx]).item()\n",
    "        \n",
    "    # Helper function: returns True if prediction at x remains unchanged.\n",
    "    def is_same(x):\n",
    "        out = f_for_sample(x, test_idx)\n",
    "        return (torch.argmax(out).item() == y0)\n",
    "    \n",
    "    radii = []\n",
    "    for _ in range(num_trials):\n",
    "        # Sample a random direction.\n",
    "        d = torch.randn_like(x0)\n",
    "        d = d / (torch.norm(d) + 1e-8)\n",
    "        epsilon = initial_epsilon\n",
    "        \n",
    "        # Increase epsilon until the classification changes or max_epsilon is reached.\n",
    "        while epsilon < max_epsilon and is_same(x0 + epsilon * d):\n",
    "            epsilon *= growth_factor\n",
    "        \n",
    "        # If maximum epsilon is reached, use max_epsilon as candidate.\n",
    "        if epsilon >= max_epsilon:\n",
    "            candidate = max_epsilon\n",
    "        else:\n",
    "            # Refine the candidate using binary search between [epsilon/growth_factor, epsilon].\n",
    "            low = epsilon / growth_factor\n",
    "            high = epsilon\n",
    "            for _ in range(bs_iters):\n",
    "                mid = (low + high) / 2\n",
    "                if is_same(x0 + mid * d):\n",
    "                    low = mid\n",
    "                else:\n",
    "                    high = mid\n",
    "            candidate = high\n",
    "        radii.append(candidate)\n",
    "    \n",
    "    return min(radii)\n",
    "\n",
    "# Iterate over each selected test node (assumed 100 samples per class are in selected_test_nodes).\n",
    "for test_idx in selected_test_nodes:\n",
    "    label_idx = graph.ndata['labels'][test_idx].item()\n",
    "    class_name = gate_types[label_idx]\n",
    "    \n",
    "    radius = adversarial_radius_for_sample(test_idx)\n",
    "    class_adv_radius[class_name].append(radius)\n",
    "    overall_adv_radius.append(radius)\n",
    "\n",
    "# Display per-class results.\n",
    "print(\"\\nClass-wise Adversarial Robustness Radius Results:\")\n",
    "header = \"{:<10s} {:>35s}\".format(\"Class\", \"Avg. Radius Â± Std\")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "for cls in gate_types:\n",
    "    values = class_adv_radius[cls]\n",
    "    if values:\n",
    "        avg_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "    else:\n",
    "        avg_val, std_val = 0, 0\n",
    "    print(\"{:<10s} {:>15.4f} Â± {:<15.4f}\".format(cls, avg_val, std_val))\n",
    "    \n",
    "# Compute overall aggregated results.\n",
    "overall_avg = np.mean(overall_adv_radius)\n",
    "overall_std = np.std(overall_adv_radius)\n",
    "print(\"\\nOverall Aggregated Adversarial Robustness Radius:\")\n",
    "print(\"Average Radius: {:.4f} Â± {:.4f}\".format(overall_avg, overall_std))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00574a57",
   "metadata": {},
   "source": [
    "#### Stability Under Input Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "351dab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stability Under Input Noise Computation (100 Samples Per Class) ---\n",
      "\n",
      "\n",
      "Class-wise Stability Under Input Noise Results:\n",
      "Class                   Avg. Stability (S(x)) Â± Std\n",
      "---------------------------------------------------\n",
      "and                 0.0692 Â± 0.0177         \n",
      "or                  0.0458 Â± 0.0049         \n",
      "nand                0.0597 Â± 0.0185         \n",
      "nor                 0.0458 Â± 0.0103         \n",
      "xor                 0.0810 Â± 0.0126         \n",
      "xnor                0.0535 Â± 0.0224         \n",
      "buf                 0.0383 Â± 0.0040         \n",
      "not                 0.0270 Â± 0.0030         \n",
      "\n",
      "Overall Aggregated Stability Under Input Noise:\n",
      "Average Stability: 0.0550 Â± 0.0204\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Stability Under Input Noise Computation (100 Samples Per Class) ---\\n\")\n",
    "\n",
    "# Standard deviation for Gaussian noise perturbations.\n",
    "sigma = 0.01\n",
    "\n",
    "# Number of noise samples to approximate the expectation.\n",
    "num_noise_samples = 20\n",
    "\n",
    "# Define gate types corresponding to your labels.\n",
    "gate_types = ['and', 'or', 'nand', 'nor', 'xor', 'xnor', 'buf', 'not']\n",
    "\n",
    "# Dictionaries to store stability metric per gate type and overall.\n",
    "class_stability = {gt: [] for gt in gate_types}\n",
    "overall_stability = []\n",
    "\n",
    "def stability_for_sample(test_idx, sigma, num_samples):\n",
    "    \"\"\"\n",
    "    Compute the stability under input noise for a given test sample.\n",
    "    For the sample at test_idx, the prediction difference is computed as the Euclidean norm \n",
    "    between the model outputs for the original and the noisy inputs. The stability measure \n",
    "    S(x_i) is obtained by averaging over multiple noise samples.\n",
    "    \"\"\"\n",
    "    # Retrieve the original feature vector.\n",
    "    x0 = graph.ndata['features'][test_idx].clone().detach()\n",
    "    \n",
    "    # Compute the model output for the unperturbed input.\n",
    "    with torch.no_grad():\n",
    "        f_orig = model(graph, graph.ndata['features'])[test_idx]\n",
    "    \n",
    "    differences = []\n",
    "    for _ in range(num_samples):\n",
    "        # Sample noise from a Gaussian distribution: N(0, sigma^2 I)\n",
    "        noise = sigma * torch.randn_like(x0)\n",
    "        x_noisy = x0 + noise\n",
    "        \n",
    "        # Replace the test node's feature with the noisy feature.\n",
    "        new_features = graph.ndata['features'].clone().detach()\n",
    "        new_features[test_idx] = x_noisy\n",
    "        \n",
    "        # Compute the model's output for the noisy input.\n",
    "        with torch.no_grad():\n",
    "            f_noisy = model(graph, new_features)[test_idx]\n",
    "        \n",
    "        # Calculate the Euclidean norm difference between the noisy and original outputs.\n",
    "        diff = torch.norm(f_noisy - f_orig).item()\n",
    "        differences.append(diff)\n",
    "    \n",
    "    # Average difference approximates the expectation.\n",
    "    return np.mean(differences)\n",
    "\n",
    "# Iterate over each selected test node (assumed to be 100 samples per class in selected_test_nodes).\n",
    "for test_idx in selected_test_nodes:\n",
    "    label_idx = graph.ndata['labels'][test_idx].item()\n",
    "    class_name = gate_types[label_idx]\n",
    "    \n",
    "    stability_measure = stability_for_sample(test_idx, sigma, num_noise_samples)\n",
    "    class_stability[class_name].append(stability_measure)\n",
    "    overall_stability.append(stability_measure)\n",
    "\n",
    "# Display per-class results.\n",
    "print(\"\\nClass-wise Stability Under Input Noise Results:\")\n",
    "header = \"{:<10s} {:>40s}\".format(\"Class\", \"Avg. Stability (S(x)) Â± Std\")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "for cls in gate_types:\n",
    "    values = class_stability[cls]\n",
    "    if values:\n",
    "        avg_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "    else:\n",
    "        avg_val, std_val = 0, 0\n",
    "    print(\"{:<10s} {:>15.4f} Â± {:<15.4f}\".format(cls, avg_val, std_val))\n",
    "    \n",
    "overall_avg = np.mean(overall_stability)\n",
    "overall_std = np.std(overall_stability)\n",
    "print(\"\\nOverall Aggregated Stability Under Input Noise:\")\n",
    "print(\"Average Stability: {:.4f} Â± {:.4f}\".format(overall_avg, overall_std))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9a82ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
