{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d2f0719",
   "metadata": {},
   "source": [
    "#### Training GAT\n",
    "\n",
    "Model using the graph dataset extracted from the csv file. TO generate csv file, please run the parsing notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52bbdb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "?? Dataset Overview:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60882 entries, 0 to 60881\n",
      "Data columns (total 18 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   circuit_name            60882 non-null  object \n",
      " 1   node                    60882 non-null  object \n",
      " 2   gate_type               60882 non-null  object \n",
      " 3   fan_in                  60882 non-null  int64  \n",
      " 4   fan_out                 60882 non-null  int64  \n",
      " 5   depth                   60882 non-null  object \n",
      " 6   dist_to_output          60882 non-null  int64  \n",
      " 7   is_primary_input        60882 non-null  int64  \n",
      " 8   is_primary_output       60882 non-null  int64  \n",
      " 9   is_internal             60882 non-null  int64  \n",
      " 10  is_key_gate             60882 non-null  int64  \n",
      " 11  key_dependency          122 non-null    object \n",
      " 12  degree_centrality       60882 non-null  float64\n",
      " 13  betweenness_centrality  60882 non-null  float64\n",
      " 14  closeness_centrality    60882 non-null  float64\n",
      " 15  clustering_coefficient  60882 non-null  float64\n",
      " 16  avg_fan_in_neighbors    60882 non-null  float64\n",
      " 17  avg_fan_out_neighbors   60882 non-null  float64\n",
      "dtypes: float64(6), int64(7), object(5)\n",
      "memory usage: 8.4+ MB\n",
      "None\n",
      "\n",
      "?? Extracted 6172 edges.\n",
      "Epoch 0/50, Loss: 2.6037\n",
      "Epoch 10/50, Loss: 0.8633\n",
      "Epoch 20/50, Loss: 0.7438\n",
      "Epoch 30/50, Loss: 0.6992\n",
      "Epoch 40/50, Loss: 0.6532\n",
      "\n",
      "? Test Accuracy on Original Graph: 92.19%\n",
      "\n",
      "?? Confusion Matrix (Original Graph):\n",
      "[[5507    0   64   66    0    0    0    0]\n",
      " [   0  475    0    0    0    0    0    0]\n",
      " [ 387    0   81   59    0    0    0    0]\n",
      " [ 101    0   17  449    0    0    0    0]\n",
      " [  57    0    0    3 4418    0    0    0]\n",
      " [ 190    0    4    3    0    0    0    0]\n",
      " [   0    0    0    0    0    0  266    0]\n",
      " [   0    0    0    0    0    0    0   30]]\n",
      "\n",
      "Classification Report (Original Graph):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         and       0.88      0.98      0.93      5637\n",
      "       input       1.00      1.00      1.00       475\n",
      "        nand       0.49      0.15      0.23       527\n",
      "         nor       0.77      0.79      0.78       567\n",
      "         not       1.00      0.99      0.99      4478\n",
      "          or       0.00      0.00      0.00       197\n",
      "      output       1.00      1.00      1.00       266\n",
      "         xor       1.00      1.00      1.00        30\n",
      "\n",
      "    accuracy                           0.92     12177\n",
      "   macro avg       0.77      0.74      0.74     12177\n",
      "weighted avg       0.90      0.92      0.90     12177\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/rrk307/anaconda3/envs/gnn_circuits/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Load Dataset and Prepare Data\n",
    "# ------------------------------\n",
    "df = pd.read_csv(\"all_circuits_features.csv\")\n",
    "print(\"\\n?? Dataset Overview:\")\n",
    "print(df.info())\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"gate_label\"] = label_encoder.fit_transform(df[\"gate_type\"])\n",
    "\n",
    "feature_columns = [\n",
    "    \"fan_in\", \"fan_out\", \"dist_to_output\", \"is_primary_input\", \"is_primary_output\",\n",
    "    \"is_internal\", \"is_key_gate\", \"degree_centrality\", \"betweenness_centrality\",\n",
    "    \"closeness_centrality\", \"clustering_coefficient\", \"avg_fan_in_neighbors\", \"avg_fan_out_neighbors\"\n",
    "]\n",
    "\n",
    "df = df.dropna(subset=feature_columns)\n",
    "df[feature_columns] = df[feature_columns].astype(float)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[feature_columns] = scaler.fit_transform(df[feature_columns])\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Build Node and Edge Lists\n",
    "# ---------------------------\n",
    "nodes = df[\"node\"].tolist()\n",
    "node_to_id = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "edges = []\n",
    "for _, row in df.iterrows():\n",
    "    node_id = node_to_id[row[\"node\"]]\n",
    "    potential_sources = df[df[\"fan_out\"] > 0][\"node\"].tolist()\n",
    "    num_fan_in = int(row[\"fan_in\"])\n",
    "    if num_fan_in > 0:\n",
    "        sources = potential_sources[:num_fan_in]\n",
    "        for src in sources:\n",
    "            if src in node_to_id:\n",
    "                edges.append((node_to_id[src], node_id))\n",
    "\n",
    "print(\"\\n?? Extracted\", len(edges), \"edges.\")\n",
    "if len(edges) == 0:\n",
    "    raise ValueError(\"No edges found! Check your fan_in values.\")\n",
    "\n",
    "src_nodes, dst_nodes = zip(*edges) if edges else ([], [])\n",
    "src_tensor = torch.tensor(src_nodes, dtype=torch.int64)\n",
    "dst_tensor = torch.tensor(dst_nodes, dtype=torch.int64)\n",
    "valid_edges = (\n",
    "    (src_tensor >= 0) & (dst_tensor >= 0) &\n",
    "    (src_tensor < len(nodes)) & (dst_tensor < len(nodes))\n",
    ")\n",
    "src_tensor = src_tensor[valid_edges]\n",
    "dst_tensor = dst_tensor[valid_edges]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Create the DGL Graph\n",
    "# ---------------------------\n",
    "graph = dgl.graph((src_tensor, dst_tensor), num_nodes=len(nodes))\n",
    "graph = dgl.add_self_loop(graph)\n",
    "\n",
    "graph.ndata['features'] = torch.tensor(df[feature_columns].values, dtype=torch.float32)\n",
    "graph.ndata['labels'] = torch.tensor(df[\"gate_label\"].values, dtype=torch.long)\n",
    "\n",
    "nodes_idx = np.arange(len(nodes))\n",
    "train_idx, test_idx = train_test_split(nodes_idx, test_size=0.2, random_state=42)\n",
    "train_nid = torch.tensor(train_idx, dtype=torch.long)\n",
    "test_nid = torch.tensor(test_idx, dtype=torch.long)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Define the GAT Model\n",
    "# ---------------------------\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, out_feats, num_heads1=4, num_heads2=1, attn_drop=0.2, feat_drop=0.2):\n",
    "        super().__init__()\n",
    "        # First GAT layer outputs [N, num_heads1, hidden_feats]\n",
    "        self.gat1 = dglnn.GATConv(\n",
    "            in_feats, hidden_feats, num_heads=num_heads1,\n",
    "            feat_drop=feat_drop, attn_drop=attn_drop, activation=nn.ELU()\n",
    "        )\n",
    "        # Second GAT layer maps concatenated heads to out_feats (set num_heads2=1 for logits)\n",
    "        self.gat2 = dglnn.GATConv(\n",
    "            hidden_feats * num_heads1, out_feats, num_heads=num_heads2,\n",
    "            feat_drop=feat_drop, attn_drop=attn_drop, activation=None\n",
    "        )\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        x = self.gat1(g, x)                 # [N, H1, D]\n",
    "        x = x.flatten(1)                    # concat heads -> [N, H1*D]\n",
    "        x = self.gat2(g, x)                 # [N, H2, C]\n",
    "        if x.dim() == 3:\n",
    "            x = x.mean(1)                   # average over heads if H2 > 1 (or keep single head)\n",
    "        return x\n",
    "\n",
    "in_feats = len(feature_columns)\n",
    "hidden_feats = 32\n",
    "out_feats = len(label_encoder.classes_)\n",
    "model = GATModel(in_feats, hidden_feats, out_feats, num_heads1=4, num_heads2=1, attn_drop=0.2, feat_drop=0.2)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Full-graph Training Loop\n",
    "# ---------------------------\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    logits = model(graph, graph.ndata['features'])\n",
    "    loss = loss_fn(logits[train_nid], graph.ndata['labels'][train_nid])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Evaluation on Full Graph\n",
    "# ---------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(graph, graph.ndata['features'])\n",
    "    test_logits = logits[test_nid]\n",
    "    test_predictions = test_logits.argmax(dim=1)\n",
    "    orig_accuracy = (test_predictions == graph.ndata['labels'][test_nid]).float().mean().item()\n",
    "\n",
    "print(f\"\\n? Test Accuracy on Original Graph: {orig_accuracy * 100:.2f}%\")\n",
    "\n",
    "true_labels_orig = graph.ndata['labels'][test_nid].cpu().numpy()\n",
    "pred_labels_orig = test_predictions.cpu().numpy()\n",
    "conf_mat_orig = confusion_matrix(true_labels_orig, pred_labels_orig)\n",
    "\n",
    "print(\"\\n?? Confusion Matrix (Original Graph):\")\n",
    "print(conf_mat_orig)\n",
    "\n",
    "print(\"\\nClassification Report (Original Graph):\")\n",
    "print(classification_report(true_labels_orig, pred_labels_orig, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018700d",
   "metadata": {},
   "source": [
    "#### Jacobian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30f42bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Jacobian Computation and Relative Error (GAT, 100 Samples Per Class) ---\n",
      "\n",
      "Sampling report:\n",
      "- and: picked 100 from 5637 available.\n",
      "- input: picked 100 from 475 available.\n",
      "- nand: picked 100 from 527 available.\n",
      "- nor: picked 100 from 567 available.\n",
      "- not: picked 100 from 4478 available.\n",
      "- or: picked 100 from 197 available.\n",
      "- output: picked 100 from 266 available.\n",
      "- xor: only 30 available, taking all.\n",
      "\n",
      "Final counts in selected set:\n",
      "- and: 100\n",
      "- input: 100\n",
      "- nand: 100\n",
      "- nor: 100\n",
      "- not: 100\n",
      "- or: 100\n",
      "- output: 100\n",
      "- xor: 30\n",
      "\n",
      "Class-wise Jacobian Analysis Results:\n",
      "Class              Avg. Jacobian Norm ± Std           Avg. Relative Error ± Std\n",
      "-------------------------------------------------------------------------------\n",
      "and                   9.3621 ± 1.6380            2.2930e-04 ± 1.8074e-04\n",
      "input                 7.5426 ± 0.1753            4.2216e-04 ± 1.9531e-04\n",
      "nand                  9.0070 ± 1.4154            2.8325e-04 ± 2.3040e-04\n",
      "nor                   8.4702 ± 0.9996            2.8226e-04 ± 1.3619e-04\n",
      "not                   9.6036 ± 1.0136            1.9396e-04 ± 9.9410e-05\n",
      "or                    8.4633 ± 2.3536            3.7191e-04 ± 3.9194e-04\n",
      "output                7.2217 ± 0.0932            4.0406e-04 ± 2.0541e-04\n",
      "xor                   6.3078 ± 0.1346            8.4554e-04 ± 4.1454e-04\n",
      "\n",
      "Overall Aggregated Jacobian Analysis Results:\n",
      "Average Jacobian Norm: 8.4333 ± 1.5909\n",
      "Average Relative Error: 3.3433e-04 ± 2.6898e-04\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Jacobian Computation and Relative Error (GAT, 100 Samples Per Class) ---\\n\")\n",
    "\n",
    "# --------------------------------\n",
    "# 0) Setup\n",
    "# --------------------------------\n",
    "model.eval()\n",
    "device = graph.ndata['features'].device\n",
    "labels_np = graph.ndata['labels'].cpu().numpy()\n",
    "class_names = list(label_encoder.classes_)\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# --------------------------------\n",
    "# 1) Build selected_test_nodes: up to 100 per class from test_nid (index array)\n",
    "# --------------------------------\n",
    "rng = np.random.default_rng(42)\n",
    "test_indices = test_nid.cpu().numpy()\n",
    "\n",
    "selected_test_nodes = []\n",
    "print(\"Sampling report:\")\n",
    "for c in range(num_classes):\n",
    "    idxs = [int(i) for i in test_indices if labels_np[int(i)] == c]\n",
    "    n_avail = len(idxs)\n",
    "    if n_avail == 0:\n",
    "        print(f\"- {class_names[c]}: 0 available in test set  skipped.\")\n",
    "        continue\n",
    "    if n_avail >= 100:\n",
    "        chosen = rng.choice(idxs, size=100, replace=False)\n",
    "        print(f\"- {class_names[c]}: picked 100 from {n_avail} available.\")\n",
    "    else:\n",
    "        chosen = np.array(idxs, dtype=np.int64)\n",
    "        print(f\"- {class_names[c]}: only {n_avail} available, taking all.\")\n",
    "    selected_test_nodes.extend(chosen.tolist())\n",
    "\n",
    "selected_test_nodes = np.array(selected_test_nodes, dtype=np.int64)\n",
    "\n",
    "# Final per-class counts (selected)\n",
    "final_counts = {class_names[c]: int(np.sum(labels_np[selected_test_nodes] == c)) for c in range(num_classes)}\n",
    "print(\"\\nFinal counts in selected set:\")\n",
    "for name, cnt in final_counts.items():\n",
    "    print(f\"- {name}: {cnt}\")\n",
    "\n",
    "# --------------------------------\n",
    "# 2) Helper: f(x) returns logits for node test_idx with its feature vector replaced by x\n",
    "# --------------------------------\n",
    "def node_logits_with_replaced_features(x, test_idx):\n",
    "    # x: [F] tensor on device\n",
    "    new_features = graph.ndata['features'].clone()\n",
    "    new_features[test_idx] = x\n",
    "    out = model(graph, new_features)\n",
    "    return out[test_idx]  # shape: [num_classes]\n",
    "\n",
    "# --------------------------------\n",
    "# 3) Main loop: Jacobian + finite-difference verification\n",
    "# --------------------------------\n",
    "class_metrics = {cn: {'jacobian_norms': [], 'relative_errors': []} for cn in class_names}\n",
    "overall_jacobian_norms = []\n",
    "overall_relative_errors = []\n",
    "\n",
    "epsilon = 1e-3  # small perturbation for FD check\n",
    "\n",
    "for test_idx in selected_test_nodes:\n",
    "    label_idx = int(graph.ndata['labels'][test_idx].item())\n",
    "    class_name = class_names[label_idx]\n",
    "\n",
    "    # Base feature vector for this node\n",
    "    x0 = graph.ndata['features'][test_idx].clone().detach().to(device).requires_grad_(True)\n",
    "\n",
    "    # Define f(x) for autograd Jacobian (must not use no_grad)\n",
    "    def f(x):\n",
    "        return node_logits_with_replaced_features(x, test_idx)\n",
    "\n",
    "    # Compute Jacobian J (num_classes x feat_dim)\n",
    "    J = torch.autograd.functional.jacobian(f, x0)  # shape: [C, F]\n",
    "\n",
    "    # Frobenius norm of the Jacobian\n",
    "    jacobian_norm = torch.norm(J, p='fro').item()\n",
    "\n",
    "    # Finite-difference verification\n",
    "    delta = epsilon * torch.randn_like(x0)\n",
    "    predicted_change = J.mv(delta)  # [C]\n",
    "    f_x0 = f(x0)\n",
    "    f_x0_perturbed = f(x0 + delta)\n",
    "    actual_change = f_x0_perturbed - f_x0\n",
    "    rel_error = (torch.norm(predicted_change - actual_change) /\n",
    "                 (torch.norm(actual_change) + 1e-8)).item()\n",
    "\n",
    "    # Store metrics\n",
    "    class_metrics[class_name]['jacobian_norms'].append(jacobian_norm)\n",
    "    class_metrics[class_name]['relative_errors'].append(rel_error)\n",
    "    overall_jacobian_norms.append(jacobian_norm)\n",
    "    overall_relative_errors.append(rel_error)\n",
    "\n",
    "# --------------------------------\n",
    "# 4) Reporting\n",
    "# --------------------------------\n",
    "print(\"\\nClass-wise Jacobian Analysis Results:\")\n",
    "header = \"{:<12s} {:>30s} {:>35s}\".format(\"Class\", \"Avg. Jacobian Norm ± Std\", \"Avg. Relative Error ± Std\")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for cn in class_names:\n",
    "    norms = class_metrics[cn]['jacobian_norms']\n",
    "    errors = class_metrics[cn]['relative_errors']\n",
    "    avg_norm = np.mean(norms) if norms else 0.0\n",
    "    std_norm = np.std(norms) if norms else 0.0\n",
    "    avg_rel_error = np.mean(errors) if errors else 0.0\n",
    "    std_rel_error = np.std(errors) if errors else 0.0\n",
    "    print(\"{:<12s} {:>15.4f} ± {:<12.4f} {:>15.4e} ± {:<10.4e}\".format(\n",
    "        cn, avg_norm, std_norm, avg_rel_error, std_rel_error\n",
    "    ))\n",
    "\n",
    "# Overall aggregates\n",
    "overall_avg_norm = float(np.mean(overall_jacobian_norms)) if overall_jacobian_norms else 0.0\n",
    "overall_std_norm = float(np.std(overall_jacobian_norms)) if overall_jacobian_norms else 0.0\n",
    "overall_avg_rel_error = float(np.mean(overall_relative_errors)) if overall_relative_errors else 0.0\n",
    "overall_std_rel_error = float(np.std(overall_relative_errors)) if overall_relative_errors else 0.0\n",
    "\n",
    "print(\"\\nOverall Aggregated Jacobian Analysis Results:\")\n",
    "print(\"Average Jacobian Norm: {:.4f} ± {:.4f}\".format(overall_avg_norm, overall_std_norm))\n",
    "print(\"Average Relative Error: {:.4e} ± {:.4e}\".format(overall_avg_rel_error, overall_std_rel_error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46efec1",
   "metadata": {},
   "source": [
    "#### Local Lipschitz constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf46d7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Local Lipschitz Constant & Relative Error (GAT, 100 Samples Per Class) ---\n",
      "\n",
      "Sampling report:\n",
      "- and: picked 100 from 5637 available.\n",
      "- input: picked 100 from 475 available.\n",
      "- nand: picked 100 from 527 available.\n",
      "- nor: picked 100 from 567 available.\n",
      "- not: picked 100 from 4478 available.\n",
      "- or: picked 100 from 197 available.\n",
      "- output: picked 100 from 266 available.\n",
      "- xor: only 30 available, taking all.\n",
      "\n",
      "Final counts in selected set:\n",
      "- and: 100\n",
      "- input: 100\n",
      "- nand: 100\n",
      "- nor: 100\n",
      "- not: 100\n",
      "- or: 100\n",
      "- output: 100\n",
      "- xor: 30\n",
      "\n",
      "Class-wise Local Lipschitz (with Relative Errors):\n",
      "Class              Avg Lipschitz ± Std           Avg Rel. Error ± Std\n",
      "---------------------------------------------------------------------\n",
      "and               6.3116 ± 0.9519          2.0032e+00 ± 5.4596e-01\n",
      "input             5.4617 ± 0.1419          2.1204e+00 ± 3.6971e-01\n",
      "nand              6.1662 ± 0.7786          2.0747e+00 ± 6.2590e-01\n",
      "nor               5.7727 ± 0.5582          1.9881e+00 ± 4.6152e-01\n",
      "not               6.5503 ± 0.6863          2.0029e+00 ± 3.3338e-01\n",
      "or                5.7984 ± 1.2422          2.1555e+00 ± 6.9872e-01\n",
      "output            5.3867 ± 0.0861          2.2905e+00 ± 3.7172e-01\n",
      "xor               4.3098 ± 0.0564          1.9322e+00 ± 4.6990e-01\n",
      "\n",
      "Overall Aggregated:\n",
      "Avg Lipschitz Constant: 5.8549 ± 0.8879\n",
      "Avg Relative Error: 2.0842e+00 ± 5.1307e-01\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Local Lipschitz Constant & Relative Error (GAT, 100 Samples Per Class) ---\\n\")\n",
    "\n",
    "# --------------------------------\n",
    "# 0) Setup\n",
    "# --------------------------------\n",
    "model.eval()\n",
    "class_names = list(label_encoder.classes_)\n",
    "num_classes = len(class_names)\n",
    "labels_np = graph.ndata['labels'].cpu().numpy()\n",
    "\n",
    "# --------------------------------\n",
    "# 1) Build selected_test_nodes: up to 100 per class from test_nid (index array)\n",
    "# --------------------------------\n",
    "rng = np.random.default_rng(42)\n",
    "test_indices = test_nid.cpu().numpy()\n",
    "\n",
    "selected_test_nodes = []\n",
    "print(\"Sampling report:\")\n",
    "for c in range(num_classes):\n",
    "    idxs = [int(i) for i in test_indices if labels_np[int(i)] == c]\n",
    "    n_avail = len(idxs)\n",
    "    if n_avail == 0:\n",
    "        print(f\"- {class_names[c]}: 0 available in test set  skipped.\")\n",
    "        continue\n",
    "    if n_avail >= 100:\n",
    "        chosen = rng.choice(idxs, size=100, replace=False)\n",
    "        print(f\"- {class_names[c]}: picked 100 from {n_avail} available.\")\n",
    "    else:\n",
    "        chosen = np.array(idxs, dtype=np.int64)\n",
    "        print(f\"- {class_names[c]}: only {n_avail} available, taking all.\")\n",
    "    selected_test_nodes.extend(chosen.tolist())\n",
    "\n",
    "selected_test_nodes = np.array(selected_test_nodes, dtype=np.int64)\n",
    "\n",
    "# Final per-class counts (selected)\n",
    "final_counts = {class_names[c]: int(np.sum(labels_np[selected_test_nodes] == c)) for c in range(num_classes)}\n",
    "print(\"\\nFinal counts in selected set:\")\n",
    "for name, cnt in final_counts.items():\n",
    "    print(f\"- {name}: {cnt}\")\n",
    "\n",
    "# --------------------------------\n",
    "# 2) Helper: f(x) returns logits for node test_idx with its feature vector replaced by x\n",
    "# --------------------------------\n",
    "def node_logits_with_replaced_features(x, test_idx):\n",
    "    new_features = graph.ndata['features'].clone()\n",
    "    new_features[test_idx] = x\n",
    "    out = model(graph, new_features)\n",
    "    return out[test_idx]  # [num_classes]\n",
    "\n",
    "# --------------------------------\n",
    "# 3) Main loop: Local Lipschitz (||J||_2) + finite-difference relative error\n",
    "# --------------------------------\n",
    "class_lipschitz = {cn: [] for cn in class_names}\n",
    "class_rel_errors = {cn: [] for cn in class_names}\n",
    "overall_lipschitz = []\n",
    "overall_rel_errors = []\n",
    "\n",
    "epsilon = 1e-3\n",
    "trials_per_node = 10\n",
    "\n",
    "for test_idx in selected_test_nodes:\n",
    "    label_idx = int(graph.ndata['labels'][test_idx].item())\n",
    "    class_name = class_names[label_idx]\n",
    "\n",
    "    # Base feature vector\n",
    "    x0 = graph.ndata['features'][test_idx].clone().detach().requires_grad_(True)\n",
    "\n",
    "    # Define scalar-to-vector function for Jacobian\n",
    "    def f(x):\n",
    "        return node_logits_with_replaced_features(x, test_idx)\n",
    "\n",
    "    # Jacobian J: [C, F]\n",
    "    J = torch.autograd.functional.jacobian(f, x0)\n",
    "\n",
    "    # Local Lipschitz constant: operator 2-norm (largest singular value)\n",
    "    L_local = torch.linalg.norm(J, ord=2).item()\n",
    "\n",
    "    # Finite-difference relative error across multiple random directions\n",
    "    rel_errors_for_node = []\n",
    "    f_x0 = f(x0).detach()\n",
    "    for _ in range(trials_per_node):\n",
    "        delta = epsilon * torch.randn_like(x0)\n",
    "        # Predicted change norm via Lipschitz bound\n",
    "        pred_change_norm = L_local * torch.norm(delta).item()\n",
    "        # Actual change norm\n",
    "        f_x0_pert = f(x0 + delta).detach()\n",
    "        actual_change_norm = torch.norm(f_x0_pert - f_x0).item()\n",
    "        rel_err = abs(pred_change_norm - actual_change_norm) / (actual_change_norm + 1e-8)\n",
    "        rel_errors_for_node.append(rel_err)\n",
    "\n",
    "    avg_rel_error_node = float(np.mean(rel_errors_for_node))\n",
    "\n",
    "    # Store\n",
    "    class_lipschitz[class_name].append(L_local)\n",
    "    class_rel_errors[class_name].append(avg_rel_error_node)\n",
    "    overall_lipschitz.append(L_local)\n",
    "    overall_rel_errors.append(avg_rel_error_node)\n",
    "\n",
    "# --------------------------------\n",
    "# 4) Reporting\n",
    "# --------------------------------\n",
    "print(\"\\nClass-wise Local Lipschitz (with Relative Errors):\")\n",
    "header = \"{:<12s} {:>25s} {:>30s}\".format(\"Class\", \"Avg Lipschitz ± Std\", \"Avg Rel. Error ± Std\")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for cn in class_names:\n",
    "    L_vals = class_lipschitz[cn]\n",
    "    E_vals = class_rel_errors[cn]\n",
    "    if L_vals:\n",
    "        print(\"{:<12s} {:>11.4f} ± {:<10.4f} {:>15.4e} ± {:<10.4e}\".format(\n",
    "            cn, np.mean(L_vals), np.std(L_vals), np.mean(E_vals), np.std(E_vals)\n",
    "        ))\n",
    "    else:\n",
    "        print(\"{:<12s} {:>11s} {:<10s} {:>15s} {:<10s}\".format(cn, \"-\", \"-\", \"-\", \"-\"))\n",
    "\n",
    "if overall_lipschitz:\n",
    "    print(\"\\nOverall Aggregated:\")\n",
    "    print(\"Avg Lipschitz Constant: {:.4f} ± {:.4f}\".format(np.mean(overall_lipschitz), np.std(overall_lipschitz)))\n",
    "    print(\"Avg Relative Error: {:.4e} ± {:.4e}\".format(np.mean(overall_rel_errors), np.std(overall_rel_errors)))\n",
    "else:\n",
    "    print(\"\\nOverall Aggregated:\")\n",
    "    print(\"No samples selected; overall metrics unavailable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b5349e",
   "metadata": {},
   "source": [
    "#### Hessian-Based Curvature Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2de8353f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hessian-Based Curvature Measure & Relative Error (GAT, 100 Samples Per Class) ---\n",
      "\n",
      "Sampling report:\n",
      "- and: picked 100 from 5637 available.\n",
      "- input: picked 100 from 475 available.\n",
      "- nand: picked 100 from 527 available.\n",
      "- nor: picked 100 from 567 available.\n",
      "- not: picked 100 from 4478 available.\n",
      "- or: picked 100 from 197 available.\n",
      "- output: picked 100 from 266 available.\n",
      "- xor: only 30 available, taking all.\n",
      "\n",
      "Final counts in selected set:\n",
      "- and: 100\n",
      "- input: 100\n",
      "- nand: 100\n",
      "- nor: 100\n",
      "- not: 100\n",
      "- or: 100\n",
      "- output: 100\n",
      "- xor: 30\n",
      "\n",
      "Class-wise Hessian-Based Curvature (with Relative Errors):\n",
      "Class                Avg. Max Eigenvalue ± Std           Avg Rel. Error ± Std\n",
      "-----------------------------------------------------------------------------\n",
      "and               0.0552 ± 0.0743          1.1623e-01 ± 3.2371e-01\n",
      "input             0.0000 ± 0.0000          5.3810e-01 ± 1.7081e-01\n",
      "nand              0.1739 ± 0.1180          1.1373e-01 ± 2.6615e-01\n",
      "nor               0.0473 ± 0.0987          5.4370e-02 ± 1.2538e-01\n",
      "not               0.0022 ± 0.0123          1.3372e-01 ± 2.6593e-01\n",
      "or                0.1046 ± 0.0679          1.7854e-01 ± 2.6333e-01\n",
      "output            0.0000 ± 0.0000          2.7729e-01 ± 1.2419e-01\n",
      "xor               0.0000 ± 0.0000          3.9135e-02 ± 1.8617e-02\n",
      "\n",
      "Overall Aggregated Hessian-Based Curvature:\n",
      "Avg Max Eigenvalue: 0.0525 ± 0.0909\n",
      "Avg Relative Error: 1.9503e-01 ± 2.7311e-01\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Hessian-Based Curvature Measure & Relative Error (GAT, 100 Samples Per Class) ---\\n\")\n",
    "\n",
    "# --------------------------------\n",
    "# 0) Setup\n",
    "# --------------------------------\n",
    "model.eval()\n",
    "class_names = list(label_encoder.classes_)\n",
    "num_classes = len(class_names)\n",
    "labels_np = graph.ndata['labels'].cpu().numpy()\n",
    "\n",
    "# --------------------------------\n",
    "# 1) Build selected_test_nodes: up to 100 per class from test_nid\n",
    "# --------------------------------\n",
    "rng = np.random.default_rng(42)\n",
    "test_indices = test_nid.cpu().numpy()\n",
    "\n",
    "selected_test_nodes = []\n",
    "print(\"Sampling report:\")\n",
    "for c in range(num_classes):\n",
    "    idxs = [int(i) for i in test_indices if labels_np[int(i)] == c]\n",
    "    n_avail = len(idxs)\n",
    "    if n_avail == 0:\n",
    "        print(f\"- {class_names[c]}: 0 available in test set  skipped.\")\n",
    "        continue\n",
    "    if n_avail >= 100:\n",
    "        chosen = rng.choice(idxs, size=100, replace=False)\n",
    "        print(f\"- {class_names[c]}: picked 100 from {n_avail} available.\")\n",
    "    else:\n",
    "        chosen = np.array(idxs, dtype=np.int64)\n",
    "        print(f\"- {class_names[c]}: only {n_avail} available, taking all.\")\n",
    "    selected_test_nodes.extend(chosen.tolist())\n",
    "\n",
    "selected_test_nodes = np.array(selected_test_nodes, dtype=np.int64)\n",
    "\n",
    "# Final per-class counts\n",
    "final_counts = {class_names[c]: int(np.sum(labels_np[selected_test_nodes] == c)) for c in range(num_classes)}\n",
    "print(\"\\nFinal counts in selected set:\")\n",
    "for name, cnt in final_counts.items():\n",
    "    print(f\"- {name}: {cnt}\")\n",
    "\n",
    "# --------------------------------\n",
    "# 2) Helper to compute Hessian, gradient, and scalar function h at x0\n",
    "# --------------------------------\n",
    "def compute_hessian_and_grad_for_sample(test_idx):\n",
    "    # Base input with gradients\n",
    "    x0 = graph.ndata['features'][test_idx].clone().detach().requires_grad_(True)\n",
    "\n",
    "    # Determine predicted class at x0\n",
    "    with torch.no_grad():\n",
    "        base_logits = model(graph, graph.ndata['features'])\n",
    "        pred_class = torch.argmax(base_logits[test_idx])\n",
    "\n",
    "    # Scalar function: log prob of predicted class\n",
    "    def h(x):\n",
    "        new_features = graph.ndata['features'].clone().detach()\n",
    "        new_features[test_idx] = x\n",
    "        logits = model(graph, new_features)[test_idx]\n",
    "        log_probs = torch.log_softmax(logits, dim=0)\n",
    "        return log_probs[pred_class]\n",
    "\n",
    "    # Hessian at x0\n",
    "    H = torch.autograd.functional.hessian(h, x0, create_graph=False, vectorize=False).detach()\n",
    "\n",
    "    # Gradient at x0\n",
    "    g = torch.autograd.functional.jacobian(h, x0).detach()\n",
    "\n",
    "    # Scalar baseline h(x0)\n",
    "    h0 = h(x0.detach())\n",
    "\n",
    "    return H, x0.detach(), g, h0, h\n",
    "\n",
    "# --------------------------------\n",
    "# 3) Main loop: curvature proxy + relative error\n",
    "# --------------------------------\n",
    "class_hessian_eig = {cn: [] for cn in class_names}\n",
    "class_rel_errors  = {cn: [] for cn in class_names}\n",
    "overall_hessian_eig = []\n",
    "overall_rel_errors  = []\n",
    "\n",
    "epsilon = 5e-3\n",
    "trials_per_node = 10\n",
    "\n",
    "for test_idx in selected_test_nodes:\n",
    "    label_idx = int(graph.ndata['labels'][test_idx].item())\n",
    "    class_name = class_names[label_idx]\n",
    "\n",
    "    H, x0, g, h0, h_func = compute_hessian_and_grad_for_sample(test_idx)\n",
    "\n",
    "    # Largest eigenvalue (lambda_max)\n",
    "    try:\n",
    "        eigvals = torch.linalg.eigvalsh(H)\n",
    "        lambda_max = torch.max(eigvals).item()\n",
    "    except RuntimeError:\n",
    "        # Fallback: power iteration\n",
    "        v = torch.randn_like(x0)\n",
    "        v = v / (v.norm() + 1e-12)\n",
    "        for _ in range(20):\n",
    "            v = H @ v\n",
    "            v = v / (v.norm() + 1e-12)\n",
    "        lambda_max = torch.dot(v, H @ v).item()\n",
    "\n",
    "    # Relative error of second-order approximation\n",
    "    node_errors = []\n",
    "    for _ in range(trials_per_node):\n",
    "        delta = epsilon * torch.randn_like(x0)\n",
    "        pred_second = 0.5 * torch.dot(delta, H @ delta).item()\n",
    "        actual_second = (h_func(x0 + delta) - h0 - torch.dot(g, delta)).item()\n",
    "        rel_error = abs(pred_second - actual_second) / (abs(actual_second) + 1e-8)\n",
    "        node_errors.append(rel_error)\n",
    "\n",
    "    avg_rel_error_node = float(np.mean(node_errors))\n",
    "\n",
    "    # Store\n",
    "    class_hessian_eig[class_name].append(lambda_max)\n",
    "    class_rel_errors[class_name].append(avg_rel_error_node)\n",
    "    overall_hessian_eig.append(lambda_max)\n",
    "    overall_rel_errors.append(avg_rel_error_node)\n",
    "\n",
    "# --------------------------------\n",
    "# 4) Reporting\n",
    "# --------------------------------\n",
    "print(\"\\nClass-wise Hessian-Based Curvature (with Relative Errors):\")\n",
    "header = \"{:<12s} {:>33s} {:>30s}\".format(\"Class\", \"Avg. Max Eigenvalue ± Std\", \"Avg Rel. Error ± Std\")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for cn in class_names:\n",
    "    c_vals = class_hessian_eig[cn]\n",
    "    e_vals = class_rel_errors[cn]\n",
    "    if c_vals:\n",
    "        print(\"{:<12s} {:>11.4f} ± {:<10.4f} {:>15.4e} ± {:<10.4e}\".format(\n",
    "            cn, np.mean(c_vals), np.std(c_vals), np.mean(e_vals), np.std(e_vals)\n",
    "        ))\n",
    "    else:\n",
    "        print(\"{:<12s} {:>11s} {:>10s} {:>15s} {:>10s}\".format(cn, \"-\", \"-\", \"-\", \"-\"))\n",
    "\n",
    "if overall_hessian_eig:\n",
    "    print(\"\\nOverall Aggregated Hessian-Based Curvature:\")\n",
    "    print(\"Avg Max Eigenvalue: {:.4f} ± {:.4f}\".format(np.mean(overall_hessian_eig), np.std(overall_hessian_eig)))\n",
    "    print(\"Avg Relative Error: {:.4e} ± {:.4e}\".format(np.mean(overall_rel_errors), np.std(overall_rel_errors)))\n",
    "else:\n",
    "    print(\"\\nOverall Aggregated Hessian-Based Curvature:\")\n",
    "    print(\"No samples selected; overall metrics unavailable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4debe1da",
   "metadata": {},
   "source": [
    "#### Prediction Margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd7ab3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction Margin & Relative Error (GAT, 100 Samples Per Class) ---\n",
      "\n",
      "Sampling report:\n",
      "- and: picked 100 from 5637 available.\n",
      "- input: picked 100 from 475 available.\n",
      "- nand: picked 100 from 527 available.\n",
      "- nor: picked 100 from 567 available.\n",
      "- not: picked 100 from 4478 available.\n",
      "- or: picked 100 from 197 available.\n",
      "- output: picked 100 from 266 available.\n",
      "- xor: only 30 available, taking all.\n",
      "\n",
      "Final counts in selected set:\n",
      "- and: 100\n",
      "- input: 100\n",
      "- nand: 100\n",
      "- nor: 100\n",
      "- not: 100\n",
      "- or: 100\n",
      "- output: 100\n",
      "- xor: 30\n",
      "\n",
      "Class-wise Prediction Margin (with Relative Error):\n",
      "Class                        Avg. Margin ± Std           Avg Rel. Error ± Std\n",
      "-----------------------------------------------------------------------------\n",
      "and               3.1969 ± 1.4619          8.9702e-06 ± 9.0874e-06\n",
      "input            13.8249 ± 0.9520          1.9818e-06 ± 1.5289e-06\n",
      "nand              1.0233 ± 0.6257          2.1029e-04 ± 1.8450e-03\n",
      "nor               1.6374 ± 0.7790          3.8585e-05 ± 1.9952e-04\n",
      "not               5.4919 ± 0.7930          9.2372e-06 ± 6.4394e-06\n",
      "or                1.6639 ± 0.5411          1.3110e-05 ± 1.7441e-05\n",
      "output           15.2672 ± 0.6195          1.4733e-06 ± 1.0205e-06\n",
      "xor              17.2003 ± 0.6693          1.0557e-06 ± 6.1537e-07\n",
      "\n",
      "Overall Aggregated Prediction Margin:\n",
      "Avg Margin: 6.4747 ± 5.9599\n",
      "Avg Relative Error: 3.8899e-05 ± 6.9037e-04\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Prediction Margin & Relative Error (GAT, 100 Samples Per Class) ---\\n\")\n",
    "\n",
    "# --------------------------------\n",
    "# 0) Setup\n",
    "# --------------------------------\n",
    "model.eval()\n",
    "class_names = list(label_encoder.classes_)\n",
    "num_classes = len(class_names)\n",
    "labels_np = graph.ndata['labels'].cpu().numpy()\n",
    "\n",
    "# --------------------------------\n",
    "# 1) Build selected_test_nodes: up to 100 per class from test_nid\n",
    "# --------------------------------\n",
    "rng = np.random.default_rng(42)\n",
    "test_indices = test_nid.cpu().numpy()\n",
    "\n",
    "selected_test_nodes = []\n",
    "print(\"Sampling report:\")\n",
    "for c in range(num_classes):\n",
    "    idxs = [int(i) for i in test_indices if labels_np[int(i)] == c]\n",
    "    n_avail = len(idxs)\n",
    "    if n_avail == 0:\n",
    "        print(f\"- {class_names[c]}: 0 available in test set  skipped.\")\n",
    "        continue\n",
    "    if n_avail >= 100:\n",
    "        chosen = rng.choice(idxs, size=100, replace=False)\n",
    "        print(f\"- {class_names[c]}: picked 100 from {n_avail} available.\")\n",
    "    else:\n",
    "        chosen = np.array(idxs, dtype=np.int64)\n",
    "        print(f\"- {class_names[c]}: only {n_avail} available, taking all.\")\n",
    "    selected_test_nodes.extend(chosen.tolist())\n",
    "\n",
    "selected_test_nodes = np.array(selected_test_nodes, dtype=np.int64)\n",
    "\n",
    "# Final per-class counts\n",
    "final_counts = {class_names[c]: int(np.sum(labels_np[selected_test_nodes] == c)) for c in range(num_classes)}\n",
    "print(\"\\nFinal counts in selected set:\")\n",
    "for name, cnt in final_counts.items():\n",
    "    print(f\"- {name}: {cnt}\")\n",
    "\n",
    "# --------------------------------\n",
    "# 2) Main loop: Prediction margin + relative error\n",
    "# --------------------------------\n",
    "class_margin_vals = {cn: [] for cn in class_names}\n",
    "class_rel_errors  = {cn: [] for cn in class_names}\n",
    "overall_margin_vals = []\n",
    "overall_rel_errors  = []\n",
    "\n",
    "epsilon = 1e-5  # small perturbation for verification\n",
    "\n",
    "for test_idx in selected_test_nodes:\n",
    "    label_idx = int(graph.ndata['labels'][test_idx].item())\n",
    "    class_name = class_names[label_idx]\n",
    "\n",
    "    # Original logits\n",
    "    with torch.no_grad():\n",
    "        logits = model(graph, graph.ndata['features'])[test_idx]\n",
    "\n",
    "    pred_class = int(torch.argmax(logits))\n",
    "    pred_logit = logits[pred_class].item()\n",
    "\n",
    "    # Second max logit\n",
    "    other_logits = logits.clone()\n",
    "    other_logits[pred_class] = -float('inf')\n",
    "    second_max = other_logits.max().item()\n",
    "\n",
    "    margin = pred_logit - second_max\n",
    "\n",
    "    # Relative error verification\n",
    "    perturbed_feats = graph.ndata['features'].clone()\n",
    "    perturb_vec = torch.randn_like(perturbed_feats[test_idx]) * epsilon\n",
    "    perturbed_feats[test_idx] += perturb_vec\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits_pert = model(graph, perturbed_feats)[test_idx]\n",
    "    pred_logit_pert = logits_pert[pred_class].item()\n",
    "    other_logits_pert = logits_pert.clone()\n",
    "    other_logits_pert[pred_class] = -float('inf')\n",
    "    second_max_pert = other_logits_pert.max().item()\n",
    "    margin_pert = pred_logit_pert - second_max_pert\n",
    "\n",
    "    rel_err = abs(margin - margin_pert) / (abs(margin_pert) + 1e-12)\n",
    "\n",
    "    # Store\n",
    "    class_margin_vals[class_name].append(margin)\n",
    "    class_rel_errors[class_name].append(rel_err)\n",
    "    overall_margin_vals.append(margin)\n",
    "    overall_rel_errors.append(rel_err)\n",
    "\n",
    "# --------------------------------\n",
    "# 3) Reporting\n",
    "# --------------------------------\n",
    "print(\"\\nClass-wise Prediction Margin (with Relative Error):\")\n",
    "header = \"{:<12s} {:>33s} {:>30s}\".format(\"Class\", \"Avg. Margin ± Std\", \"Avg Rel. Error ± Std\")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for cn in class_names:\n",
    "    m_vals = class_margin_vals[cn]\n",
    "    e_vals = class_rel_errors[cn]\n",
    "    if m_vals:\n",
    "        print(\"{:<12s} {:>11.4f} ± {:<10.4f} {:>15.4e} ± {:<10.4e}\".format(\n",
    "            cn, np.mean(m_vals), np.std(m_vals),\n",
    "            np.mean(e_vals), np.std(e_vals)\n",
    "        ))\n",
    "    else:\n",
    "        print(\"{:<12s} {:>11s} {:<10s} {:>15s} {:<10s}\".format(cn, \"-\", \"-\", \"-\", \"-\"))\n",
    "\n",
    "if overall_margin_vals:\n",
    "    print(\"\\nOverall Aggregated Prediction Margin:\")\n",
    "    print(\"Avg Margin: {:.4f} ± {:.4f}\".format(np.mean(overall_margin_vals), np.std(overall_margin_vals)))\n",
    "    print(\"Avg Relative Error: {:.4e} ± {:.4e}\".format(np.mean(overall_rel_errors), np.std(overall_rel_errors)))\n",
    "else:\n",
    "    print(\"\\nOverall Aggregated Prediction Margin:\")\n",
    "    print(\"No samples selected; overall metrics unavailable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a96ce7",
   "metadata": {},
   "source": [
    "#### Adversarial Robustness Radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2175c8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Adversarial Robustness Radius & Relative Error (GAT, 100 Samples Per Class) ---\n",
      "\n",
      "Sampling report:\n",
      "- and: picked 100 from 5637 available.\n",
      "- input: picked 100 from 475 available.\n",
      "- nand: picked 100 from 527 available.\n",
      "- nor: picked 100 from 567 available.\n",
      "- not: picked 100 from 4478 available.\n",
      "- or: picked 100 from 197 available.\n",
      "- output: picked 100 from 266 available.\n",
      "- xor: only 30 available, taking all.\n",
      "\n",
      "Final counts in selected set:\n",
      "- and: 100\n",
      "- input: 100\n",
      "- nand: 100\n",
      "- nor: 100\n",
      "- not: 100\n",
      "- or: 100\n",
      "- output: 100\n",
      "- xor: 30\n",
      "\n",
      "Class-wise Adversarial Robustness Radius (with Relative Error):\n",
      "Class                  Avg. Radius ± Std           Avg Rel. Error ± Std\n",
      "-----------------------------------------------------------------------\n",
      "and               2.1802 ± 2.0720          3.3623e-01 ± 3.2424e-01\n",
      "input             7.7854 ± 1.5368          2.0797e-01 ± 1.8350e-01\n",
      "nand              1.3549 ± 1.3504          3.7618e-01 ± 5.7107e-01\n",
      "nor               1.9357 ± 1.5046          2.5689e-01 ± 2.9091e-01\n",
      "not               2.2723 ± 1.0205          3.7456e-01 ± 3.8623e-01\n",
      "or                2.8211 ± 2.9086          4.2190e-01 ± 4.8478e-01\n",
      "output            8.8391 ± 1.1466          1.3582e-01 ± 1.1728e-01\n",
      "xor              10.0000 ± 0.0000          0.0000e+00 ± 0.0000e+00\n",
      "\n",
      "Overall Aggregated Adversarial Robustness Radius:\n",
      "Avg Radius: 4.1354 ± 3.4900\n",
      "Avg Relative Error: 2.8898e-01 ± 3.7716e-01\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Adversarial Robustness Radius & Relative Error (GAT, 100 Samples Per Class) ---\\n\")\n",
    "\n",
    "# --------------------------------\n",
    "# 0) Setup\n",
    "# --------------------------------\n",
    "model.eval()\n",
    "class_names = list(label_encoder.classes_)\n",
    "num_classes = len(class_names)\n",
    "labels_np = graph.ndata['labels'].cpu().numpy()\n",
    "\n",
    "# --------------------------------\n",
    "# 1) Build selected_test_nodes: up to 100 per class from test_nid\n",
    "# --------------------------------\n",
    "rng = np.random.default_rng(42)\n",
    "test_indices = test_nid.cpu().numpy()\n",
    "\n",
    "selected_test_nodes = []\n",
    "print(\"Sampling report:\")\n",
    "for c in range(num_classes):\n",
    "    idxs = [int(i) for i in test_indices if labels_np[int(i)] == c]\n",
    "    n_avail = len(idxs)\n",
    "    if n_avail == 0:\n",
    "        print(f\"- {class_names[c]}: 0 available in test set  skipped.\")\n",
    "        continue\n",
    "    if n_avail >= 100:\n",
    "        chosen = rng.choice(idxs, size=100, replace=False)\n",
    "        print(f\"- {class_names[c]}: picked 100 from {n_avail} available.\")\n",
    "    else:\n",
    "        chosen = np.array(idxs, dtype=np.int64)\n",
    "        print(f\"- {class_names[c]}: only {n_avail} available, taking all.\")\n",
    "    selected_test_nodes.extend(chosen.tolist())\n",
    "\n",
    "selected_test_nodes = np.array(selected_test_nodes, dtype=np.int64)\n",
    "\n",
    "# Final per-class counts\n",
    "final_counts = {class_names[c]: int(np.sum(labels_np[selected_test_nodes] == c)) for c in range(num_classes)}\n",
    "print(\"\\nFinal counts in selected set:\")\n",
    "for name, cnt in final_counts.items():\n",
    "    print(f\"- {name}: {cnt}\")\n",
    "\n",
    "# --------------------------------\n",
    "# 2) Prediction wrapper for a node (replace its feature vector with x)\n",
    "# --------------------------------\n",
    "def f_for_sample(x, test_idx):\n",
    "    \"\"\"\n",
    "    Given an input x (modified feature vector for the test node),\n",
    "    returns the output (logit vector) for that node.\n",
    "    \"\"\"\n",
    "    new_features = graph.ndata['features'].clone().detach()\n",
    "    new_features[test_idx] = x\n",
    "    with torch.no_grad():\n",
    "        out = model(graph, new_features)\n",
    "    return out[test_idx]\n",
    "\n",
    "# --------------------------------\n",
    "# 3) Adversarial radius along random directions + binary search\n",
    "# --------------------------------\n",
    "def adversarial_radius_for_sample(\n",
    "    test_idx,\n",
    "    initial_epsilon=1e-3,\n",
    "    growth_factor=1.2,\n",
    "    max_epsilon=10.0,\n",
    "    bs_iters=10,\n",
    "    num_trials=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Minimal perturbation norm required to change the prediction, estimated by:\n",
    "      - Sampling random directions d (||d||=1)\n",
    "      - Expanding epsilon multiplicatively until the prediction flips (or cap)\n",
    "      - Binary-searching in the bracket to refine the boundary\n",
    "    Returns the minimum over 'num_trials' sampled directions.\n",
    "    \"\"\"\n",
    "    x0 = graph.ndata['features'][test_idx].clone().detach()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        base_out = model(graph, graph.ndata['features'])\n",
    "        y0 = int(torch.argmax(base_out[test_idx]).item())\n",
    "\n",
    "    def is_same(x):\n",
    "        out = f_for_sample(x, test_idx)\n",
    "        return int(torch.argmax(out).item()) == y0\n",
    "\n",
    "    radii = []\n",
    "    for _ in range(num_trials):\n",
    "        d = torch.randn_like(x0)\n",
    "        d = d / (torch.norm(d) + 1e-12)\n",
    "\n",
    "        epsilon = initial_epsilon\n",
    "        # Expand until flip or cap\n",
    "        while epsilon < max_epsilon and is_same(x0 + epsilon * d):\n",
    "            epsilon *= growth_factor\n",
    "\n",
    "        if epsilon >= max_epsilon:\n",
    "            candidate = max_epsilon\n",
    "        else:\n",
    "            # Binary search in [epsilon/growth_factor, epsilon]\n",
    "            low = epsilon / growth_factor\n",
    "            high = epsilon\n",
    "            for _ in range(bs_iters):\n",
    "                mid = 0.5 * (low + high)\n",
    "                if is_same(x0 + mid * d):\n",
    "                    low = mid\n",
    "                else:\n",
    "                    high = mid\n",
    "            candidate = high\n",
    "        radii.append(candidate)\n",
    "\n",
    "    return float(min(radii))\n",
    "\n",
    "# --------------------------------\n",
    "# 4) Relative error verification (re-estimate with different config)\n",
    "# --------------------------------\n",
    "def adversarial_radius_relerr(test_idx):\n",
    "    r1 = adversarial_radius_for_sample(\n",
    "        test_idx,\n",
    "        initial_epsilon=1e-3,\n",
    "        growth_factor=1.2,\n",
    "        max_epsilon=10.0,\n",
    "        bs_iters=10,\n",
    "        num_trials=10,\n",
    "    )\n",
    "    r2 = adversarial_radius_for_sample(\n",
    "        test_idx,\n",
    "        initial_epsilon=1e-3,\n",
    "        growth_factor=1.3,  # different growth factor / search iters\n",
    "        max_epsilon=10.0,\n",
    "        bs_iters=12,\n",
    "        num_trials=10,\n",
    "    )\n",
    "    rel_err = abs(r1 - r2) / (abs(r2) + 1e-12)\n",
    "    return r1, rel_err\n",
    "\n",
    "# --------------------------------\n",
    "# 5) Main loop: compute ARR + relative error per class\n",
    "# --------------------------------\n",
    "class_adv_radius = {cn: [] for cn in class_names}\n",
    "class_rel_errors  = {cn: [] for cn in class_names}\n",
    "overall_adv_radius_vals = []\n",
    "overall_rel_errors      = []\n",
    "\n",
    "for test_idx in selected_test_nodes:\n",
    "    label_idx = int(graph.ndata['labels'][test_idx].item())\n",
    "    class_name = class_names[label_idx]\n",
    "\n",
    "    radius, rel_err = adversarial_radius_relerr(test_idx)\n",
    "\n",
    "    class_adv_radius[class_name].append(radius)\n",
    "    class_rel_errors[class_name].append(rel_err)\n",
    "    overall_adv_radius_vals.append(radius)\n",
    "    overall_rel_errors.append(rel_err)\n",
    "\n",
    "# --------------------------------\n",
    "# 6) Reporting\n",
    "# --------------------------------\n",
    "print(\"\\nClass-wise Adversarial Robustness Radius (with Relative Error):\")\n",
    "header = \"{:<12s} {:>27s} {:>30s}\".format(\"Class\", \"Avg. Radius ± Std\", \"Avg Rel. Error ± Std\")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for cn in class_names:\n",
    "    r_vals = class_adv_radius[cn]\n",
    "    e_vals = class_rel_errors[cn]\n",
    "    if r_vals:\n",
    "        print(\"{:<12s} {:>11.4f} ± {:<10.4f} {:>15.4e} ± {:<10.4e}\".format(\n",
    "            cn, np.mean(r_vals), np.std(r_vals),\n",
    "            np.mean(e_vals), np.std(e_vals)\n",
    "        ))\n",
    "    else:\n",
    "        print(\"{:<12s} {:>11s} {:<10s} {:>15s} {:<10s}\".format(cn, \"-\", \"-\", \"-\", \"-\"))\n",
    "\n",
    "if overall_adv_radius_vals:\n",
    "    print(\"\\nOverall Aggregated Adversarial Robustness Radius:\")\n",
    "    print(\"Avg Radius: {:.4f} ± {:.4f}\".format(np.mean(overall_adv_radius_vals), np.std(overall_adv_radius_vals)))\n",
    "    print(\"Avg Relative Error: {:.4e} ± {:.4e}\".format(np.mean(overall_rel_errors), np.std(overall_rel_errors)))\n",
    "else:\n",
    "    print(\"\\nOverall Aggregated Adversarial Robustness Radius:\")\n",
    "    print(\"No samples selected; overall metrics unavailable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00574a57",
   "metadata": {},
   "source": [
    "#### Stability Under Input Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "351dab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stability Under Input Noise & Relative Error (GAT, 100 Samples Per Class) ---\n",
      "\n",
      "Sampling report:\n",
      "- and: picked 100 from 5637 available.\n",
      "- input: picked 100 from 475 available.\n",
      "- nand: picked 100 from 527 available.\n",
      "- nor: picked 100 from 567 available.\n",
      "- not: picked 100 from 4478 available.\n",
      "- or: picked 100 from 197 available.\n",
      "- output: picked 100 from 266 available.\n",
      "- xor: only 30 available, taking all.\n",
      "\n",
      "Final counts in selected set:\n",
      "- and: 100\n",
      "- input: 100\n",
      "- nand: 100\n",
      "- nor: 100\n",
      "- not: 100\n",
      "- or: 100\n",
      "- output: 100\n",
      "- xor: 30\n",
      "\n",
      "Class-wise Stability Under Input Noise (with Relative Error):\n",
      "Class                     Avg. Stability ± Std           Avg Rel. Error ± Std\n",
      "-----------------------------------------------------------------------------\n",
      "and               0.0861 ± 0.0173          7.8425e-02 ± 6.2327e-02\n",
      "input             0.0700 ± 0.0073          9.0830e-02 ± 5.9467e-02\n",
      "nand              0.0837 ± 0.0159          9.2659e-02 ± 6.2658e-02\n",
      "nor               0.0777 ± 0.0117          7.3371e-02 ± 5.1060e-02\n",
      "not               0.0883 ± 0.0123          7.7306e-02 ± 5.7889e-02\n",
      "or                0.0787 ± 0.0237          8.3472e-02 ± 6.3931e-02\n",
      "output            0.0662 ± 0.0056          7.1796e-02 ± 4.9603e-02\n",
      "xor               0.0587 ± 0.0043          6.2230e-02 ± 6.7709e-02\n",
      "\n",
      "Overall Aggregated Stability Under Input Noise:\n",
      "Avg Stability: 0.0779 ± 0.0166\n",
      "Avg Relative Error: 8.0346e-02 ± 5.9372e-02\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Stability Under Input Noise & Relative Error (GAT, 100 Samples Per Class) ---\\n\")\n",
    "\n",
    "# --------------------------------\n",
    "# 0) Setup\n",
    "# --------------------------------\n",
    "model.eval()\n",
    "class_names = list(label_encoder.classes_)\n",
    "num_classes = len(class_names)\n",
    "labels_np = graph.ndata['labels'].cpu().numpy()\n",
    "\n",
    "# --------------------------------\n",
    "# 1) Build selected_test_nodes: up to 100 per class from test_nid\n",
    "# --------------------------------\n",
    "rng = np.random.default_rng(42)\n",
    "test_indices = test_nid.cpu().numpy()\n",
    "\n",
    "selected_test_nodes = []\n",
    "print(\"Sampling report:\")\n",
    "for c in range(num_classes):\n",
    "    idxs = [int(i) for i in test_indices if labels_np[int(i)] == c]\n",
    "    n_avail = len(idxs)\n",
    "    if n_avail == 0:\n",
    "        print(f\"- {class_names[c]}: 0 available in test set  skipped.\")\n",
    "        continue\n",
    "    if n_avail >= 100:\n",
    "        chosen = rng.choice(idxs, size=100, replace=False)\n",
    "        print(f\"- {class_names[c]}: picked 100 from {n_avail} available.\")\n",
    "    else:\n",
    "        chosen = np.array(idxs, dtype=np.int64)\n",
    "        print(f\"- {class_names[c]}: only {n_avail} available, taking all.\")\n",
    "    selected_test_nodes.extend(chosen.tolist())\n",
    "\n",
    "selected_test_nodes = np.array(selected_test_nodes, dtype=np.int64)\n",
    "\n",
    "# Final per-class counts\n",
    "final_counts = {class_names[c]: int(np.sum(labels_np[selected_test_nodes] == c)) for c in range(num_classes)}\n",
    "print(\"\\nFinal counts in selected set:\")\n",
    "for name, cnt in final_counts.items():\n",
    "    print(f\"- {name}: {cnt}\")\n",
    "\n",
    "# --------------------------------\n",
    "# 2) Helper: stability under Gaussian noise\n",
    "# --------------------------------\n",
    "def stability_for_sample(test_idx, sigma, num_samples):\n",
    "    \"\"\"\n",
    "    Average L2 change in logits between clean and noisy versions of the node.\n",
    "    \"\"\"\n",
    "    x0 = graph.ndata['features'][test_idx].clone().detach()\n",
    "    with torch.no_grad():\n",
    "        f_orig = model(graph, graph.ndata['features'])[test_idx]\n",
    "\n",
    "    diffs = []\n",
    "    for _ in range(num_samples):\n",
    "        noise = sigma * torch.randn_like(x0)\n",
    "        x_noisy = x0 + noise\n",
    "        new_feats = graph.ndata['features'].clone().detach()\n",
    "        new_feats[test_idx] = x_noisy\n",
    "        with torch.no_grad():\n",
    "            f_noisy = model(graph, new_feats)[test_idx]\n",
    "        diffs.append(torch.norm(f_noisy - f_orig).item())\n",
    "    return float(np.mean(diffs))\n",
    "\n",
    "# --------------------------------\n",
    "# 3) Main loop: stability + relative error\n",
    "# --------------------------------\n",
    "class_stability = {cn: [] for cn in class_names}\n",
    "class_rel_errors = {cn: [] for cn in class_names}\n",
    "overall_stability_vals = []\n",
    "overall_rel_errors = []\n",
    "\n",
    "sigma = 0.01\n",
    "num_noise_samples = 20\n",
    "relerr_resamples = 5\n",
    "\n",
    "for test_idx in selected_test_nodes:\n",
    "    label_idx = int(graph.ndata['labels'][test_idx].item())\n",
    "    class_name = class_names[label_idx]\n",
    "\n",
    "    stab_val = stability_for_sample(test_idx, sigma, num_noise_samples)\n",
    "\n",
    "    # Relative error check: re-estimate stability with fresh noise\n",
    "    re_vals = [stability_for_sample(test_idx, sigma, num_noise_samples)\n",
    "               for _ in range(relerr_resamples)]\n",
    "    avg_reval = float(np.mean(re_vals))\n",
    "    rel_err = abs(stab_val - avg_reval) / (abs(avg_reval) + 1e-12)\n",
    "\n",
    "    class_stability[class_name].append(stab_val)\n",
    "    class_rel_errors[class_name].append(rel_err)\n",
    "    overall_stability_vals.append(stab_val)\n",
    "    overall_rel_errors.append(rel_err)\n",
    "\n",
    "# --------------------------------\n",
    "# 4) Reporting\n",
    "# --------------------------------\n",
    "print(\"\\nClass-wise Stability Under Input Noise (with Relative Error):\")\n",
    "header = \"{:<12s} {:>33s} {:>30s}\".format(\"Class\", \"Avg. Stability ± Std\", \"Avg Rel. Error ± Std\")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for cn in class_names:\n",
    "    s_vals = class_stability[cn]\n",
    "    e_vals = class_rel_errors[cn]\n",
    "    if s_vals:\n",
    "        print(\"{:<12s} {:>11.4f} ± {:<10.4f} {:>15.4e} ± {:<10.4e}\".format(\n",
    "            cn, np.mean(s_vals), np.std(s_vals),\n",
    "            np.mean(e_vals), np.std(e_vals)\n",
    "        ))\n",
    "    else:\n",
    "        print(\"{:<12s} {:>11s} {:<10s} {:>15s} {:<10s}\".format(cn, \"-\", \"-\", \"-\", \"-\"))\n",
    "\n",
    "if overall_stability_vals:\n",
    "    print(\"\\nOverall Aggregated Stability Under Input Noise:\")\n",
    "    print(\"Avg Stability: {:.4f} ± {:.4f}\".format(np.mean(overall_stability_vals), np.std(overall_stability_vals)))\n",
    "    print(\"Avg Relative Error: {:.4e} ± {:.4e}\".format(np.mean(overall_rel_errors), np.std(overall_rel_errors)))\n",
    "else:\n",
    "    print(\"\\nOverall Aggregated Stability Under Input Noise:\")\n",
    "    print(\"No samples selected; overall metrics unavailable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09891bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
