{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d2f0719",
   "metadata": {},
   "source": [
    "#### Training GraphSAINT\n",
    "\n",
    "Model using the graph dataset extracted from the csv file. TO generate csv file, please run the parsing notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52bbdb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "?? Dataset Overview:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60882 entries, 0 to 60881\n",
      "Data columns (total 18 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   circuit_name            60882 non-null  object \n",
      " 1   node                    60882 non-null  object \n",
      " 2   gate_type               60882 non-null  object \n",
      " 3   fan_in                  60882 non-null  int64  \n",
      " 4   fan_out                 60882 non-null  int64  \n",
      " 5   depth                   60882 non-null  object \n",
      " 6   dist_to_output          60882 non-null  int64  \n",
      " 7   is_primary_input        60882 non-null  int64  \n",
      " 8   is_primary_output       60882 non-null  int64  \n",
      " 9   is_internal             60882 non-null  int64  \n",
      " 10  is_key_gate             60882 non-null  int64  \n",
      " 11  key_dependency          122 non-null    object \n",
      " 12  degree_centrality       60882 non-null  float64\n",
      " 13  betweenness_centrality  60882 non-null  float64\n",
      " 14  closeness_centrality    60882 non-null  float64\n",
      " 15  clustering_coefficient  60882 non-null  float64\n",
      " 16  avg_fan_in_neighbors    60882 non-null  float64\n",
      " 17  avg_fan_out_neighbors   60882 non-null  float64\n",
      "dtypes: float64(6), int64(7), object(5)\n",
      "memory usage: 8.4+ MB\n",
      "None\n",
      "\n",
      "?? Extracted 6172 edges.\n",
      "Epoch 0/50, SAINT avg loss: 0.9391\n",
      "Epoch 10/50, SAINT avg loss: 0.1323\n",
      "Epoch 20/50, SAINT avg loss: 0.1113\n",
      "Epoch 30/50, SAINT avg loss: 0.1000\n",
      "Epoch 40/50, SAINT avg loss: 0.0925\n",
      "\n",
      "? Test Accuracy on Original Graph: 96.68%\n",
      "\n",
      "?? Confusion Matrix (Original Graph):\n",
      "[[5537    0   71   12    0   17    0    0]\n",
      " [   0  475    0    0    0    0    0    0]\n",
      " [ 107    0  411    3    0    6    0    0]\n",
      " [  51    0   16  493    0    7    0    0]\n",
      " [   0    0    0    0 4478    0    0    0]\n",
      " [  96    0   13    5    0   83    0    0]\n",
      " [   0    0    0    0    0    0  266    0]\n",
      " [   0    0    0    0    0    0    0   30]]\n",
      "\n",
      "Classification Report (Original Graph):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         and       0.96      0.98      0.97      5637\n",
      "       input       1.00      1.00      1.00       475\n",
      "        nand       0.80      0.78      0.79       527\n",
      "         nor       0.96      0.87      0.91       567\n",
      "         not       1.00      1.00      1.00      4478\n",
      "          or       0.73      0.42      0.54       197\n",
      "      output       1.00      1.00      1.00       266\n",
      "         xor       1.00      1.00      1.00        30\n",
      "\n",
      "    accuracy                           0.97     12177\n",
      "   macro avg       0.93      0.88      0.90     12177\n",
      "weighted avg       0.97      0.97      0.97     12177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "from dgl.dataloading import SAINTSampler\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Load Dataset and Prepare Data\n",
    "# ------------------------------\n",
    "df = pd.read_csv(\"all_circuits_features.csv\")\n",
    "print(\"\\n?? Dataset Overview:\")\n",
    "print(df.info())\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"gate_label\"] = label_encoder.fit_transform(df[\"gate_type\"])\n",
    "\n",
    "feature_columns = [\n",
    "    \"fan_in\", \"fan_out\", \"dist_to_output\", \"is_primary_input\", \"is_primary_output\",\n",
    "    \"is_internal\", \"is_key_gate\", \"degree_centrality\", \"betweenness_centrality\",\n",
    "    \"closeness_centrality\", \"clustering_coefficient\", \"avg_fan_in_neighbors\", \"avg_fan_out_neighbors\"\n",
    "]\n",
    "\n",
    "df = df.dropna(subset=feature_columns)\n",
    "df[feature_columns] = df[feature_columns].astype(float)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[feature_columns] = scaler.fit_transform(df[feature_columns])\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Build Node and Edge Lists\n",
    "# ---------------------------\n",
    "nodes = df[\"node\"].tolist()\n",
    "node_to_id = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "edges = []\n",
    "for _, row in df.iterrows():\n",
    "    node_id = node_to_id[row[\"node\"]]\n",
    "    potential_sources = df[df[\"fan_out\"] > 0][\"node\"].tolist()\n",
    "    num_fan_in = int(row[\"fan_in\"])\n",
    "    if num_fan_in > 0:\n",
    "        sources = potential_sources[:num_fan_in]\n",
    "        for src in sources:\n",
    "            if src in node_to_id:\n",
    "                edges.append((node_to_id[src], node_id))\n",
    "\n",
    "print(\"\\n?? Extracted\", len(edges), \"edges.\")\n",
    "if len(edges) == 0:\n",
    "    raise ValueError(\"No edges found! Check your fan_in values.\")\n",
    "\n",
    "src_nodes, dst_nodes = zip(*edges) if edges else ([], [])\n",
    "src_tensor = torch.tensor(src_nodes, dtype=torch.int64)\n",
    "dst_tensor = torch.tensor(dst_nodes, dtype=torch.int64)\n",
    "valid_edges = (\n",
    "    (src_tensor >= 0) & (dst_tensor >= 0) &\n",
    "    (src_tensor < len(nodes)) & (dst_tensor < len(nodes))\n",
    ")\n",
    "src_tensor = src_tensor[valid_edges]\n",
    "dst_tensor = dst_tensor[valid_edges]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Create the DGL Graph\n",
    "# ---------------------------\n",
    "graph = dgl.graph((src_tensor, dst_tensor), num_nodes=len(nodes))\n",
    "graph = dgl.add_self_loop(graph)\n",
    "\n",
    "graph.ndata['features'] = torch.tensor(df[feature_columns].values, dtype=torch.float32)\n",
    "graph.ndata['labels'] = torch.tensor(df[\"gate_label\"].values, dtype=torch.long)\n",
    "\n",
    "nodes_idx = np.arange(len(nodes))\n",
    "train_idx, test_idx = train_test_split(nodes_idx, test_size=0.2, random_state=42)\n",
    "train_nid = torch.tensor(train_idx, dtype=torch.long)\n",
    "test_nid = torch.tensor(test_idx, dtype=torch.long)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Define the GraphSAINT Model\n",
    "# ---------------------------\n",
    "class GraphSAINTModel(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, out_feats):\n",
    "        super().__init__()\n",
    "        self.conv1 = dglnn.GraphConv(in_feats, hidden_feats, allow_zero_in_degree=True)\n",
    "        self.conv2 = dglnn.GraphConv(hidden_feats, out_feats, allow_zero_in_degree=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        x = self.conv1(g, x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(g, x)\n",
    "        return x\n",
    "\n",
    "in_feats = len(feature_columns)\n",
    "hidden_feats = 32\n",
    "out_feats = len(label_encoder.classes_)\n",
    "model = GraphSAINTModel(in_feats, hidden_feats, out_feats)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. GraphSAINT Sampler & Training Loop\n",
    "# ---------------------------\n",
    "saint_mode = 'walk'         # 'walk', 'node', or 'edge'\n",
    "saint_budget = (3000, 2)    # For 'walk': (num_roots, walk_length)\n",
    "num_traversals = 30         # subgraphs per epoch\n",
    "\n",
    "sampler = SAINTSampler(\n",
    "    mode=saint_mode,\n",
    "    budget=saint_budget\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    for _ in range(num_traversals):\n",
    "        subg = sampler.sample(graph, train_nid)\n",
    "\n",
    "        # Materialize features/labels from parent graph\n",
    "        nids = subg.ndata[dgl.NID]\n",
    "        feats = graph.ndata['features'][nids]\n",
    "        labels = graph.ndata['labels'][nids]\n",
    "\n",
    "        logits = model(subg, feats)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        steps += 1\n",
    "\n",
    "    avg_loss = total_loss / max(steps, 1)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, SAINT avg loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Evaluation on Full Graph\n",
    "# ---------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(graph, graph.ndata['features'])\n",
    "    test_logits = logits[test_nid]\n",
    "    test_predictions = test_logits.argmax(dim=1)\n",
    "    orig_accuracy = (test_predictions == graph.ndata['labels'][test_nid]).float().mean().item()\n",
    "\n",
    "print(f\"\\n? Test Accuracy on Original Graph: {orig_accuracy * 100:.2f}%\")\n",
    "\n",
    "true_labels_orig = graph.ndata['labels'][test_nid].cpu().numpy()\n",
    "pred_labels_orig = test_predictions.cpu().numpy()\n",
    "conf_mat_orig = confusion_matrix(true_labels_orig, pred_labels_orig)\n",
    "\n",
    "print(\"\\n?? Confusion Matrix (Original Graph):\")\n",
    "print(conf_mat_orig)\n",
    "\n",
    "print(\"\\nClassification Report (Original Graph):\")\n",
    "print(classification_report(true_labels_orig, pred_labels_orig, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018700d",
   "metadata": {},
   "source": [
    "#### Jacobian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30f42bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Jacobian Computation and Relative Error (GraphSAINT, 100 Samples Per Class) ---\n",
      "\n",
      "Sampling report:\n",
      "- and: picked 100 from 5637 available.\n",
      "- input: picked 100 from 475 available.\n",
      "- nand: picked 100 from 527 available.\n",
      "- nor: picked 100 from 567 available.\n",
      "- not: picked 100 from 4478 available.\n",
      "- or: picked 100 from 197 available.\n",
      "- output: picked 100 from 266 available.\n",
      "- xor: only 30 available, taking all.\n",
      "\n",
      "Final counts in selected set:\n",
      "- and: 100\n",
      "- input: 100\n",
      "- nand: 100\n",
      "- nor: 100\n",
      "- not: 100\n",
      "- or: 100\n",
      "- output: 100\n",
      "- xor: 30\n",
      "\n",
      "Class-wise Jacobian Analysis Results:\n",
      "Class              Avg. Jacobian Norm ± Std           Avg. Relative Error ± Std\n",
      "-------------------------------------------------------------------------------\n",
      "and                  45.7088 ± 13.3837           1.1304e-04 ± 2.9016e-04\n",
      "input                27.1540 ± 3.0831            1.8050e-02 ± 1.0712e-01\n",
      "nand                 22.2573 ± 7.0761            2.6081e-03 ± 1.6647e-02\n",
      "nor                  22.7983 ± 3.6259            1.6883e-03 ± 1.5398e-02\n",
      "not                  48.2800 ± 7.0847            1.0774e-04 ± 7.4573e-05\n",
      "or                   29.8241 ± 17.7125           1.4782e-04 ± 1.3114e-04\n",
      "output               21.8241 ± 2.2570            3.2845e-04 ± 1.9138e-04\n",
      "xor                  19.0144 ± 1.8339            6.4684e-04 ± 5.5346e-04\n",
      "\n",
      "Overall Aggregated Jacobian Analysis Results:\n",
      "Average Jacobian Norm: 30.6234 ± 13.9616\n",
      "Average Relative Error: 3.1832e-03 ± 4.0968e-02\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Jacobian Computation and Relative Error (GraphSAINT, 100 Samples Per Class) ---\\n\")\n",
    "\n",
    "# --------------------------------\n",
    "# 0) Setup\n",
    "# --------------------------------\n",
    "model.eval()\n",
    "device = graph.ndata['features'].device\n",
    "labels_np = graph.ndata['labels'].cpu().numpy()\n",
    "class_names = list(label_encoder.classes_)\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# --------------------------------\n",
    "# 1) Build selected_test_nodes: up to 100 per class from test_nid (index array)\n",
    "# --------------------------------\n",
    "rng = np.random.default_rng(42)\n",
    "test_indices = test_nid.cpu().numpy()\n",
    "\n",
    "selected_test_nodes = []\n",
    "print(\"Sampling report:\")\n",
    "for c in range(num_classes):\n",
    "    idxs = [int(i) for i in test_indices if labels_np[int(i)] == c]\n",
    "    n_avail = len(idxs)\n",
    "    if n_avail == 0:\n",
    "        print(f\"- {class_names[c]}: 0 available in test set  skipped.\")\n",
    "        continue\n",
    "    if n_avail >= 100:\n",
    "        chosen = rng.choice(idxs, size=100, replace=False)\n",
    "        print(f\"- {class_names[c]}: picked 100 from {n_avail} available.\")\n",
    "    else:\n",
    "        chosen = np.array(idxs, dtype=np.int64)\n",
    "        print(f\"- {class_names[c]}: only {n_avail} available, taking all.\")\n",
    "    selected_test_nodes.extend(chosen.tolist())\n",
    "\n",
    "selected_test_nodes = np.array(selected_test_nodes, dtype=np.int64)\n",
    "\n",
    "# Final per-class counts (selected)\n",
    "final_counts = {class_names[c]: int(np.sum(labels_np[selected_test_nodes] == c)) for c in range(num_classes)}\n",
    "print(\"\\nFinal counts in selected set:\")\n",
    "for name, cnt in final_counts.items():\n",
    "    print(f\"- {name}: {cnt}\")\n",
    "\n",
    "# --------------------------------\n",
    "# 2) Helper: f(x) returns logits for node test_idx with its feature vector replaced by x\n",
    "# --------------------------------\n",
    "def node_logits_with_replaced_features(x, test_idx):\n",
    "    # x: [F] tensor on device\n",
    "    new_features = graph.ndata['features'].clone()\n",
    "    new_features[test_idx] = x\n",
    "    out = model(graph, new_features)\n",
    "    return out[test_idx]  # shape: [num_classes]\n",
    "\n",
    "# --------------------------------\n",
    "# 3) Main loop: Jacobian + finite-difference verification\n",
    "# --------------------------------\n",
    "class_metrics = {cn: {'jacobian_norms': [], 'relative_errors': []} for cn in class_names}\n",
    "overall_jacobian_norms = []\n",
    "overall_relative_errors = []\n",
    "\n",
    "epsilon = 1e-3  # small perturbation for FD check\n",
    "\n",
    "for test_idx in selected_test_nodes:\n",
    "    label_idx = int(graph.ndata['labels'][test_idx].item())\n",
    "    class_name = class_names[label_idx]\n",
    "\n",
    "    # Base feature vector for this node\n",
    "    x0 = graph.ndata['features'][test_idx].clone().detach().to(device).requires_grad_(True)\n",
    "\n",
    "    # Define f(x) for autograd Jacobian (must not use no_grad)\n",
    "    def f(x):\n",
    "        return node_logits_with_replaced_features(x, test_idx)\n",
    "\n",
    "    # Compute Jacobian J (num_classes x feat_dim)\n",
    "    J = torch.autograd.functional.jacobian(f, x0)  # shape: [C, F]\n",
    "\n",
    "    # Frobenius norm of the Jacobian\n",
    "    jacobian_norm = torch.norm(J, p='fro').item()\n",
    "\n",
    "    # Finite-difference verification\n",
    "    delta = epsilon * torch.randn_like(x0)\n",
    "    predicted_change = J.mv(delta)  # [C]\n",
    "    f_x0 = f(x0)\n",
    "    f_x0_perturbed = f(x0 + delta)\n",
    "    actual_change = f_x0_perturbed - f_x0\n",
    "    rel_error = (torch.norm(predicted_change - actual_change) /\n",
    "                 (torch.norm(actual_change) + 1e-8)).item()\n",
    "\n",
    "    # Store metrics\n",
    "    class_metrics[class_name]['jacobian_norms'].append(jacobian_norm)\n",
    "    class_metrics[class_name]['relative_errors'].append(rel_error)\n",
    "    overall_jacobian_norms.append(jacobian_norm)\n",
    "    overall_relative_errors.append(rel_error)\n",
    "\n",
    "# --------------------------------\n",
    "# 4) Reporting\n",
    "# --------------------------------\n",
    "print(\"\\nClass-wise Jacobian Analysis Results:\")\n",
    "header = \"{:<12s} {:>30s} {:>35s}\".format(\"Class\", \"Avg. Jacobian Norm ± Std\", \"Avg. Relative Error ± Std\")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for cn in class_names:\n",
    "    norms = class_metrics[cn]['jacobian_norms']\n",
    "    errors = class_metrics[cn]['relative_errors']\n",
    "    avg_norm = np.mean(norms) if norms else 0.0\n",
    "    std_norm = np.std(norms) if norms else 0.0\n",
    "    avg_rel_error = np.mean(errors) if errors else 0.0\n",
    "    std_rel_error = np.std(errors) if errors else 0.0\n",
    "    print(\"{:<12s} {:>15.4f} ± {:<12.4f} {:>15.4e} ± {:<10.4e}\".format(\n",
    "        cn, avg_norm, std_norm, avg_rel_error, std_rel_error\n",
    "    ))\n",
    "\n",
    "# Overall aggregates\n",
    "overall_avg_norm = float(np.mean(overall_jacobian_norms)) if overall_jacobian_norms else 0.0\n",
    "overall_std_norm = float(np.std(overall_jacobian_norms)) if overall_jacobian_norms else 0.0\n",
    "overall_avg_rel_error = float(np.mean(overall_relative_errors)) if overall_relative_errors else 0.0\n",
    "overall_std_rel_error = float(np.std(overall_relative_errors)) if overall_relative_errors else 0.0\n",
    "\n",
    "print(\"\\nOverall Aggregated Jacobian Analysis Results:\")\n",
    "print(\"Average Jacobian Norm: {:.4f} ± {:.4f}\".format(overall_avg_norm, overall_std_norm))\n",
    "print(\"Average Relative Error: {:.4e} ± {:.4e}\".format(overall_avg_rel_error, overall_std_rel_error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46efec1",
   "metadata": {},
   "source": [
    "#### Local Lipschitz constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf46d7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Local Lipschitz Constant & Relative Error (GraphSAINT, 100 Samples Per Class) ---\n",
      "\n",
      "Sampling report:\n",
      "- and: picked 100 from 5637 available.\n",
      "- input: picked 100 from 475 available.\n",
      "- nand: picked 100 from 527 available.\n",
      "- nor: picked 100 from 567 available.\n",
      "- not: picked 100 from 4478 available.\n",
      "- or: picked 100 from 197 available.\n",
      "- output: picked 100 from 266 available.\n",
      "- xor: only 30 available, taking all.\n",
      "\n",
      "Final counts in selected set:\n",
      "- and: 100\n",
      "- input: 100\n",
      "- nand: 100\n",
      "- nor: 100\n",
      "- not: 100\n",
      "- or: 100\n",
      "- output: 100\n",
      "- xor: 30\n",
      "\n",
      "Class-wise Local Lipschitz (with Relative Errors):\n",
      "Class              Avg Lipschitz ± Std           Avg Rel. Error ± Std\n",
      "---------------------------------------------------------------------\n",
      "and              39.9835 ± 13.4321         3.2278e+00 ± 1.0483e+00\n",
      "input            21.0064 ± 2.9048          2.5257e+00 ± 6.1535e-01\n",
      "nand             15.8666 ± 6.4651          2.2312e+00 ± 6.6427e-01\n",
      "nor              15.5405 ± 2.1582          1.9631e+00 ± 4.9402e-01\n",
      "not              41.9981 ± 7.1122          3.2827e+00 ± 8.7305e-01\n",
      "or               23.4585 ± 16.1533         2.3483e+00 ± 9.2034e-01\n",
      "output           16.4373 ± 2.0249          2.2937e+00 ± 5.0829e-01\n",
      "xor              13.1824 ± 1.4315          1.9854e+00 ± 3.8600e-01\n",
      "\n",
      "Overall Aggregated:\n",
      "Avg Lipschitz Constant: 24.4172 ± 13.6941\n",
      "Avg Relative Error: 2.5299e+00 ± 8.8497e-01\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Local Lipschitz Constant & Relative Error (GraphSAINT, 100 Samples Per Class) ---\\n\")\n",
    "\n",
    "# --------------------------------\n",
    "# 0) Setup\n",
    "# --------------------------------\n",
    "model.eval()\n",
    "class_names = list(label_encoder.classes_)\n",
    "num_classes = len(class_names)\n",
    "labels_np = graph.ndata['labels'].cpu().numpy()\n",
    "\n",
    "# --------------------------------\n",
    "# 1) Build selected_test_nodes: up to 100 per class from test_nid (index array)\n",
    "# --------------------------------\n",
    "rng = np.random.default_rng(42)\n",
    "test_indices = test_nid.cpu().numpy()\n",
    "\n",
    "selected_test_nodes = []\n",
    "print(\"Sampling report:\")\n",
    "for c in range(num_classes):\n",
    "    idxs = [int(i) for i in test_indices if labels_np[int(i)] == c]\n",
    "    n_avail = len(idxs)\n",
    "    if n_avail == 0:\n",
    "        print(f\"- {class_names[c]}: 0 available in test set  skipped.\")\n",
    "        continue\n",
    "    if n_avail >= 100:\n",
    "        chosen = rng.choice(idxs, size=100, replace=False)\n",
    "        print(f\"- {class_names[c]}: picked 100 from {n_avail} available.\")\n",
    "    else:\n",
    "        chosen = np.array(idxs, dtype=np.int64)\n",
    "        print(f\"- {class_names[c]}: only {n_avail} available, taking all.\")\n",
    "    selected_test_nodes.extend(chosen.tolist())\n",
    "\n",
    "selected_test_nodes = np.array(selected_test_nodes, dtype=np.int64)\n",
    "\n",
    "# Final per-class counts (selected)\n",
    "final_counts = {class_names[c]: int(np.sum(labels_np[selected_test_nodes] == c)) for c in range(num_classes)}\n",
    "print(\"\\nFinal counts in selected set:\")\n",
    "for name, cnt in final_counts.items():\n",
    "    print(f\"- {name}: {cnt}\")\n",
    "\n",
    "# --------------------------------\n",
    "# 2) Helper: f(x) returns logits for node test_idx with its feature vector replaced by x\n",
    "# --------------------------------\n",
    "def node_logits_with_replaced_features(x, test_idx):\n",
    "    new_features = graph.ndata['features'].clone()\n",
    "    new_features[test_idx] = x\n",
    "    out = model(graph, new_features)\n",
    "    return out[test_idx]  # [num_classes]\n",
    "\n",
    "# --------------------------------\n",
    "# 3) Main loop: Local Lipschitz (||J||_2) + finite-difference relative error\n",
    "# --------------------------------\n",
    "class_lipschitz = {cn: [] for cn in class_names}\n",
    "class_rel_errors = {cn: [] for cn in class_names}\n",
    "overall_lipschitz = []\n",
    "overall_rel_errors = []\n",
    "\n",
    "epsilon = 1e-3\n",
    "trials_per_node = 10\n",
    "\n",
    "for test_idx in selected_test_nodes:\n",
    "    label_idx = int(graph.ndata['labels'][test_idx].item())\n",
    "    class_name = class_names[label_idx]\n",
    "\n",
    "    # Base feature vector\n",
    "    x0 = graph.ndata['features'][test_idx].clone().detach().requires_grad_(True)\n",
    "\n",
    "    # Define scalar-to-vector function for Jacobian\n",
    "    def f(x):\n",
    "        return node_logits_with_replaced_features(x, test_idx)\n",
    "\n",
    "    # Jacobian J: [C, F]\n",
    "    J = torch.autograd.functional.jacobian(f, x0)\n",
    "\n",
    "    # Local Lipschitz constant: operator 2-norm (largest singular value)\n",
    "    L_local = torch.linalg.norm(J, ord=2).item()\n",
    "\n",
    "    # Finite-difference relative error across multiple random directions\n",
    "    rel_errors_for_node = []\n",
    "    f_x0 = f(x0).detach()\n",
    "    for _ in range(trials_per_node):\n",
    "        delta = epsilon * torch.randn_like(x0)\n",
    "        # Predicted change norm via Lipschitz bound\n",
    "        pred_change_norm = L_local * torch.norm(delta).item()\n",
    "        # Actual change norm\n",
    "        f_x0_pert = f(x0 + delta).detach()\n",
    "        actual_change_norm = torch.norm(f_x0_pert - f_x0).item()\n",
    "        rel_err = abs(pred_change_norm - actual_change_norm) / (actual_change_norm + 1e-8)\n",
    "        rel_errors_for_node.append(rel_err)\n",
    "\n",
    "    avg_rel_error_node = float(np.mean(rel_errors_for_node))\n",
    "\n",
    "    # Store\n",
    "    class_lipschitz[class_name].append(L_local)\n",
    "    class_rel_errors[class_name].append(avg_rel_error_node)\n",
    "    overall_lipschitz.append(L_local)\n",
    "    overall_rel_errors.append(avg_rel_error_node)\n",
    "\n",
    "# --------------------------------\n",
    "# 4) Reporting\n",
    "# --------------------------------\n",
    "print(\"\\nClass-wise Local Lipschitz (with Relative Errors):\")\n",
    "header = \"{:<12s} {:>25s} {:>30s}\".format(\"Class\", \"Avg Lipschitz ± Std\", \"Avg Rel. Error ± Std\")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for cn in class_names:\n",
    "    L_vals = class_lipschitz[cn]\n",
    "    E_vals = class_rel_errors[cn]\n",
    "    if L_vals:\n",
    "        print(\"{:<12s} {:>11.4f} ± {:<10.4f} {:>15.4e} ± {:<10.4e}\".format(\n",
    "            cn, np.mean(L_vals), np.std(L_vals), np.mean(E_vals), np.std(E_vals)\n",
    "        ))\n",
    "    else:\n",
    "        print(\"{:<12s} {:>11s} {:<10s} {:>15s} {:<10s}\".format(cn, \"-\", \"-\", \"-\", \"-\"))\n",
    "\n",
    "if overall_lipschitz:\n",
    "    print(\"\\nOverall Aggregated:\")\n",
    "    print(\"Avg Lipschitz Constant: {:.4f} ± {:.4f}\".format(np.mean(overall_lipschitz), np.std(overall_lipschitz)))\n",
    "    print(\"Avg Relative Error: {:.4e} ± {:.4e}\".format(np.mean(overall_rel_errors), np.std(overall_rel_errors)))\n",
    "else:\n",
    "    print(\"\\nOverall Aggregated:\")\n",
    "    print(\"No samples selected; overall metrics unavailable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b5349e",
   "metadata": {},
   "source": [
    "#### Hessian-Based Curvature Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2de8353f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fast Hessian-Based Curvature (Grad Outer-Product, double precision) ---\n",
      "\n",
      "Sampling report:\n",
      "- and: picked 100 from 5637 available.\n",
      "- input: picked 100 from 475 available.\n",
      "- nand: picked 100 from 527 available.\n",
      "- nor: picked 100 from 567 available.\n",
      "- not: picked 100 from 4478 available.\n",
      "- or: picked 100 from 197 available.\n",
      "- output: picked 100 from 266 available.\n",
      "- xor: only 30 available, taking all.\n",
      "\n",
      "Final counts in selected set:\n",
      "- and: 100\n",
      "- input: 100\n",
      "- nand: 100\n",
      "- nor: 100\n",
      "- not: 100\n",
      "- or: 100\n",
      "- output: 100\n",
      "- xor: 30\n",
      "\n",
      "Class-wise Hessian-Based Curvature (Grad Outer-Product) with Relative Errors:\n",
      "Class                Avg. Max Eigenvalue ± Std           Avg Rel. Error ± Std\n",
      "-----------------------------------------------------------------------------\n",
      "and             2.209153 ± 8.051845        1.0515e+00 ± 1.3327e-01\n",
      "input           0.000000 ± 0.000000        8.4607e-01 ± 2.3893e-01\n",
      "nand           14.297103 ± 33.644669       1.2095e+00 ± 2.2113e-01\n",
      "nor             2.889931 ± 9.089566        1.0882e+00 ± 2.0553e-01\n",
      "not             0.000242 ± 0.002378        9.8871e-01 ± 2.9023e-02\n",
      "or             21.185191 ± 49.239007       1.2506e+00 ± 2.3182e-01\n",
      "output          0.000000 ± 0.000000        9.9722e-01 ± 1.3112e-02\n",
      "xor             0.000001 ± 0.000006        9.1061e-01 ± 1.2802e-01\n",
      "\n",
      "Overall Aggregated Hessian-Based Curvature (Grad Outer-Product):\n",
      "Avg Max Eigenvalue: 5.559126 ± 23.828006\n",
      "Avg Relative Error: 1.0555e+00 ± 2.1787e-01\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import dgl\n",
    "\n",
    "print(\"\\n--- Fast Hessian-Based Curvature (Grad Outer-Product, double precision) ---\\n\")\n",
    "\n",
    "# -------------------------------\n",
    "# 0) Config\n",
    "# -------------------------------\n",
    "max_per_class = 100\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "epsilon_fd = 5e-3         # step for finite-difference check\n",
    "trials_per_node = 3       # reduce trials for speed\n",
    "power_iters = 8           # kept for compatibility (unused here)\n",
    "khop_enable = True\n",
    "khop_K = 2                # for 2-layer GNNs, K=2 covers dependency\n",
    "dtype_eval = torch.float64\n",
    "\n",
    "# -------------------------------\n",
    "# 1) Setup\n",
    "# -------------------------------\n",
    "model.eval()\n",
    "\n",
    "class_names = list(label_encoder.classes_)\n",
    "num_classes = len(class_names)\n",
    "labels_np = graph.ndata['labels'].cpu().numpy()\n",
    "\n",
    "device = graph.ndata['features'].device\n",
    "orig_dtype = next(model.parameters()).dtype if any(p.requires_grad for p in model.parameters()) else torch.float32\n",
    "\n",
    "# Cast model and features to double for stable curvature; restore later\n",
    "model = model.to(dtype=dtype_eval, device=device)\n",
    "orig_feats = graph.ndata['features']\n",
    "graph.ndata['features'] = graph.ndata['features'].to(dtype_eval)\n",
    "\n",
    "# Precompute logits once (full graph) for predicted class\n",
    "with torch.no_grad():\n",
    "    base_logits_full = model(graph, graph.ndata['features']).detach()\n",
    "\n",
    "# -------------------------------\n",
    "# 2) Select up to max_per_class test nodes per class\n",
    "# -------------------------------\n",
    "test_indices = test_nid.cpu().numpy()\n",
    "selected = []\n",
    "print(\"Sampling report:\")\n",
    "for c in range(num_classes):\n",
    "    idxs = [int(i) for i in test_indices if labels_np[int(i)] == c]\n",
    "    if len(idxs) == 0:\n",
    "        print(f\"- {class_names[c]}: 0 available in test set  skipped.\")\n",
    "        continue\n",
    "    if len(idxs) > max_per_class:\n",
    "        chosen = rng.choice(idxs, size=max_per_class, replace=False).tolist()\n",
    "        print(f\"- {class_names[c]}: picked {max_per_class} from {len(idxs)} available.\")\n",
    "    else:\n",
    "        chosen = idxs\n",
    "        print(f\"- {class_names[c]}: only {len(idxs)} available, taking all.\")\n",
    "    selected.extend(chosen)\n",
    "\n",
    "selected = np.array(selected, dtype=np.int64)\n",
    "\n",
    "print(\"\\nFinal counts in selected set:\")\n",
    "for c in range(num_classes):\n",
    "    cnt = int(np.sum(labels_np[selected] == c))\n",
    "    print(f\"- {class_names[c]}: {cnt}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3) Caching k-hop subgraphs\n",
    "# -------------------------------\n",
    "subgraph_cache = {}\n",
    "\n",
    "def get_local_subgraph(g, node_idx, K):\n",
    "    if not khop_enable:\n",
    "        return g, torch.arange(g.num_nodes(), device=device), int(node_idx)\n",
    "    key = (int(node_idx), K)\n",
    "    if key in subgraph_cache:\n",
    "        return subgraph_cache[key]\n",
    "    \n",
    "    subg, nids = dgl.khop_in_subgraph(g, int(node_idx), k=K)\n",
    "    # Ensure target node is present; if not, fall back to full graph\n",
    "    match_idx = (nids == int(node_idx)).nonzero(as_tuple=False)\n",
    "    if match_idx.numel() == 0:\n",
    "        # Fallback: use full graph to avoid crash\n",
    "        subgraph_cache[key] = (g, torch.arange(g.num_nodes(), device=device), int(node_idx))\n",
    "        return subgraph_cache[key]\n",
    "    \n",
    "    local_target = match_idx.view(-1).item()\n",
    "    subg = subg.to(device)\n",
    "    nids = nids.to(device)\n",
    "    subgraph_cache[key] = (subg, nids, local_target)\n",
    "    return subgraph_cache[key]\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 4) h(x) and grad-outer-product curvature\n",
    "# -------------------------------\n",
    "def h_scalar(subg, x_local, local_to_global_nids, local_target, pred_class):\n",
    "    \"\"\"\n",
    "    h(x) = log prob of 'pred_class' for the target node.\n",
    "    Build feats as a differentiable function of x_local (no in-place assign).\n",
    "    \"\"\"\n",
    "    feats0 = graph.ndata['features'][local_to_global_nids].to(dtype=x_local.dtype, device=x_local.device)\n",
    "    delta = x_local - feats0[local_target]\n",
    "    selector = torch.zeros(feats0.size(0), dtype=x_local.dtype, device=x_local.device)\n",
    "    selector[local_target] = 1.0\n",
    "    feats = feats0 + selector.unsqueeze(1) * delta.unsqueeze(0)\n",
    "\n",
    "    logits = model(subg, feats)[local_target]\n",
    "    logits = logits.squeeze(0)  # handles (C,) or (1,C)\n",
    "    return torch.log_softmax(logits, dim=-1)[pred_class]\n",
    "\n",
    "def grad_outer_prod_lambda_max(subg, x0, local_to_global_nids, local_target, pred_class):\n",
    "    \"\"\"\n",
    "    Approximate largest Hessian eigenvalue by squared L2 norm of gradient:\n",
    "    lambda_max  ||?_x h(x0)||²\n",
    "    \"\"\"\n",
    "    x0_req = x0.detach().clone().requires_grad_(True)\n",
    "    y = h_scalar(subg, x0_req, local_to_global_nids, local_target, pred_class)\n",
    "    grad = torch.autograd.grad(y, x0_req, retain_graph=False, create_graph=False)[0]\n",
    "    return grad.norm(p=2).item() ** 2, grad.detach(), y.detach()\n",
    "\n",
    "# -------------------------------\n",
    "# 5) Main loop: curvature + relative error (using GaussNewton approx)\n",
    "# -------------------------------\n",
    "class_curv = {cn: [] for cn in class_names}\n",
    "class_relerr = {cn: [] for cn in class_names}\n",
    "overall_curv, overall_rel = [], []\n",
    "\n",
    "for test_idx in selected:\n",
    "    label_idx = int(labels_np[int(test_idx)])\n",
    "    class_name = class_names[label_idx]\n",
    "\n",
    "    # Predicted class from precomputed logits\n",
    "    pred_class = int(torch.argmax(base_logits_full[int(test_idx)]).item())\n",
    "\n",
    "    # Local subgraph\n",
    "    subg, local_to_global, local_target = get_local_subgraph(graph, int(test_idx), khop_K)\n",
    "\n",
    "    # Local feature vector\n",
    "    x0 = graph.ndata['features'][local_to_global[local_target]].detach().clone().to(dtype_eval).to(device)\n",
    "\n",
    "    # Curvature via gradient outer-product: lambda_max  ||grad||^2\n",
    "    lambda_max, g, y0_val = grad_outer_prod_lambda_max(subg, x0, local_to_global, local_target, pred_class)\n",
    "\n",
    "    # Relative error of quadratic approximation using H  g g^T:\n",
    "    # predicted second-order term: 0.5 * (g^T delta)^2\n",
    "    errs = []\n",
    "    for _ in range(trials_per_node):\n",
    "        delta = epsilon_fd * torch.randn_like(x0)\n",
    "        # Predicted second-order term under rank-1 Hessian approx\n",
    "        gt_delta = torch.dot(g, delta).item()\n",
    "        pred_second = 0.5 * (gt_delta ** 2)\n",
    "\n",
    "        # Actual second-order remainder from Taylor expansion\n",
    "        actual_second = (h_scalar(subg, x0 + delta, local_to_global, local_target, pred_class) - y0_val - torch.dot(g, delta)).item()\n",
    "        rel = abs(pred_second - actual_second) / (abs(actual_second) + 1e-12)\n",
    "        errs.append(rel)\n",
    "\n",
    "    class_curv[class_name].append(lambda_max)\n",
    "    class_relerr[class_name].append(float(np.mean(errs)))\n",
    "    overall_curv.append(lambda_max)\n",
    "    overall_rel.append(float(np.mean(errs)))\n",
    "\n",
    "# -------------------------------\n",
    "# 6) Reporting\n",
    "# -------------------------------\n",
    "print(\"\\nClass-wise Hessian-Based Curvature (Grad Outer-Product) with Relative Errors:\")\n",
    "header = \"{:<12s} {:>33s} {:>30s}\".format(\"Class\", \"Avg. Max Eigenvalue ± Std\", \"Avg Rel. Error ± Std\")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "for cn in class_names:\n",
    "    vals = class_curv[cn]\n",
    "    errs = class_relerr[cn]\n",
    "    if vals:\n",
    "        print(\"{:<12s} {:>11.6f} ± {:<10.6f} {:>15.4e} ± {:<10.4e}\".format(\n",
    "            cn, float(np.mean(vals)), float(np.std(vals)),\n",
    "            float(np.mean(errs)), float(np.std(errs))\n",
    "        ))\n",
    "    else:\n",
    "        print(\"{:<12s} {:>11s} {:<10s} {:>15s} {:<10s}\".format(cn, \"-\", \"-\", \"-\", \"-\"))\n",
    "\n",
    "if overall_curv:\n",
    "    print(\"\\nOverall Aggregated Hessian-Based Curvature (Grad Outer-Product):\")\n",
    "    print(\"Avg Max Eigenvalue: {:.6f} ± {:.6f}\".format(float(np.mean(overall_curv)), float(np.std(overall_curv))))\n",
    "    print(\"Avg Relative Error: {:.4e} ± {:.4e}\".format(float(np.mean(overall_rel)), float(np.std(overall_rel))))\n",
    "else:\n",
    "    print(\"\\nOverall Aggregated Hessian-Based Curvature (Grad Outer-Product):\")\n",
    "    print(\"No samples selected; overall metrics unavailable.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7) Cleanup: restore original types\n",
    "# -------------------------------\n",
    "graph.ndata['features'] = orig_feats\n",
    "model = model.to(dtype=orig_dtype, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4debe1da",
   "metadata": {},
   "source": [
    "#### Prediction Margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd7ab3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction Margin & Relative Error (GraphSAINT, 100 Samples Per Class) ---\n",
      "\n",
      "Sampling report:\n",
      "- and: picked 100 from 5637 available.\n",
      "- input: picked 100 from 475 available.\n",
      "- nand: picked 100 from 527 available.\n",
      "- nor: picked 100 from 567 available.\n",
      "- not: picked 100 from 4478 available.\n",
      "- or: picked 100 from 197 available.\n",
      "- output: picked 100 from 266 available.\n",
      "- xor: only 30 available, taking all.\n",
      "\n",
      "Final counts in selected set:\n",
      "- and: 100\n",
      "- input: 100\n",
      "- nand: 100\n",
      "- nor: 100\n",
      "- not: 100\n",
      "- or: 100\n",
      "- output: 100\n",
      "- xor: 30\n",
      "\n",
      "Class-wise Prediction Margin (with Relative Error):\n",
      "Class                        Avg. Margin ± Std           Avg Rel. Error ± Std\n",
      "-----------------------------------------------------------------------------\n",
      "and               9.1655 ± 4.9898          3.8058e-05 ± 9.7565e-05\n",
      "input            16.2503 ± 2.4308          4.6727e-06 ± 4.0869e-06\n",
      "nand              2.5752 ± 2.7748          1.4034e-04 ± 3.0827e-04\n",
      "nor               4.9486 ± 2.4715          4.2161e-05 ± 6.7022e-05\n",
      "not              14.5761 ± 2.6303          1.4061e-05 ± 9.9351e-06\n",
      "or                1.4596 ± 1.1554          7.2181e-04 ± 6.0877e-03\n",
      "output           10.2581 ± 1.6700          3.7706e-06 ± 3.4583e-06\n",
      "xor              13.9153 ± 2.5236          3.7185e-06 ± 4.5325e-06\n",
      "\n",
      "Overall Aggregated Prediction Margin:\n",
      "Avg Margin: 8.6860 ± 6.0127\n",
      "Avg Relative Error: 1.3233e-04 ± 2.2691e-03\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Prediction Margin & Relative Error (GraphSAINT, 100 Samples Per Class) ---\\n\")\n",
    "\n",
    "# --------------------------------\n",
    "# 0) Setup\n",
    "# --------------------------------\n",
    "model.eval()\n",
    "class_names = list(label_encoder.classes_)\n",
    "num_classes = len(class_names)\n",
    "labels_np = graph.ndata['labels'].cpu().numpy()\n",
    "\n",
    "# --------------------------------\n",
    "# 1) Build selected_test_nodes: up to 100 per class from test_nid\n",
    "# --------------------------------\n",
    "rng = np.random.default_rng(42)\n",
    "test_indices = test_nid.cpu().numpy()\n",
    "\n",
    "selected_test_nodes = []\n",
    "print(\"Sampling report:\")\n",
    "for c in range(num_classes):\n",
    "    idxs = [int(i) for i in test_indices if labels_np[int(i)] == c]\n",
    "    n_avail = len(idxs)\n",
    "    if n_avail == 0:\n",
    "        print(f\"- {class_names[c]}: 0 available in test set  skipped.\")\n",
    "        continue\n",
    "    if n_avail >= 100:\n",
    "        chosen = rng.choice(idxs, size=100, replace=False)\n",
    "        print(f\"- {class_names[c]}: picked 100 from {n_avail} available.\")\n",
    "    else:\n",
    "        chosen = np.array(idxs, dtype=np.int64)\n",
    "        print(f\"- {class_names[c]}: only {n_avail} available, taking all.\")\n",
    "    selected_test_nodes.extend(chosen.tolist())\n",
    "\n",
    "selected_test_nodes = np.array(selected_test_nodes, dtype=np.int64)\n",
    "\n",
    "# Final per-class counts\n",
    "final_counts = {class_names[c]: int(np.sum(labels_np[selected_test_nodes] == c)) for c in range(num_classes)}\n",
    "print(\"\\nFinal counts in selected set:\")\n",
    "for name, cnt in final_counts.items():\n",
    "    print(f\"- {name}: {cnt}\")\n",
    "\n",
    "# --------------------------------\n",
    "# 2) Main loop: Prediction margin + relative error\n",
    "# --------------------------------\n",
    "class_margin_vals = {cn: [] for cn in class_names}\n",
    "class_rel_errors  = {cn: [] for cn in class_names}\n",
    "overall_margin_vals = []\n",
    "overall_rel_errors  = []\n",
    "\n",
    "epsilon = 1e-5  # small perturbation for verification\n",
    "\n",
    "for test_idx in selected_test_nodes:\n",
    "    label_idx = int(graph.ndata['labels'][test_idx].item())\n",
    "    class_name = class_names[label_idx]\n",
    "\n",
    "    # Original logits\n",
    "    with torch.no_grad():\n",
    "        logits = model(graph, graph.ndata['features'])[test_idx]\n",
    "\n",
    "    pred_class = int(torch.argmax(logits))\n",
    "    pred_logit = logits[pred_class].item()\n",
    "\n",
    "    # Second max logit\n",
    "    other_logits = logits.clone()\n",
    "    other_logits[pred_class] = -float('inf')\n",
    "    second_max = other_logits.max().item()\n",
    "\n",
    "    margin = pred_logit - second_max\n",
    "\n",
    "    # Relative error verification\n",
    "    perturbed_feats = graph.ndata['features'].clone()\n",
    "    perturb_vec = torch.randn_like(perturbed_feats[test_idx]) * epsilon\n",
    "    perturbed_feats[test_idx] += perturb_vec\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits_pert = model(graph, perturbed_feats)[test_idx]\n",
    "    pred_logit_pert = logits_pert[pred_class].item()\n",
    "    other_logits_pert = logits_pert.clone()\n",
    "    other_logits_pert[pred_class] = -float('inf')\n",
    "    second_max_pert = other_logits_pert.max().item()\n",
    "    margin_pert = pred_logit_pert - second_max_pert\n",
    "\n",
    "    rel_err = abs(margin - margin_pert) / (abs(margin_pert) + 1e-12)\n",
    "\n",
    "    # Store\n",
    "    class_margin_vals[class_name].append(margin)\n",
    "    class_rel_errors[class_name].append(rel_err)\n",
    "    overall_margin_vals.append(margin)\n",
    "    overall_rel_errors.append(rel_err)\n",
    "\n",
    "# --------------------------------\n",
    "# 3) Reporting\n",
    "# --------------------------------\n",
    "print(\"\\nClass-wise Prediction Margin (with Relative Error):\")\n",
    "header = \"{:<12s} {:>33s} {:>30s}\".format(\"Class\", \"Avg. Margin ± Std\", \"Avg Rel. Error ± Std\")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for cn in class_names:\n",
    "    m_vals = class_margin_vals[cn]\n",
    "    e_vals = class_rel_errors[cn]\n",
    "    if m_vals:\n",
    "        print(\"{:<12s} {:>11.4f} ± {:<10.4f} {:>15.4e} ± {:<10.4e}\".format(\n",
    "            cn, np.mean(m_vals), np.std(m_vals),\n",
    "            np.mean(e_vals), np.std(e_vals)\n",
    "        ))\n",
    "    else:\n",
    "        print(\"{:<12s} {:>11s} {:<10s} {:>15s} {:<10s}\".format(cn, \"-\", \"-\", \"-\", \"-\"))\n",
    "\n",
    "if overall_margin_vals:\n",
    "    print(\"\\nOverall Aggregated Prediction Margin:\")\n",
    "    print(\"Avg Margin: {:.4f} ± {:.4f}\".format(np.mean(overall_margin_vals), np.std(overall_margin_vals)))\n",
    "    print(\"Avg Relative Error: {:.4e} ± {:.4e}\".format(np.mean(overall_rel_errors), np.std(overall_rel_errors)))\n",
    "else:\n",
    "    print(\"\\nOverall Aggregated Prediction Margin:\")\n",
    "    print(\"No samples selected; overall metrics unavailable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a96ce7",
   "metadata": {},
   "source": [
    "#### Adversarial Robustness Radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2175c8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Adversarial Robustness Radius & Relative Error (GraphSAINT, 100 Samples Per Class) ---\n",
      "\n",
      "Sampling report:\n",
      "- and: picked 100 from 5637 available.\n",
      "- input: picked 100 from 475 available.\n",
      "- nand: picked 100 from 527 available.\n",
      "- nor: picked 100 from 567 available.\n",
      "- not: picked 100 from 4478 available.\n",
      "- or: picked 100 from 197 available.\n",
      "- output: picked 100 from 266 available.\n",
      "- xor: only 30 available, taking all.\n",
      "\n",
      "Final counts in selected set:\n",
      "- and: 100\n",
      "- input: 100\n",
      "- nand: 100\n",
      "- nor: 100\n",
      "- not: 100\n",
      "- or: 100\n",
      "- output: 100\n",
      "- xor: 30\n",
      "\n",
      "Class-wise Adversarial Robustness Radius (with Relative Error):\n",
      "Class                  Avg. Radius ± Std           Avg Rel. Error ± Std\n",
      "-----------------------------------------------------------------------\n",
      "and               1.1797 ± 0.7354          4.9843e-01 ± 1.0521e+00\n",
      "input             3.6205 ± 1.0731          2.3660e-01 ± 1.8090e-01\n",
      "nand              0.5220 ± 0.8734          4.5337e-01 ± 5.1204e-01\n",
      "nor               0.6590 ± 0.3118          2.9343e-01 ± 2.6010e-01\n",
      "not               1.7089 ± 0.8135          4.3420e-01 ± 4.7669e-01\n",
      "or                0.3894 ± 0.3780          4.5631e-01 ± 5.3906e-01\n",
      "output            2.8391 ± 0.6949          2.1045e-01 ± 1.6622e-01\n",
      "xor               4.2825 ± 1.9205          3.8717e-01 ± 3.6657e-01\n",
      "\n",
      "Overall Aggregated Adversarial Robustness Radius:\n",
      "Avg Radius: 1.6717 ± 1.4983\n",
      "Avg Relative Error: 3.6972e-01 ± 5.4136e-01\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Adversarial Robustness Radius & Relative Error (GraphSAINT, 100 Samples Per Class) ---\\n\")\n",
    "\n",
    "# --------------------------------\n",
    "# 0) Setup\n",
    "# --------------------------------\n",
    "model.eval()\n",
    "class_names = list(label_encoder.classes_)\n",
    "num_classes = len(class_names)\n",
    "labels_np = graph.ndata['labels'].cpu().numpy()\n",
    "\n",
    "# --------------------------------\n",
    "# 1) Build selected_test_nodes: up to 100 per class from test_nid\n",
    "# --------------------------------\n",
    "rng = np.random.default_rng(42)\n",
    "test_indices = test_nid.cpu().numpy()\n",
    "\n",
    "selected_test_nodes = []\n",
    "print(\"Sampling report:\")\n",
    "for c in range(num_classes):\n",
    "    idxs = [int(i) for i in test_indices if labels_np[int(i)] == c]\n",
    "    n_avail = len(idxs)\n",
    "    if n_avail == 0:\n",
    "        print(f\"- {class_names[c]}: 0 available in test set  skipped.\")\n",
    "        continue\n",
    "    if n_avail >= 100:\n",
    "        chosen = rng.choice(idxs, size=100, replace=False)\n",
    "        print(f\"- {class_names[c]}: picked 100 from {n_avail} available.\")\n",
    "    else:\n",
    "        chosen = np.array(idxs, dtype=np.int64)\n",
    "        print(f\"- {class_names[c]}: only {n_avail} available, taking all.\")\n",
    "    selected_test_nodes.extend(chosen.tolist())\n",
    "\n",
    "selected_test_nodes = np.array(selected_test_nodes, dtype=np.int64)\n",
    "\n",
    "# Final per-class counts\n",
    "final_counts = {class_names[c]: int(np.sum(labels_np[selected_test_nodes] == c)) for c in range(num_classes)}\n",
    "print(\"\\nFinal counts in selected set:\")\n",
    "for name, cnt in final_counts.items():\n",
    "    print(f\"- {name}: {cnt}\")\n",
    "\n",
    "# --------------------------------\n",
    "# 2) Prediction wrapper for a node (replace its feature vector with x)\n",
    "# --------------------------------\n",
    "def f_for_sample(x, test_idx):\n",
    "    \"\"\"\n",
    "    Given an input x (modified feature vector for the test node),\n",
    "    returns the output (logit vector) for that node.\n",
    "    \"\"\"\n",
    "    new_features = graph.ndata['features'].clone().detach()\n",
    "    new_features[test_idx] = x\n",
    "    with torch.no_grad():\n",
    "        out = model(graph, new_features)\n",
    "    return out[test_idx]\n",
    "\n",
    "# --------------------------------\n",
    "# 3) Adversarial radius along random directions + binary search\n",
    "# --------------------------------\n",
    "def adversarial_radius_for_sample(\n",
    "    test_idx,\n",
    "    initial_epsilon=1e-3,\n",
    "    growth_factor=1.2,\n",
    "    max_epsilon=10.0,\n",
    "    bs_iters=10,\n",
    "    num_trials=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Minimal perturbation norm required to change the prediction, estimated by:\n",
    "      - Sampling random directions d (||d||=1)\n",
    "      - Expanding epsilon multiplicatively until the prediction flips (or cap)\n",
    "      - Binary-searching in the bracket to refine the boundary\n",
    "    Returns the minimum over 'num_trials' sampled directions.\n",
    "    \"\"\"\n",
    "    x0 = graph.ndata['features'][test_idx].clone().detach()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        base_out = model(graph, graph.ndata['features'])\n",
    "        y0 = int(torch.argmax(base_out[test_idx]).item())\n",
    "\n",
    "    def is_same(x):\n",
    "        out = f_for_sample(x, test_idx)\n",
    "        return int(torch.argmax(out).item()) == y0\n",
    "\n",
    "    radii = []\n",
    "    for _ in range(num_trials):\n",
    "        d = torch.randn_like(x0)\n",
    "        d = d / (torch.norm(d) + 1e-12)\n",
    "\n",
    "        epsilon = initial_epsilon\n",
    "        # Expand until flip or cap\n",
    "        while epsilon < max_epsilon and is_same(x0 + epsilon * d):\n",
    "            epsilon *= growth_factor\n",
    "\n",
    "        if epsilon >= max_epsilon:\n",
    "            candidate = max_epsilon\n",
    "        else:\n",
    "            # Binary search in [epsilon/growth_factor, epsilon]\n",
    "            low = epsilon / growth_factor\n",
    "            high = epsilon\n",
    "            for _ in range(bs_iters):\n",
    "                mid = 0.5 * (low + high)\n",
    "                if is_same(x0 + mid * d):\n",
    "                    low = mid\n",
    "                else:\n",
    "                    high = mid\n",
    "            candidate = high\n",
    "        radii.append(candidate)\n",
    "\n",
    "    return float(min(radii))\n",
    "\n",
    "# --------------------------------\n",
    "# 4) Relative error verification (re-estimate with different config)\n",
    "# --------------------------------\n",
    "def adversarial_radius_relerr(test_idx):\n",
    "    r1 = adversarial_radius_for_sample(\n",
    "        test_idx,\n",
    "        initial_epsilon=1e-3,\n",
    "        growth_factor=1.2,\n",
    "        max_epsilon=10.0,\n",
    "        bs_iters=10,\n",
    "        num_trials=10,\n",
    "    )\n",
    "    r2 = adversarial_radius_for_sample(\n",
    "        test_idx,\n",
    "        initial_epsilon=1e-3,\n",
    "        growth_factor=1.3,  # different growth factor / search iters\n",
    "        max_epsilon=10.0,\n",
    "        bs_iters=12,\n",
    "        num_trials=10,\n",
    "    )\n",
    "    rel_err = abs(r1 - r2) / (abs(r2) + 1e-12)\n",
    "    return r1, rel_err\n",
    "\n",
    "# --------------------------------\n",
    "# 5) Main loop: compute ARR + relative error per class\n",
    "# --------------------------------\n",
    "class_adv_radius = {cn: [] for cn in class_names}\n",
    "class_rel_errors  = {cn: [] for cn in class_names}\n",
    "overall_adv_radius_vals = []\n",
    "overall_rel_errors      = []\n",
    "\n",
    "for test_idx in selected_test_nodes:\n",
    "    label_idx = int(graph.ndata['labels'][test_idx].item())\n",
    "    class_name = class_names[label_idx]\n",
    "\n",
    "    radius, rel_err = adversarial_radius_relerr(test_idx)\n",
    "\n",
    "    class_adv_radius[class_name].append(radius)\n",
    "    class_rel_errors[class_name].append(rel_err)\n",
    "    overall_adv_radius_vals.append(radius)\n",
    "    overall_rel_errors.append(rel_err)\n",
    "\n",
    "# --------------------------------\n",
    "# 6) Reporting\n",
    "# --------------------------------\n",
    "print(\"\\nClass-wise Adversarial Robustness Radius (with Relative Error):\")\n",
    "header = \"{:<12s} {:>27s} {:>30s}\".format(\"Class\", \"Avg. Radius ± Std\", \"Avg Rel. Error ± Std\")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for cn in class_names:\n",
    "    r_vals = class_adv_radius[cn]\n",
    "    e_vals = class_rel_errors[cn]\n",
    "    if r_vals:\n",
    "        print(\"{:<12s} {:>11.4f} ± {:<10.4f} {:>15.4e} ± {:<10.4e}\".format(\n",
    "            cn, np.mean(r_vals), np.std(r_vals),\n",
    "            np.mean(e_vals), np.std(e_vals)\n",
    "        ))\n",
    "    else:\n",
    "        print(\"{:<12s} {:>11s} {:<10s} {:>15s} {:<10s}\".format(cn, \"-\", \"-\", \"-\", \"-\"))\n",
    "\n",
    "if overall_adv_radius_vals:\n",
    "    print(\"\\nOverall Aggregated Adversarial Robustness Radius:\")\n",
    "    print(\"Avg Radius: {:.4f} ± {:.4f}\".format(np.mean(overall_adv_radius_vals), np.std(overall_adv_radius_vals)))\n",
    "    print(\"Avg Relative Error: {:.4e} ± {:.4e}\".format(np.mean(overall_rel_errors), np.std(overall_rel_errors)))\n",
    "else:\n",
    "    print(\"\\nOverall Aggregated Adversarial Robustness Radius:\")\n",
    "    print(\"No samples selected; overall metrics unavailable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00574a57",
   "metadata": {},
   "source": [
    "#### Stability Under Input Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "351dab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stability Under Input Noise & Relative Error (GraphSAINT, 100 Samples Per Class) ---\n",
      "\n",
      "Sampling report:\n",
      "- and: picked 100 from 5637 available.\n",
      "- input: picked 100 from 475 available.\n",
      "- nand: picked 100 from 527 available.\n",
      "- nor: picked 100 from 567 available.\n",
      "- not: picked 100 from 4478 available.\n",
      "- or: picked 100 from 197 available.\n",
      "- output: picked 100 from 266 available.\n",
      "- xor: only 30 available, taking all.\n",
      "\n",
      "Final counts in selected set:\n",
      "- and: 100\n",
      "- input: 100\n",
      "- nand: 100\n",
      "- nor: 100\n",
      "- not: 100\n",
      "- or: 100\n",
      "- output: 100\n",
      "- xor: 30\n",
      "\n",
      "Class-wise Stability Under Input Noise (with Relative Error):\n",
      "Class                     Avg. Stability ± Std           Avg Rel. Error ± Std\n",
      "-----------------------------------------------------------------------------\n",
      "and               0.4084 ± 0.1270          1.0617e-01 ± 7.4555e-02\n",
      "input             0.2531 ± 0.0427          8.4543e-02 ± 6.3901e-02\n",
      "nand              0.2225 ± 0.0744          8.5895e-02 ± 6.7289e-02\n",
      "nor               0.2112 ± 0.0391          7.3242e-02 ± 5.2835e-02\n",
      "not               0.4227 ± 0.0721          8.1569e-02 ± 6.4556e-02\n",
      "or                0.2650 ± 0.1560          8.9851e-02 ± 7.0191e-02\n",
      "output            0.1992 ± 0.0281          8.1296e-02 ± 5.9060e-02\n",
      "xor               0.1732 ± 0.0200          7.6196e-02 ± 5.3719e-02\n",
      "\n",
      "Overall Aggregated Stability Under Input Noise:\n",
      "Avg Stability: 0.2787 ± 0.1234\n",
      "Avg Relative Error: 8.5675e-02 ± 6.5235e-02\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Stability Under Input Noise & Relative Error (GraphSAINT, 100 Samples Per Class) ---\\n\")\n",
    "\n",
    "# --------------------------------\n",
    "# 0) Setup\n",
    "# --------------------------------\n",
    "model.eval()\n",
    "class_names = list(label_encoder.classes_)\n",
    "num_classes = len(class_names)\n",
    "labels_np = graph.ndata['labels'].cpu().numpy()\n",
    "\n",
    "# --------------------------------\n",
    "# 1) Build selected_test_nodes: up to 100 per class from test_nid\n",
    "# --------------------------------\n",
    "rng = np.random.default_rng(42)\n",
    "test_indices = test_nid.cpu().numpy()\n",
    "\n",
    "selected_test_nodes = []\n",
    "print(\"Sampling report:\")\n",
    "for c in range(num_classes):\n",
    "    idxs = [int(i) for i in test_indices if labels_np[int(i)] == c]\n",
    "    n_avail = len(idxs)\n",
    "    if n_avail == 0:\n",
    "        print(f\"- {class_names[c]}: 0 available in test set  skipped.\")\n",
    "        continue\n",
    "    if n_avail >= 100:\n",
    "        chosen = rng.choice(idxs, size=100, replace=False)\n",
    "        print(f\"- {class_names[c]}: picked 100 from {n_avail} available.\")\n",
    "    else:\n",
    "        chosen = np.array(idxs, dtype=np.int64)\n",
    "        print(f\"- {class_names[c]}: only {n_avail} available, taking all.\")\n",
    "    selected_test_nodes.extend(chosen.tolist())\n",
    "\n",
    "selected_test_nodes = np.array(selected_test_nodes, dtype=np.int64)\n",
    "\n",
    "# Final per-class counts\n",
    "final_counts = {class_names[c]: int(np.sum(labels_np[selected_test_nodes] == c)) for c in range(num_classes)}\n",
    "print(\"\\nFinal counts in selected set:\")\n",
    "for name, cnt in final_counts.items():\n",
    "    print(f\"- {name}: {cnt}\")\n",
    "\n",
    "# --------------------------------\n",
    "# 2) Helper: stability under Gaussian noise\n",
    "# --------------------------------\n",
    "def stability_for_sample(test_idx, sigma, num_samples):\n",
    "    \"\"\"\n",
    "    Average L2 change in logits between clean and noisy versions of the node.\n",
    "    \"\"\"\n",
    "    x0 = graph.ndata['features'][test_idx].clone().detach()\n",
    "    with torch.no_grad():\n",
    "        f_orig = model(graph, graph.ndata['features'])[test_idx]\n",
    "\n",
    "    diffs = []\n",
    "    for _ in range(num_samples):\n",
    "        noise = sigma * torch.randn_like(x0)\n",
    "        x_noisy = x0 + noise\n",
    "        new_feats = graph.ndata['features'].clone().detach()\n",
    "        new_feats[test_idx] = x_noisy\n",
    "        with torch.no_grad():\n",
    "            f_noisy = model(graph, new_feats)[test_idx]\n",
    "        diffs.append(torch.norm(f_noisy - f_orig).item())\n",
    "    return float(np.mean(diffs))\n",
    "\n",
    "# --------------------------------\n",
    "# 3) Main loop: stability + relative error\n",
    "# --------------------------------\n",
    "class_stability = {cn: [] for cn in class_names}\n",
    "class_rel_errors = {cn: [] for cn in class_names}\n",
    "overall_stability_vals = []\n",
    "overall_rel_errors = []\n",
    "\n",
    "sigma = 0.01\n",
    "num_noise_samples = 20\n",
    "relerr_resamples = 5\n",
    "\n",
    "for test_idx in selected_test_nodes:\n",
    "    label_idx = int(graph.ndata['labels'][test_idx].item())\n",
    "    class_name = class_names[label_idx]\n",
    "\n",
    "    stab_val = stability_for_sample(test_idx, sigma, num_noise_samples)\n",
    "\n",
    "    # Relative error check: re-estimate stability with fresh noise\n",
    "    re_vals = [stability_for_sample(test_idx, sigma, num_noise_samples)\n",
    "               for _ in range(relerr_resamples)]\n",
    "    avg_reval = float(np.mean(re_vals))\n",
    "    rel_err = abs(stab_val - avg_reval) / (abs(avg_reval) + 1e-12)\n",
    "\n",
    "    class_stability[class_name].append(stab_val)\n",
    "    class_rel_errors[class_name].append(rel_err)\n",
    "    overall_stability_vals.append(stab_val)\n",
    "    overall_rel_errors.append(rel_err)\n",
    "\n",
    "# --------------------------------\n",
    "# 4) Reporting\n",
    "# --------------------------------\n",
    "print(\"\\nClass-wise Stability Under Input Noise (with Relative Error):\")\n",
    "header = \"{:<12s} {:>33s} {:>30s}\".format(\"Class\", \"Avg. Stability ± Std\", \"Avg Rel. Error ± Std\")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for cn in class_names:\n",
    "    s_vals = class_stability[cn]\n",
    "    e_vals = class_rel_errors[cn]\n",
    "    if s_vals:\n",
    "        print(\"{:<12s} {:>11.4f} ± {:<10.4f} {:>15.4e} ± {:<10.4e}\".format(\n",
    "            cn, np.mean(s_vals), np.std(s_vals),\n",
    "            np.mean(e_vals), np.std(e_vals)\n",
    "        ))\n",
    "    else:\n",
    "        print(\"{:<12s} {:>11s} {:<10s} {:>15s} {:<10s}\".format(cn, \"-\", \"-\", \"-\", \"-\"))\n",
    "\n",
    "if overall_stability_vals:\n",
    "    print(\"\\nOverall Aggregated Stability Under Input Noise:\")\n",
    "    print(\"Avg Stability: {:.4f} ± {:.4f}\".format(np.mean(overall_stability_vals), np.std(overall_stability_vals)))\n",
    "    print(\"Avg Relative Error: {:.4e} ± {:.4e}\".format(np.mean(overall_rel_errors), np.std(overall_rel_errors)))\n",
    "else:\n",
    "    print(\"\\nOverall Aggregated Stability Under Input Noise:\")\n",
    "    print(\"No samples selected; overall metrics unavailable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09891bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
