{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "627cdf0a",
   "metadata": {},
   "source": [
    "1. Graph Convolutional Networks (GCN)\n",
    "GCN is a basic method for applying deep learning to graph-structured data. It works by aggregating information from neighboring nodes to update each nodeâ€™s representation.\n",
    "\n",
    "How it works?\n",
    "\n",
    "Each node collects features from its neighbors.\n",
    "\n",
    "The features are averaged (or summed) with fixed weights (predefined).\n",
    "\n",
    "A neural network transforms the aggregated features.\n",
    "\n",
    "ðŸ”¹ Limitation of GCN:\n",
    "\n",
    "All neighbors contribute equally (fixed weights).\n",
    "\n",
    "Cannot distinguish which neighbors are more important.\n",
    "\n",
    "2. Graph Attention Networks (GAT)\n",
    "GAT improves GCN by introducing attention mechanisms, meaning it learns to focus more on important neighbors and less on irrelevant ones.\n",
    "\n",
    "How it works?\n",
    "\n",
    "Instead of fixed weights, GAT computes dynamic attention scores between nodes.\n",
    "\n",
    "The model learns which neighbors are more relevant for a given node.\n",
    "\n",
    "The aggregation is now a weighted sum, where important neighbors contribute more.\n",
    "\n",
    "ðŸ”¹ Key Differences from GCN:\n",
    "\n",
    "Dynamic Weights (Attention):\n",
    "\n",
    "GCN â†’ Fixed weights for all neighbors.\n",
    "\n",
    "GAT â†’ Learns weights based on node features.\n",
    "\n",
    "Interpretability:\n",
    "\n",
    "Attention scores tell us which neighbors are more important.\n",
    "\n",
    "Flexibility:\n",
    "\n",
    "Can handle varying importance of neighbors (e.g., in social networks, some friends influence you more than others).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30236207",
   "metadata": {},
   "source": [
    "Download the dataset from \"https://github.com/jpsety/verilog_benchmark_circuits\" and put it under \"verilog_benchmark_circuits\" folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96e8837",
   "metadata": {},
   "source": [
    "#### Fault Injection\n",
    "\n",
    "Adding faults  ramdonly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a2e37e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fault injection complete. Metadata saved to: fault_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# === CONFIGURATION === #\n",
    "SRC_DIR = Path(\"verilog_benchmark_circuits\")\n",
    "FAULT_TYPES = [\"stuck_at\", \"glitch\", \"bridging\"]\n",
    "OUT_DIRS = {ft: Path(ft) for ft in FAULT_TYPES}\n",
    "CLEAN_DIR = Path(\"clean_netlists\")\n",
    "META_LOG = Path(\"fault_metadata.csv\")\n",
    "random.seed(42)  # reproducibility\n",
    "\n",
    "# === CLEANUP === #\n",
    "for d in OUT_DIRS.values():\n",
    "    if d.exists():\n",
    "        shutil.rmtree(d)\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CLEAN_DIR.mkdir(exist_ok=True)\n",
    "if META_LOG.exists():\n",
    "    META_LOG.unlink()\n",
    "\n",
    "# === REGEX to Extract Nets === #\n",
    "net_decl_re = re.compile(r'\\b(?:wire|reg)\\s+([^;]+);')\n",
    "\n",
    "# === Fault Injection Helper === #\n",
    "def inject_into_module(verilog_text: str, injection: str) -> str:\n",
    "    parts = verilog_text.rsplit(\"endmodule\", maxsplit=1)\n",
    "    if len(parts) != 2:\n",
    "        raise ValueError(f\"No endmodule found in {verilog_text[:50]}...\")\n",
    "    return parts[0] + injection + \"\\nendmodule\" + parts[1]\n",
    "\n",
    "# === MAIN LOOP === #\n",
    "with open(META_LOG, \"w\") as metaf:\n",
    "    metaf.write(\"filename,fault_type,net1,net2(optional)\\n\")\n",
    "\n",
    "    for vfile in sorted(SRC_DIR.glob(\"*.v\")):\n",
    "        text = vfile.read_text()\n",
    "\n",
    "        # Save clean copy\n",
    "        shutil.copy(vfile, CLEAN_DIR / vfile.name)\n",
    "\n",
    "        # Extract nets\n",
    "        nets = []\n",
    "        for m in net_decl_re.finditer(text):\n",
    "            for tok in re.split(r'[\\s,]+', m.group(1).strip()):\n",
    "                if tok:\n",
    "                    nets.append(tok)\n",
    "        if not nets:\n",
    "            continue\n",
    "\n",
    "        # Determine fault count: 10% of nets, min 10, max 100\n",
    "        k = min(100, max(10, int(0.1 * len(nets))))\n",
    "\n",
    "        ### === STUCK-AT FAULTS === ###\n",
    "        sa_nets = random.sample(nets, k)\n",
    "        sa_lines = []\n",
    "        for net_sa in sa_nets:\n",
    "            val = random.choice([\"1'b0\", \"1'b1\"])\n",
    "            sa_lines.append(f\"  // __INJECTED_FAULT__ type=stuck_at net={net_sa} value={val}\")\n",
    "            sa_lines.append(f\"  assign {net_sa} = {val};\")\n",
    "            metaf.write(f\"{vfile.name},stuck_at,{net_sa},\\n\")\n",
    "        sa_block = \"\\n\".join(sa_lines)\n",
    "        out_sa = OUT_DIRS[\"stuck_at\"] / f\"stuck_at_{vfile.name}\"\n",
    "        out_sa.write_text(inject_into_module(text, sa_block))\n",
    "\n",
    "        ### === GLITCH FAULTS === ###\n",
    "        gl_nets = random.sample(nets, k)\n",
    "        gl_lines = []\n",
    "        for net_gl in gl_nets:\n",
    "            delay1 = random.randint(1, 20)\n",
    "            delay2 = delay1 + random.randint(1, 10)\n",
    "            gl_lines.append(f\"  // __INJECTED_FAULT__ type=glitch net={net_gl} delays=({delay1},{delay2})\")\n",
    "            gl_lines.append(f\"  initial begin\")\n",
    "            gl_lines.append(f\"    #{delay1} {net_gl} = ~{net_gl};\")\n",
    "            gl_lines.append(f\"    #{delay2} {net_gl} = ~{net_gl};\")\n",
    "            gl_lines.append(f\"  end\")\n",
    "            metaf.write(f\"{vfile.name},glitch,{net_gl},\\n\")\n",
    "        gl_block = \"\\n\".join(gl_lines)\n",
    "        out_gl = OUT_DIRS[\"glitch\"] / f\"glitch_{vfile.name}\"\n",
    "        out_gl.write_text(inject_into_module(text, gl_block))\n",
    "\n",
    "        ### === BRIDGING FAULTS === ###\n",
    "        br_lines = []\n",
    "        used_pairs = set()\n",
    "        for _ in range(k):\n",
    "            while True:\n",
    "                n1, n2 = random.sample(nets, 2)\n",
    "                if n1 != n2 and (n1, n2) not in used_pairs and (n2, n1) not in used_pairs:\n",
    "                    used_pairs.add((n1, n2))\n",
    "                    break\n",
    "            br_lines.append(f\"  // __INJECTED_FAULT__ type=bridging nets=({n1},{n2})\")\n",
    "            br_lines.append(f\"  assign {n1} = {n2};\")\n",
    "            metaf.write(f\"{vfile.name},bridging,{n1},{n2}\\n\")\n",
    "        br_block = \"\\n\".join(br_lines)\n",
    "        out_br = OUT_DIRS[\"bridging\"] / f\"bridging_{vfile.name}\"\n",
    "        out_br.write_text(inject_into_module(text, br_block))\n",
    "\n",
    "print(\"âœ… Fault injection complete. Metadata saved to:\", META_LOG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788e95e5",
   "metadata": {},
   "source": [
    "#### Parser: Netlist to Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3a7e167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote clean netlists: 30\n",
      "Wrote 30 files for fault_type=stuck_at\n",
      "Wrote 30 files for fault_type=glitch\n",
      "Wrote 30 files for fault_type=bridging\n",
      "nodes.csv and edges.csv regenerated.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "SRC_DIR    = Path(\"verilog_benchmark_circuits\")  # original clean netlists\n",
    "FAULT_DIRS = {\"stuck_at\": Path(\"stuck_at\"),\n",
    "              \"glitch\"  : Path(\"glitch\"),\n",
    "              \"bridging\": Path(\"bridging\")}\n",
    "OUT_NODE   = Path(\"nodes.csv\")\n",
    "OUT_EDGE   = Path(\"edges.csv\")\n",
    "\n",
    "# ----------------- REGEX HELPERS -----------------\n",
    "COMMENT_RE        = re.compile(r'//.*')\n",
    "BLOCK_COMMENT_RE  = re.compile(r'/\\*.*?\\*/', re.DOTALL)\n",
    "MODULE_RE         = re.compile(r'\\bmodule\\b.*?\\bendmodule\\b', re.DOTALL)\n",
    "PORT_DECL_RE      = re.compile(r'\\b(input|output|inout)\\b\\s+([^;]+);', re.DOTALL)\n",
    "NET_DECL_RE       = re.compile(r'\\b(wire|reg)\\b\\s+([^;]+);', re.DOTALL)\n",
    "# Instance like: <GateType> <InstName> ( ... ) ;   (allow multiline ports)\n",
    "INST_RE           = re.compile(r'^\\s*([A-Za-z_]\\w*)\\s+([A-Za-z_]\\w*)\\s*\\(\\s*(.*?)\\s*\\)\\s*$', re.DOTALL | re.MULTILINE)\n",
    "# Named port maps: .PORT(<anything not crossing ')'>)\n",
    "PORT_MAP_RE       = re.compile(r'\\.(\\w+)\\s*\\(\\s*([^)]+)\\s*\\)')\n",
    "# Identifiers: escaped (\\foo ) or standard\n",
    "ID_RE             = re.compile(r'(\\\\[^\\s]+\\s|[A-Za-z_]\\w*)')\n",
    "# Replace (...) groups to flatten newlines within parens (for easier splitting)\n",
    "PARENS_RE         = re.compile(r'\\((.*?)\\)', re.DOTALL)\n",
    "# Constants / empties / concat\n",
    "CONST_LIKE        = re.compile(r\"^\\s*(?:\\d+'[bdhoBDHO][0-9xzXZ_]+|['\\\"].*['\\\"]|\\{.*\\}|\\s*)$\")\n",
    "\n",
    "# Known output port names across common cell libs\n",
    "OUT_PORTS = {\"Y\",\"ZN\",\"Z\",\"Q\",\"O\",\"CO\",\"S\",\"SUM\",\"QN\",\"QB\",\"OUT\"}\n",
    "\n",
    "# Qualifiers we donâ€™t want to treat as nets\n",
    "QUAL_STOP = {\n",
    "    \"signed\",\"unsigned\",\"wire\",\"reg\",\"logic\",\"tri\",\"supply0\",\"supply1\",\n",
    "    \"wand\",\"wor\",\"tri0\",\"tri1\"\n",
    "}\n",
    "\n",
    "# Stopwords to ignore when scanning fault-comments\n",
    "FAULT_STOP = {\n",
    "    \"fault\",\"force\",\"initial\",\"short\",\"to\",\"faults\",\"glitch\",\"bridging\",\"bridge\",\n",
    "    \"type\",\"net\",\"nets\",\"value\",\"delays\",\"assign\",\"__INJECTED_FAULT__\",\"begin\",\"end\"\n",
    "}\n",
    "\n",
    "def strip_comments(txt: str) -> str:\n",
    "    txt = BLOCK_COMMENT_RE.sub(\"\", txt)\n",
    "    return COMMENT_RE.sub(\"\", txt)\n",
    "\n",
    "def extract_module(txt: str) -> str:\n",
    "    \"\"\"\n",
    "    Choose the module block with the highest count of instance-like lines,\n",
    "    rather than the first occurrence.\n",
    "    \"\"\"\n",
    "    candidates = list(MODULE_RE.finditer(txt))\n",
    "    if not candidates:\n",
    "        return \"\"\n",
    "    best_block = None\n",
    "    best_score = -1\n",
    "    for m in candidates:\n",
    "        block = m.group(0)\n",
    "        # Count lines that look like \"<type> <inst> (\" as a heuristic\n",
    "        score = len(re.findall(r'^\\s*[A-Za-z_]\\w*\\s+[A-Za-z_]\\w*\\s*\\(', block, flags=re.MULTILINE))\n",
    "        if score > best_score:\n",
    "            best_block, best_score = block, score\n",
    "    return best_block\n",
    "\n",
    "def find_ids(s: str):\n",
    "    return [m.group(1).rstrip() for m in ID_RE.finditer(s)]\n",
    "\n",
    "def parse_named_ports(s: str):\n",
    "    \"\"\"\n",
    "    Parse .PORT(EXPR) pairs. Skip constants/concat/empty. For expressions,\n",
    "    pick the first identifier-like token as the net name.\n",
    "    \"\"\"\n",
    "    pm = {}\n",
    "    for m in PORT_MAP_RE.finditer(s):\n",
    "        p, raw = m.group(1), m.group(2).strip()\n",
    "        if CONST_LIKE.match(raw):\n",
    "            continue\n",
    "        cand = ID_RE.search(raw)\n",
    "        if cand:\n",
    "            pm[p] = cand.group(1).rstrip()\n",
    "    return pm\n",
    "\n",
    "def extract_fault_nets(raw: str, known_nets: set):\n",
    "    \"\"\"\n",
    "    Robustly find nets that were targeted by injected faults.\n",
    "\n",
    "    Strategies:\n",
    "     - comments containing keywords (fault / short / bridge / glitch / force)\n",
    "     - assign <net> = 1'b0 / 1'b1   --> stuck-at\n",
    "     - initial ... #<delay> <net> = ... --> glitch (with/without 'begin')\n",
    "     - assign <n1> = <n2> near file tail (injection appended before endmodule) --> bridging\n",
    "\n",
    "    Returns a set of net identifiers (strings) intersected with known_nets.\n",
    "    \"\"\"\n",
    "    nets = set()\n",
    "    lines = raw.splitlines()\n",
    "\n",
    "    # 1) comment-keyword scan\n",
    "    for ln in lines:\n",
    "        low = ln.lower()\n",
    "        if any(k in low for k in (\"fault\", \"short\", \"bridge\", \"bridging\", \"glitch\", \"force\")):\n",
    "            for tok in find_ids(ln):\n",
    "                if tok in FAULT_STOP:\n",
    "                    continue\n",
    "                nets.add(tok)\n",
    "\n",
    "    # 2) assign to constant -> stuck-at\n",
    "    for m in re.finditer(r'assign\\s+([\\w\\[\\]\\\\]+)\\s*=\\s*1\\'b([01])', raw, flags=re.IGNORECASE):\n",
    "        nets.add(m.group(1))\n",
    "\n",
    "    # 3) glitch toggles: initial [begin] ... #delay NET = ...\n",
    "    for m in re.finditer(r'initial\\b(?:\\s+begin)?[\\s\\S]*?#\\s*\\d+\\s*([\\w\\[\\]\\\\]+)\\s*=', raw, flags=re.IGNORECASE):\n",
    "        nets.add(m.group(1))\n",
    "\n",
    "    # 4) assign net = net near tail (bridging)\n",
    "    tail = \"\\n\".join(lines[-300:])  # examine last ~300 lines for injected assigns\n",
    "    for m in re.finditer(r'assign\\s+([\\w\\[\\]\\\\]+)\\s*=\\s*([\\w\\[\\]\\\\]+)\\s*;', tail, flags=re.IGNORECASE):\n",
    "        n1, n2 = m.group(1), m.group(2)\n",
    "        nets.add(n1); nets.add(n2)\n",
    "\n",
    "    # 5) fallback keywords\n",
    "    if not nets:\n",
    "        for ln in lines:\n",
    "            low = ln.lower()\n",
    "            if \"short\" in low or \"bridge\" in low or \"glitch\" in low:\n",
    "                for tok in find_ids(ln):\n",
    "                    if tok in FAULT_STOP:\n",
    "                        continue\n",
    "                    nets.add(tok)\n",
    "\n",
    "    # Keep only known nets\n",
    "    return nets & known_nets\n",
    "\n",
    "# ----------------- OUTPUT RESET -----------------\n",
    "for f in (OUT_NODE, OUT_EDGE):\n",
    "    if f.exists():\n",
    "        f.unlink()\n",
    "\n",
    "with OUT_NODE.open(\"w\", newline=\"\") as nf, OUT_EDGE.open(\"w\", newline=\"\") as ef:\n",
    "    node_w = csv.writer(nf)\n",
    "    edge_w = csv.writer(ef)\n",
    "\n",
    "    node_w.writerow([\n",
    "        \"netlist_file\",\"fault_type\",\"node_id\",\"gate_type\",\n",
    "        \"in_degree\",\"out_degree\",\"total_degree\",\n",
    "        \"path_depth\",\"CC0\",\"CC1\",\"CO\",\n",
    "        \"is_input\",\"is_output\",\"label\"\n",
    "    ])\n",
    "    edge_w.writerow([\n",
    "        \"netlist_file\",\"fault_type\",\"src_node\",\"dst_node\",\"fan_out\"\n",
    "    ])\n",
    "\n",
    "    def process_verilog_and_write(vfile_path: Path, fault_type: str):\n",
    "        raw_txt = vfile_path.read_text()\n",
    "        body = extract_module(strip_comments(raw_txt))\n",
    "        if not body:\n",
    "            return False\n",
    "        fname = vfile_path.name\n",
    "        # base name: if file is stuck_at_adder.v -> base = adder.v\n",
    "        if fault_type != \"clean\" and fname.startswith(f\"{fault_type}_\"):\n",
    "            base_name = fname[len(fault_type)+1:]\n",
    "        else:\n",
    "            base_name = fname\n",
    "\n",
    "        # --- Parse IOs ---\n",
    "        directions = {}\n",
    "        for m in PORT_DECL_RE.finditer(body):\n",
    "            d, nets_str = m.groups()\n",
    "            for net in find_ids(nets_str):\n",
    "                if net in QUAL_STOP:\n",
    "                    continue\n",
    "                directions[net] = d  # input/output/inout\n",
    "\n",
    "        # --- Net declarations ---\n",
    "        nets = set(directions)\n",
    "        for m in NET_DECL_RE.finditer(body):\n",
    "            for net in find_ids(m.group(2)):\n",
    "                if net in QUAL_STOP:\n",
    "                    continue\n",
    "                nets.add(net)\n",
    "\n",
    "        # --- Gate instances ---\n",
    "        driver = {}\n",
    "        loads  = defaultdict(list)\n",
    "        gates  = {}\n",
    "        # Flatten newlines within parentheses to simplify statement splitting\n",
    "        tmp = PARENS_RE.sub(lambda x: x.group(0).replace('\\n',' '), body)\n",
    "        for stmt in tmp.split(';'):\n",
    "            stmt = stmt.strip()\n",
    "            if not stmt or \"(\" not in stmt:\n",
    "                continue\n",
    "            head = stmt.split()[0]\n",
    "            if head in {\"module\",\"endmodule\",\"input\",\"output\",\"inout\",\"wire\",\"reg\",\"assign\"}:\n",
    "                continue\n",
    "            m = INST_RE.match(stmt)\n",
    "            if not m:\n",
    "                continue\n",
    "            gatetype, inst, ports_str = m.groups()\n",
    "            gates[inst] = gatetype\n",
    "            if \".\" in ports_str:\n",
    "                pm = parse_named_ports(ports_str)\n",
    "                for p, net in pm.items():\n",
    "                    if p in OUT_PORTS:\n",
    "                        driver[net] = inst\n",
    "                    else:\n",
    "                        loads[net].append(inst)\n",
    "            else:\n",
    "                tokens = find_ids(ports_str)\n",
    "                if not tokens:\n",
    "                    continue\n",
    "                out, ins = tokens[0], tokens[1:]\n",
    "                driver[out] = inst\n",
    "                for net in ins:\n",
    "                    loads[net].append(inst)\n",
    "\n",
    "        nets |= set(driver) | set(loads)\n",
    "\n",
    "        # --- Build graph structure (instance-level with PI_/PO_ sentinels) ---\n",
    "        Gnodes = set(gates.keys())\n",
    "        for net in nets:\n",
    "            if net not in driver:\n",
    "                Gnodes.add(f\"PI_{net}\")\n",
    "            if not loads.get(net):\n",
    "                Gnodes.add(f\"PO_{net}\")\n",
    "\n",
    "        succ = defaultdict(list)\n",
    "        pred = defaultdict(list)\n",
    "        for net in nets:\n",
    "            src = driver.get(net, f\"PI_{net}\")\n",
    "            if loads.get(net):\n",
    "                for dst in loads[net]:\n",
    "                    succ[src].append(dst)\n",
    "                    pred[dst].append(src)\n",
    "            else:\n",
    "                dst = f\"PO_{net}\"\n",
    "                succ[src].append(dst)\n",
    "                pred[dst].append(src)\n",
    "        for n in Gnodes:\n",
    "            succ.setdefault(n, [])\n",
    "            pred.setdefault(n, [])\n",
    "\n",
    "        # --- Path depth via BFS from PIs ---\n",
    "        path_depth = {}\n",
    "        dq = deque([n for n in Gnodes if n.startswith(\"PI_\")])\n",
    "        for n in dq:\n",
    "            path_depth[n] = 0\n",
    "        while dq:\n",
    "            u = dq.popleft()\n",
    "            for v in succ[u]:\n",
    "                if v not in path_depth:\n",
    "                    path_depth[v] = path_depth[u] + 1\n",
    "                    dq.append(v)\n",
    "        maxd = max(path_depth.values(), default=0)\n",
    "        for n in Gnodes:\n",
    "            path_depth.setdefault(n, maxd)\n",
    "\n",
    "        # --- Fault nets detection ---\n",
    "        if fault_type == \"clean\":\n",
    "            fault_nets = set()\n",
    "        else:\n",
    "            known_nets = set(nets)\n",
    "            fault_nets = extract_fault_nets(raw_txt, known_nets)\n",
    "\n",
    "        # map fault_nets -> nodes: prefer driver[net] if present else PI_net\n",
    "        fault_nodes = set()\n",
    "        for net in fault_nets:\n",
    "            if net in driver:\n",
    "                fault_nodes.add(driver[net])\n",
    "            else:\n",
    "                fault_nodes.add(f\"PI_{net}\")\n",
    "\n",
    "        # --- approx SCOAP ---\n",
    "        def approx_scoap(n):\n",
    "            typ = \"PI\" if n.startswith(\"PI_\") else (\"PO\" if n.startswith(\"PO_\") else gates.get(n,\"unk\"))\n",
    "            d   = path_depth.get(n, maxd)\n",
    "            cc0 = cc1 = max(1, 10 - d)\n",
    "            co  = d + 1\n",
    "            if typ == \"PI\": cc0 = cc1 = 1\n",
    "            if typ == \"PO\": co = 1\n",
    "            return cc0, cc1, co\n",
    "\n",
    "        # --- Dump nodes ---\n",
    "        for n in sorted(Gnodes):\n",
    "            indeg = len(pred[n])\n",
    "            outdeg = len(succ[n])\n",
    "            deg = indeg + outdeg\n",
    "            cc0, cc1, co = approx_scoap(n)\n",
    "            is_in = int(n.startswith(\"PI_\"))\n",
    "            is_out = int(n.startswith(\"PO_\"))\n",
    "            lbl = int(n in fault_nodes)  # 1 for faulty node, 0 otherwise\n",
    "            node_w.writerow([\n",
    "                base_name, fault_type, n, (\"PI\" if is_in else (\"PO\" if is_out else gates.get(n,\"unk\"))),\n",
    "                indeg, outdeg, deg,\n",
    "                path_depth.get(n, maxd), cc0, cc1, co,\n",
    "                is_in, is_out, lbl\n",
    "            ])\n",
    "\n",
    "        # --- Dump edges ---\n",
    "        for net in sorted(nets):\n",
    "            src = driver.get(net, f\"PI_{net}\")\n",
    "            if loads.get(net):\n",
    "                fout = len(loads[net])\n",
    "                for dst in loads[net]:\n",
    "                    edge_w.writerow([base_name, fault_type, src, dst, fout])\n",
    "            else:\n",
    "                fout = 0\n",
    "                dst = f\"PO_{net}\"\n",
    "                edge_w.writerow([base_name, fault_type, src, dst, fout])\n",
    "\n",
    "        return True\n",
    "\n",
    "    # ----------------- PROCESS CLEAN NETLISTS -----------------\n",
    "    clean_count = 0\n",
    "    for v in sorted(SRC_DIR.glob(\"*.v\")):\n",
    "        ok = process_verilog_and_write(v, \"clean\")\n",
    "        if ok:\n",
    "            clean_count += 1\n",
    "\n",
    "    # ----------------- PROCESS FAULTED NETLISTS -----------------\n",
    "    ft_counts = defaultdict(int)\n",
    "    for ft, folder in FAULT_DIRS.items():\n",
    "        if not folder.exists():\n",
    "            continue\n",
    "        for v in sorted(folder.glob(\"*.v\")):\n",
    "            ok = process_verilog_and_write(v, ft)\n",
    "            if ok:\n",
    "                ft_counts[ft] += 1\n",
    "\n",
    "    print(f\"Wrote clean netlists: {clean_count}\")\n",
    "    for ft, c in ft_counts.items():\n",
    "        print(f\"Wrote {c} files for fault_type={ft}\")\n",
    "\n",
    "print(\"nodes.csv and edges.csv regenerated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4b4f2",
   "metadata": {},
   "source": [
    "Inspecting the data gathered in the csv files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67ebaaa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Inspecting nodes.csv ===\n",
      "Shape: (1746912, 14)\n",
      "\n",
      "Column: netlist_file\n",
      "  dtype      : object\n",
      "  unique vals: 30\n",
      "  samples    : ['adder.v' 'arbiter.v' 'bar.v' 'c1355.v' 'c17.v' 'c1908.v' 'c2670.v'\n",
      " 'c3540.v' 'c432.v' 'c499.v']\n",
      "\n",
      "Column: fault_type\n",
      "  dtype      : object\n",
      "  unique vals: 4\n",
      "  samples    : ['clean' 'stuck_at' 'glitch' 'bridging']\n",
      "\n",
      "Column: node_id\n",
      "  dtype      : object\n",
      "  unique vals: 117701\n",
      "  samples    : ['PI_\\\\a[0]' 'PI_\\\\a[100]' 'PI_\\\\a[101]' 'PI_\\\\a[102]' 'PI_\\\\a[103]'\n",
      " 'PI_\\\\a[104]' 'PI_\\\\a[105]' 'PI_\\\\a[106]' 'PI_\\\\a[107]' 'PI_\\\\a[108]']\n",
      "\n",
      "Column: gate_type\n",
      "  dtype      : object\n",
      "  unique vals: 8\n",
      "  samples    : ['PI' 'PO' 'not' 'and' 'or' 'nand' 'nor' 'xor']\n",
      "\n",
      "Column: in_degree\n",
      "  dtype      : int64\n",
      "  min        : 0\n",
      "  max        : 8\n",
      "  unique vals: 7\n",
      "\n",
      "Column: out_degree\n",
      "  dtype      : int64\n",
      "  min        : 0\n",
      "  max        : 1667\n",
      "  unique vals: 176\n",
      "\n",
      "Column: total_degree\n",
      "  dtype      : int64\n",
      "  min        : 1\n",
      "  max        : 1667\n",
      "  unique vals: 177\n",
      "\n",
      "Column: path_depth\n",
      "  dtype      : int64\n",
      "  min        : 0\n",
      "  max        : 40\n",
      "  unique vals: 41\n",
      "\n",
      "Column: CC0\n",
      "  dtype      : int64\n",
      "  min        : 1\n",
      "  max        : 9\n",
      "  unique vals: 9\n",
      "\n",
      "Column: CC1\n",
      "  dtype      : int64\n",
      "  min        : 1\n",
      "  max        : 9\n",
      "  unique vals: 9\n",
      "\n",
      "Column: CO\n",
      "  dtype      : int64\n",
      "  min        : 1\n",
      "  max        : 41\n",
      "  unique vals: 41\n",
      "\n",
      "Column: is_input\n",
      "  dtype      : int64\n",
      "  min        : 0\n",
      "  max        : 1\n",
      "  unique vals: 2\n",
      "\n",
      "Column: is_output\n",
      "  dtype      : int64\n",
      "  min        : 0\n",
      "  max        : 1\n",
      "  unique vals: 2\n",
      "\n",
      "Column: label\n",
      "  dtype      : int64\n",
      "  min        : 0\n",
      "  max        : 1\n",
      "  unique vals: 2\n",
      "\n",
      "\n",
      "=== Inspecting edges.csv ===\n",
      "Shape: (2757428, 5)\n",
      "\n",
      "Column: netlist_file\n",
      "  dtype      : object\n",
      "  unique vals: 30\n",
      "  samples    : ['adder.v' 'arbiter.v' 'bar.v' 'c1355.v' 'c17.v' 'c1908.v' 'c2670.v'\n",
      " 'c3540.v' 'c432.v' 'c499.v']\n",
      "\n",
      "Column: fault_type\n",
      "  dtype      : object\n",
      "  unique vals: 4\n",
      "  samples    : ['clean' 'stuck_at' 'glitch' 'bridging']\n",
      "\n",
      "Column: src_node\n",
      "  dtype      : object\n",
      "  unique vals: 114187\n",
      "  samples    : ['PI_\\\\a[0]' 'PI_\\\\a[100]' 'PI_\\\\a[101]' 'PI_\\\\a[102]' 'PI_\\\\a[103]'\n",
      " 'PI_\\\\a[104]' 'PI_\\\\a[105]' 'PI_\\\\a[106]' 'PI_\\\\a[107]' 'PI_\\\\a[108]']\n",
      "\n",
      "Column: dst_node\n",
      "  dtype      : object\n",
      "  unique vals: 113265\n",
      "  samples    : ['g2' 'g6' 'g1691' 'g1708' 'g1725' 'g1742' 'g1759' 'g1776' 'g1793' 'g1810']\n",
      "\n",
      "Column: fan_out\n",
      "  dtype      : int64\n",
      "  min        : 0\n",
      "  max        : 1667\n",
      "  unique vals: 176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def inspect_csv(path):\n",
    "    \"\"\"\n",
    "    Print file shape, and for each column:\n",
    "      - numeric: min, max, number of unique values\n",
    "      - non-numeric: number of unique values and sample uniques\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"\\n=== Inspecting {path} ===\")\n",
    "    print(f\"Shape: {df.shape}\\n\")\n",
    "    for col in df.columns:\n",
    "        series = df[col]\n",
    "        print(f\"Column: {col}\")\n",
    "        if pd.api.types.is_numeric_dtype(series):\n",
    "            print(f\"  dtype      : {series.dtype}\")\n",
    "            print(f\"  min        : {series.min()}\")\n",
    "            print(f\"  max        : {series.max()}\")\n",
    "            print(f\"  unique vals: {series.nunique()}\\n\")\n",
    "        else:\n",
    "            uniques = series.dropna().unique()\n",
    "            print(f\"  dtype      : {series.dtype}\")\n",
    "            print(f\"  unique vals: {len(uniques)}\")\n",
    "            # show up to 10 sample unique values\n",
    "            print(f\"  samples    : {uniques[:10]}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for fname in [\"nodes.csv\", \"edges.csv\"]:\n",
    "        inspect_csv(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee2b6136",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fault_type\n",
      "clean       436728\n",
      "stuck_at    436728\n",
      "glitch      436728\n",
      "bridging    436728\n",
      "Name: count, dtype: int64\n",
      "Labels per fault type:\n",
      "fault_type\n",
      "bridging    4532\n",
      "clean          0\n",
      "glitch      2376\n",
      "stuck_at    2373\n",
      "Name: label, dtype: int64\n",
      "       netlist_file fault_type     node_id gate_type  in_degree  out_degree  \\\n",
      "436731      adder.v   stuck_at  PI_\\a[102]        PI          0           1   \n",
      "436737      adder.v   stuck_at  PI_\\a[108]        PI          0           1   \n",
      "436753      adder.v   stuck_at  PI_\\a[122]        PI          0           1   \n",
      "436774      adder.v   stuck_at   PI_\\a[26]        PI          0           1   \n",
      "436858      adder.v   stuck_at  PI_\\b[101]        PI          0           1   \n",
      "436906      adder.v   stuck_at    PI_\\b[2]        PI          0           1   \n",
      "436917      adder.v   stuck_at    PI_\\b[3]        PI          0           1   \n",
      "436927      adder.v   stuck_at   PI_\\b[49]        PI          0           1   \n",
      "436938      adder.v   stuck_at   PI_\\b[59]        PI          0           1   \n",
      "437267      adder.v   stuck_at        g102       not          1           1   \n",
      "437291      adder.v   stuck_at       g1041       and          2           2   \n",
      "437320      adder.v   stuck_at       g1068       not          1           2   \n",
      "437359      adder.v   stuck_at       g1102       not          1           2   \n",
      "437382      adder.v   stuck_at       g1123       and          2           1   \n",
      "437405      adder.v   stuck_at       g1144       not          1           1   \n",
      "437408      adder.v   stuck_at       g1147       and          2           1   \n",
      "437409      adder.v   stuck_at       g1148       not          1           2   \n",
      "437426      adder.v   stuck_at       g1163       and          2           1   \n",
      "437434      adder.v   stuck_at       g1170       not          1           2   \n",
      "437441      adder.v   stuck_at       g1177       and          2           2   \n",
      "\n",
      "        total_degree  path_depth  CC0  CC1  CO  is_input  is_output  label  \n",
      "436731             1           0    1    1   1         1          0      1  \n",
      "436737             1           0    1    1   1         1          0      1  \n",
      "436753             1           0    1    1   1         1          0      1  \n",
      "436774             1           0    1    1   1         1          0      1  \n",
      "436858             1           0    1    1   1         1          0      1  \n",
      "436906             1           0    1    1   1         1          0      1  \n",
      "436917             1           0    1    1   1         1          0      1  \n",
      "436927             1           0    1    1   1         1          0      1  \n",
      "436938             1           0    1    1   1         1          0      1  \n",
      "437267             2           5    5    5   6         0          0      1  \n",
      "437291             4           3    7    7   4         0          0      1  \n",
      "437320             3           4    6    6   5         0          0      1  \n",
      "437359             3           4    6    6   5         0          0      1  \n",
      "437382             3           6    4    4   7         0          0      1  \n",
      "437405             2           1    9    9   2         0          0      1  \n",
      "437408             3           1    9    9   2         0          0      1  \n",
      "437409             3           3    7    7   4         0          0      1  \n",
      "437426             3           2    8    8   3         0          0      1  \n",
      "437434             3           4    6    6   5         0          0      1  \n",
      "437441             4           3    7    7   4         0          0      1  \n",
      "(6908, 14)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"nodes.csv\")\n",
    "print(df.fault_type.value_counts())\n",
    "print(\"Labels per fault type:\")\n",
    "print(df.groupby(\"fault_type\").label.sum())\n",
    "# Example: show some rows for stuck_at with label==1\n",
    "print(df[(df.fault_type==\"stuck_at\") & (df.label==1)].head(20))\n",
    "\n",
    "nodes_all = pd.read_csv(\"nodes.csv\")\n",
    "filtered_nodes = nodes_all[(nodes_all['fault_type'] != 'stuck_at') & (nodes_all['label']==1)]\n",
    "print(filtered_nodes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d2b162",
   "metadata": {},
   "source": [
    "Confirmation: Why No Clock/Reset Nets?\n",
    "ISCAS-85:\n",
    " - These are purely combinational circuits.\n",
    " - No flip-flops, registers, clocks, or resets.\n",
    " - So our result â€” zero matches for clk, rst, etc. â€” is expected.\n",
    "\n",
    "EPFL Benchmark Suite: \n",
    "- EPFL includes both combinational and sequential circuits.\n",
    "- However, the versions in verilog_benchmark_circuits repo (like from jpsety) are typically synthesized as combinational gate-level netlists.\n",
    "\n",
    "Clocks and registers are flattened away unless specifically preserved (e.g., for formal verification or sequential test)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeecae5",
   "metadata": {},
   "source": [
    "#### Generating the clean_nodes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6357e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean CSV generation complete:\n",
      " - clean_nodes.csv\n",
      " - clean_edges.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# -------- CONFIGURATION --------\n",
    "SRC_DIR     = Path(\"verilog_benchmark_circuits\")\n",
    "NODE_CSV    = Path(\"clean_nodes.csv\")\n",
    "EDGE_CSV    = Path(\"clean_edges.csv\")\n",
    "\n",
    "# -------- REGEX PATTERNS --------\n",
    "COMMENT_RE    = re.compile(r'//.*')\n",
    "MODULE_RE     = re.compile(r'\\bmodule\\b.*?\\bendmodule\\b', re.DOTALL)\n",
    "PORT_DECL_RE  = re.compile(r'\\b(input|output)\\b\\s+([^;]+);')\n",
    "NET_DECL_RE   = re.compile(r'\\b(wire|reg)\\b\\s+([^;]+);')\n",
    "INST_RE       = re.compile(r'^\\s*([A-Za-z_]\\w*)\\s+([A-Za-z_]\\w*)\\s*\\(\\s*(.*?)\\s*\\)\\s*$', re.DOTALL)\n",
    "PORT_MAP_RE   = re.compile(r'\\.(\\w+)\\s*\\(\\s*([\\w\\[\\]\\\\]+)\\s*\\)')\n",
    "ID_RE         = re.compile(r'(\\\\[^\\s]+\\s|[A-Za-z_]\\w*)')\n",
    "OUT_PORTS     = {\"Y\",\"ZN\",\"Z\",\"Q\",\"O\",\"CO\",\"S\",\"SUM\"}\n",
    "\n",
    "# -------- HELPERS --------\n",
    "def strip_comments(txt): return COMMENT_RE.sub(\"\", txt)\n",
    "def extract_module(txt): return MODULE_RE.search(txt).group(0) if MODULE_RE.search(txt) else \"\"\n",
    "def find_ids(s): return [m.group(1).rstrip() for m in ID_RE.finditer(s)]\n",
    "def parse_named_ports(s): return {m.group(1): m.group(2).rstrip() for m in PORT_MAP_RE.finditer(s)}\n",
    "\n",
    "# -------- RESET OUTPUT --------\n",
    "for f in (NODE_CSV, EDGE_CSV):\n",
    "    if f.exists(): f.unlink()\n",
    "\n",
    "with NODE_CSV.open(\"w\", newline=\"\") as nf, EDGE_CSV.open(\"w\", newline=\"\") as ef:\n",
    "    node_w = csv.writer(nf)\n",
    "    edge_w = csv.writer(ef)\n",
    "\n",
    "    node_w.writerow([\n",
    "        \"netlist_file\", \"node_id\", \"gate_type\",\n",
    "        \"in_degree\", \"out_degree\", \"total_degree\",\n",
    "        \"path_depth\", \"CC0\", \"CC1\", \"CO\",\n",
    "        \"is_input\", \"is_output\", \"label\"\n",
    "    ])\n",
    "    edge_w.writerow([\n",
    "        \"netlist_file\", \"src_node\", \"dst_node\", \"fan_out\", \"net_class\"\n",
    "    ])\n",
    "\n",
    "    for vfile in sorted(SRC_DIR.glob(\"*.v\")):\n",
    "        raw_txt = vfile.read_text()\n",
    "        body    = extract_module(strip_comments(raw_txt))\n",
    "        if not body: continue\n",
    "        fname = vfile.name\n",
    "\n",
    "        # --- Parse IOs ---\n",
    "        directions = {}\n",
    "        for m in PORT_DECL_RE.finditer(body):\n",
    "            d, nets = m.groups()\n",
    "            for net in find_ids(nets):\n",
    "                directions[net] = d\n",
    "\n",
    "        # --- Net declarations ---\n",
    "        nets = set(directions)\n",
    "        for m in NET_DECL_RE.finditer(body):\n",
    "            for net in find_ids(m.group(2)):\n",
    "                nets.add(net)\n",
    "\n",
    "        # --- Gate instances ---\n",
    "        driver = {}\n",
    "        loads  = defaultdict(list)\n",
    "        gates  = {}\n",
    "\n",
    "        tmp = re.sub(r'\\((.*?)\\)', lambda x: x.group(0).replace('\\n',' '), body, flags=re.DOTALL)\n",
    "        for stmt in tmp.split(';'):\n",
    "            stmt = stmt.strip()\n",
    "            if not stmt or \"(\" not in stmt:\n",
    "                continue\n",
    "            if stmt.split()[0] in {\"module\",\"endmodule\",\"input\",\"output\",\"wire\",\"reg\",\"assign\"}:\n",
    "                continue\n",
    "            m = INST_RE.match(stmt)\n",
    "            if not m: continue\n",
    "            gatetype, inst, ports_str = m.groups()\n",
    "            gates[inst] = gatetype\n",
    "            if \".\" in ports_str:\n",
    "                pm = parse_named_ports(ports_str)\n",
    "                for p, net in pm.items():\n",
    "                    if p in OUT_PORTS:\n",
    "                        driver[net] = inst\n",
    "                    else:\n",
    "                        loads[net].append(inst)\n",
    "            else:\n",
    "                tokens = find_ids(ports_str)\n",
    "                if tokens:\n",
    "                    out, ins = tokens[0], tokens[1:]\n",
    "                    driver[out] = inst\n",
    "                    for net in ins:\n",
    "                        loads[net].append(inst)\n",
    "\n",
    "        nets |= set(driver) | set(loads)\n",
    "\n",
    "        # --- Build graph ---\n",
    "        G = nx.DiGraph()\n",
    "        for inst, gt in gates.items():\n",
    "            G.add_node(inst, gate_type=gt)\n",
    "\n",
    "        for net in nets:\n",
    "            if net not in driver:\n",
    "                G.add_node(f\"PI_{net}\", gate_type=\"PI\")\n",
    "            if not loads.get(net):\n",
    "                G.add_node(f\"PO_{net}\", gate_type=\"PO\")\n",
    "        for net in nets:\n",
    "            src = driver.get(net, f\"PI_{net}\")\n",
    "            for dst in loads.get(net, []):\n",
    "                G.add_edge(src, dst, net=net)\n",
    "            if not loads.get(net):\n",
    "                G.add_edge(src, f\"PO_{net}\", net=net)\n",
    "\n",
    "        # --- Path depth (BFS) ---\n",
    "        path_depth = {}\n",
    "        dq = deque([n for n in G if n.startswith(\"PI_\")])\n",
    "        for n in dq: path_depth[n] = 0\n",
    "        while dq:\n",
    "            u = dq.popleft()\n",
    "            for v in G.successors(u):\n",
    "                if v not in path_depth:\n",
    "                    path_depth[v] = path_depth[u]+1\n",
    "                    dq.append(v)\n",
    "        maxd = max(path_depth.values(), default=0)\n",
    "        for n in G:\n",
    "            path_depth.setdefault(n, maxd)\n",
    "\n",
    "        # --- Approximated SCOAP ---\n",
    "        def approx_scoap(n):\n",
    "            typ = G.nodes[n][\"gate_type\"]\n",
    "            d   = path_depth[n]\n",
    "            cc0 = cc1 = max(1, 10 - d)\n",
    "            co  = d + 1\n",
    "            if typ == \"PI\": cc0 = cc1 = 1\n",
    "            if typ == \"PO\": co = 1\n",
    "            return cc0, cc1, co\n",
    "\n",
    "        # --- Dump nodes (all label=0 for clean) ---\n",
    "        for n, data in G.nodes(data=True):\n",
    "            indeg, outdeg = G.in_degree(n), G.out_degree(n)\n",
    "            deg = indeg + outdeg\n",
    "            cc0, cc1, co = approx_scoap(n)\n",
    "            node_w.writerow([\n",
    "                fname, n, data[\"gate_type\"],\n",
    "                indeg, outdeg, deg,\n",
    "                path_depth[n], cc0, cc1, co,\n",
    "                int(n.startswith(\"PI_\")), int(n.startswith(\"PO_\")),\n",
    "                0  # label = 0 (no fault)\n",
    "            ])\n",
    "\n",
    "        # --- Dump edges ---\n",
    "        for u, v, ed in G.edges(data=True):\n",
    "            net = ed[\"net\"]\n",
    "            fout = len(loads.get(net, []))\n",
    "            nclass = \"clock\" if any(t in net.lower() for t in [\"clk\", \"clock\", \"rst\", \"reset\"]) else \"data\"\n",
    "            edge_w.writerow([\n",
    "                fname, u, v, fout, nclass\n",
    "            ])\n",
    "\n",
    "print(\"Clean CSV generation complete:\")\n",
    "print(\" - clean_nodes.csv\")\n",
    "print(\" - clean_edges.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f8de363",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Inspecting clean_nodes.csv ===\n",
      "Shape: (436728, 13)\n",
      "\n",
      "Column: netlist_file\n",
      "  dtype      : object\n",
      "  unique vals: 30\n",
      "  samples    : ['adder.v' 'arbiter.v' 'bar.v' 'c1355.v' 'c17.v' 'c1908.v' 'c2670.v'\n",
      " 'c3540.v' 'c432.v' 'c499.v']\n",
      "\n",
      "Column: node_id\n",
      "  dtype      : object\n",
      "  unique vals: 117701\n",
      "  samples    : ['g1' 'g2' 'g3' 'g4' 'g5' 'g6' 'g7' 'g8' 'g9' 'g10']\n",
      "\n",
      "Column: gate_type\n",
      "  dtype      : object\n",
      "  unique vals: 8\n",
      "  samples    : ['not' 'and' 'or' 'PI' 'PO' 'nand' 'nor' 'xor']\n",
      "\n",
      "Column: in_degree\n",
      "  dtype      : int64\n",
      "  min        : 0\n",
      "  max        : 8\n",
      "  unique vals: 7\n",
      "\n",
      "Column: out_degree\n",
      "  dtype      : int64\n",
      "  min        : 0\n",
      "  max        : 1667\n",
      "  unique vals: 176\n",
      "\n",
      "Column: total_degree\n",
      "  dtype      : int64\n",
      "  min        : 1\n",
      "  max        : 1667\n",
      "  unique vals: 177\n",
      "\n",
      "Column: path_depth\n",
      "  dtype      : int64\n",
      "  min        : 0\n",
      "  max        : 40\n",
      "  unique vals: 41\n",
      "\n",
      "Column: CC0\n",
      "  dtype      : int64\n",
      "  min        : 1\n",
      "  max        : 9\n",
      "  unique vals: 9\n",
      "\n",
      "Column: CC1\n",
      "  dtype      : int64\n",
      "  min        : 1\n",
      "  max        : 9\n",
      "  unique vals: 9\n",
      "\n",
      "Column: CO\n",
      "  dtype      : int64\n",
      "  min        : 1\n",
      "  max        : 41\n",
      "  unique vals: 41\n",
      "\n",
      "Column: is_input\n",
      "  dtype      : int64\n",
      "  min        : 0\n",
      "  max        : 1\n",
      "  unique vals: 2\n",
      "\n",
      "Column: is_output\n",
      "  dtype      : int64\n",
      "  min        : 0\n",
      "  max        : 1\n",
      "  unique vals: 2\n",
      "\n",
      "Column: label\n",
      "  dtype      : int64\n",
      "  min        : 0\n",
      "  max        : 0\n",
      "  unique vals: 1\n",
      "\n",
      "\n",
      "=== Inspecting clean_edges.csv ===\n",
      "Shape: (689357, 5)\n",
      "\n",
      "Column: netlist_file\n",
      "  dtype      : object\n",
      "  unique vals: 30\n",
      "  samples    : ['adder.v' 'arbiter.v' 'bar.v' 'c1355.v' 'c17.v' 'c1908.v' 'c2670.v'\n",
      " 'c3540.v' 'c432.v' 'c499.v']\n",
      "\n",
      "Column: src_node\n",
      "  dtype      : object\n",
      "  unique vals: 114187\n",
      "  samples    : ['g1' 'g2' 'g3' 'g4' 'g5' 'g6' 'g7' 'g8' 'g9' 'g10']\n",
      "\n",
      "Column: dst_node\n",
      "  dtype      : object\n",
      "  unique vals: 113265\n",
      "  samples    : ['g2' 'g5' 'g4' 'PO_\\\\f[0]' 'g15' 'g16' 'g19' 'g9' 'g11' 'g12']\n",
      "\n",
      "Column: fan_out\n",
      "  dtype      : int64\n",
      "  min        : 0\n",
      "  max        : 1667\n",
      "  unique vals: 176\n",
      "\n",
      "Column: net_class\n",
      "  dtype      : object\n",
      "  unique vals: 1\n",
      "  samples    : ['data']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def inspect_csv(path):\n",
    "    \"\"\"\n",
    "    Print file shape, and for each column:\n",
    "      - numeric: min, max, number of unique values\n",
    "      - non-numeric: number of unique values and sample uniques\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"\\n=== Inspecting {path} ===\")\n",
    "    print(f\"Shape: {df.shape}\\n\")\n",
    "    for col in df.columns:\n",
    "        series = df[col]\n",
    "        print(f\"Column: {col}\")\n",
    "        if pd.api.types.is_numeric_dtype(series):\n",
    "            print(f\"  dtype      : {series.dtype}\")\n",
    "            print(f\"  min        : {series.min()}\")\n",
    "            print(f\"  max        : {series.max()}\")\n",
    "            print(f\"  unique vals: {series.nunique()}\\n\")\n",
    "        else:\n",
    "            uniques = series.dropna().unique()\n",
    "            print(f\"  dtype      : {series.dtype}\")\n",
    "            print(f\"  unique vals: {len(uniques)}\")\n",
    "            # show up to 10 sample unique values\n",
    "            print(f\"  samples    : {uniques[:10]}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for fname in [\"clean_nodes.csv\", \"clean_edges.csv\"]:\n",
    "        inspect_csv(fname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4205d6c0",
   "metadata": {},
   "source": [
    "#### Trainign & Evaluation GAT\n",
    "\n",
    "Currently the clean_nodes,csv and clean_edges.csv is not given to maintain the class-balance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1bec67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# --------------------\n",
    "# Reproducibility\n",
    "# --------------------\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# --------------------\n",
    "# Data loading and preprocessing\n",
    "# --------------------\n",
    "NUMERIC_NODE_FEATS = [\"in_degree\",\"out_degree\",\"total_degree\",\"path_depth\",\"CC0\",\"CC1\",\"CO\",\"is_input\",\"is_output\"]\n",
    "EDGE_FEATS = [\"fan_out\"]\n",
    "FAULT_TYPES = [\"stuck_at\", \"bridging\", \"glitch\", \"clean\"]\n",
    "\n",
    "def load_graph_tables(nodes_path=\"nodes.csv\", edges_path=\"edges.csv\"):\n",
    "    nodes = pd.read_csv(nodes_path)\n",
    "    edges = pd.read_csv(edges_path)\n",
    "    # Basic sanity\n",
    "    assert set([\"netlist_file\",\"fault_type\",\"node_id\",\"gate_type\",\"label\"]).issubset(nodes.columns)\n",
    "    assert set([\"netlist_file\",\"fault_type\",\"src_node\",\"dst_node\",\"fan_out\"]).issubset(edges.columns)\n",
    "    return nodes, edges\n",
    "\n",
    "def build_graph_index(nodes_df: pd.DataFrame) -> List[Tuple[str,str]]:\n",
    "    # Graph = (netlist_file, fault_type)\n",
    "    graphs = sorted(nodes_df[[\"netlist_file\",\"fault_type\"]].drop_duplicates().itertuples(index=False, name=None))\n",
    "    return graphs\n",
    "\n",
    "def fit_feature_encoders(nodes_df: pd.DataFrame, top_k_gates: int = 64):\n",
    "    # Gate type vocab (top-K + other). Keep PI and PO as explicit gate types.\n",
    "    gt_counts = Counter(nodes_df[\"gate_type\"].astype(str).tolist())\n",
    "    # Ensure PI and PO are kept\n",
    "    forced = [\"PI\",\"PO\"]\n",
    "    most_common = [g for g,_ in gt_counts.most_common() if g not in forced]\n",
    "    vocab = forced + most_common[:max(0, top_k_gates - len(forced))]\n",
    "    gate_to_idx = {g:i for i,g in enumerate(vocab)}\n",
    "    gate_other_idx = len(vocab)  # for unknowns\n",
    "    meta = {\n",
    "        \"gate_vocab\": vocab,\n",
    "        \"gate_other_idx\": gate_other_idx,\n",
    "        \"numeric_means\": {},\n",
    "        \"numeric_stds\": {}\n",
    "    }\n",
    "    # Fit numeric feature stats on all nodes but compute later only on TRAIN split\n",
    "    # We'll recompute after split. Here we just return placeholders.\n",
    "    return gate_to_idx, gate_other_idx, meta\n",
    "\n",
    "def compute_graphwise_positional(nodes_g: pd.DataFrame):\n",
    "    # Positional encodings p_v: [norm_depth, is_input, is_output]\n",
    "    # Normalize depth per graph\n",
    "    max_depth = max(nodes_g[\"path_depth\"].max(), 1)\n",
    "    norm_depth = nodes_g[\"path_depth\"] / max_depth\n",
    "    p = np.stack([\n",
    "        norm_depth.values.astype(np.float32),\n",
    "        nodes_g[\"is_input\"].values.astype(np.float32),\n",
    "        nodes_g[\"is_output\"].values.astype(np.float32),\n",
    "    ], axis=1)\n",
    "    return p\n",
    "\n",
    "def one_hot_gate_type(gate_type: str, gate_to_idx: Dict[str,int], gate_other_idx: int, dim: int):\n",
    "    idx = gate_to_idx.get(gate_type, gate_other_idx)\n",
    "    v = np.zeros(dim, dtype=np.float32)\n",
    "    v[idx] = 1.0\n",
    "    return v\n",
    "\n",
    "def standardize_features(train_arr: np.ndarray, arr: np.ndarray, eps: float = 1e-6):\n",
    "    mean = train_arr.mean(axis=0)\n",
    "    std = train_arr.std(axis=0)\n",
    "    std = np.where(std < eps, 1.0, std)\n",
    "    arr_std = (arr - mean) / std\n",
    "    return arr_std, mean, std\n",
    "\n",
    "def collate_graph(nodes_df: pd.DataFrame, edges_df: pd.DataFrame,\n",
    "                  gkey: Tuple[str,str],\n",
    "                  gate_to_idx, gate_other_idx, gate_feat_dim,\n",
    "                  numeric_scaler=None):\n",
    "    nf, ft = gkey\n",
    "    n_g = nodes_df[(nodes_df[\"netlist_file\"]==nf) & (nodes_df[\"fault_type\"]==ft)].copy()\n",
    "    e_g = edges_df[(edges_df[\"netlist_file\"]==nf) & (edges_df[\"fault_type\"]==ft)].copy()\n",
    "    # Node index map\n",
    "    node_ids = n_g[\"node_id\"].astype(str).tolist()\n",
    "    nidx = {n:i for i,n in enumerate(node_ids)}\n",
    "    N = len(node_ids)\n",
    "\n",
    "    # Node features\n",
    "    gate_oh = np.stack([one_hot_gate_type(g, gate_to_idx, gate_other_idx, gate_feat_dim) for g in n_g[\"gate_type\"].astype(str)], axis=0)\n",
    "    numeric = n_g[NUMERIC_NODE_FEATS].values.astype(np.float32)\n",
    "    # Positional encodings p_v\n",
    "    pos = compute_graphwise_positional(n_g)  # shape (N, 3)\n",
    "\n",
    "    # Standardize numeric features with provided scaler\n",
    "    if numeric_scaler is not None:\n",
    "        mean, std = numeric_scaler\n",
    "        std = np.where(std < 1e-6, 1.0, std)\n",
    "        numeric = (numeric - mean) / std\n",
    "\n",
    "    x = np.concatenate([numeric, gate_oh], axis=1).astype(np.float32)\n",
    "\n",
    "    # Edge index and edge features\n",
    "    if len(e_g) > 0:\n",
    "        src = e_g[\"src_node\"].astype(str).map(nidx).values\n",
    "        dst = e_g[\"dst_node\"].astype(str).map(nidx).values\n",
    "        # Filter edges with unknown nodes just in case\n",
    "        mask = (~pd.isna(src)) & (~pd.isna(dst))\n",
    "        src = src[mask].astype(np.int64)\n",
    "        dst = dst[mask].astype(np.int64)\n",
    "        E = len(src)\n",
    "        edge_index = np.stack([src, dst], axis=0)\n",
    "        # Edge features: fan_out -> normalized log1p\n",
    "        fan_out = e_g.loc[mask, \"fan_out\"].values.astype(np.float32)\n",
    "        edge_attr = np.log1p(fan_out)\n",
    "        # Normalize per-graph to [0,1]\n",
    "        if E > 0:\n",
    "            e_min = edge_attr.min()\n",
    "            e_max = edge_attr.max()\n",
    "            if e_max > e_min:\n",
    "                edge_attr = (edge_attr - e_min) / (e_max - e_min)\n",
    "            else:\n",
    "                edge_attr = np.zeros_like(edge_attr)\n",
    "        edge_attr = edge_attr[:,None]\n",
    "    else:\n",
    "        edge_index = np.zeros((2,0), dtype=np.int64)\n",
    "        edge_attr = np.zeros((0,1), dtype=np.float32)\n",
    "        E = 0\n",
    "\n",
    "    y = n_g[\"label\"].values.astype(np.int64)  # 0/1\n",
    "    return {\n",
    "        \"node_ids\": node_ids,\n",
    "        \"x\": x,\n",
    "        \"pos\": pos,\n",
    "        \"edge_index\": edge_index,\n",
    "        \"edge_attr\": edge_attr,\n",
    "        \"y\": y,\n",
    "        \"fault_type\": ft,\n",
    "        \"netlist_file\": nf\n",
    "    }\n",
    "\n",
    "# --------------------\n",
    "# Model: Edge-aware, position-encoded, residual GAT\n",
    "# --------------------\n",
    "class EdgeAwareGATLayer(nn.Module):\n",
    "    def __init__(self, in_dim, edge_dim, pos_dim, out_dim, heads=4, dropout=0.2, leaky_relu_neg=0.2):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.out_dim = out_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leaky_relu = nn.LeakyReLU(leaky_relu_neg)\n",
    "        # Per-head projections\n",
    "        self.W = nn.ModuleList([nn.Linear(in_dim, out_dim, bias=False) for _ in range(heads)])\n",
    "        self.U = nn.ModuleList([nn.Linear(edge_dim, out_dim, bias=False) for _ in range(heads)])\n",
    "        self.a = nn.ParameterList([nn.Parameter(torch.randn(2*out_dim + out_dim + 2*pos_dim)) for _ in range(heads)])\n",
    "        self.norm = nn.LayerNorm(heads*out_dim)\n",
    "\n",
    "    @staticmethod\n",
    "    def segment_softmax(dst, logits):\n",
    "        # Stable softmax over incoming edges per destination node\n",
    "        # Sort edges by dst\n",
    "        E = logits.size(0)\n",
    "        device = logits.device\n",
    "        perm = torch.argsort(dst)\n",
    "        dst_s = dst[perm]\n",
    "        z_s = logits[perm]\n",
    "\n",
    "        # boundaries where dst changes\n",
    "        b = torch.cat([torch.tensor([0], device=device),\n",
    "                       torch.nonzero(dst_s[1:] != dst_s[:-1], as_tuple=False).flatten()+1,\n",
    "                       torch.tensor([E], device=device)])\n",
    "        alphas = torch.empty_like(z_s)\n",
    "        for i in range(b.numel()-1):\n",
    "            s, e = b[i].item(), b[i+1].item()\n",
    "            seg = z_s[s:e]\n",
    "            m = torch.max(seg)\n",
    "            exp = torch.exp(seg - m)\n",
    "            denom = torch.sum(exp) + 1e-9\n",
    "            alphas[s:e] = exp / denom\n",
    "        # Unsort back\n",
    "        inv = torch.empty_like(perm)\n",
    "        inv[perm] = torch.arange(E, device=device)\n",
    "        return alphas[inv]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, pos):\n",
    "        # x: (N, in_dim), edge_index: (2, E), edge_attr:(E, e_dim), pos:(N, pos_dim)\n",
    "        N = x.size(0)\n",
    "        src = edge_index[0]\n",
    "        dst = edge_index[1]\n",
    "        head_outs = []\n",
    "        for h in range(self.heads):\n",
    "            x_proj = self.W[h](x)              # (N, out_dim)\n",
    "            e_proj = self.U[h](edge_attr)      # (E, out_dim)\n",
    "            xu = x_proj[src]                   # (E, out_dim)\n",
    "            xv = x_proj[dst]                   # (E, out_dim)\n",
    "            pv = pos[dst]                      # (E, pos_dim)\n",
    "            pu = pos[src]                      # (E, pos_dim)\n",
    "            cat = torch.cat([xv, xu, e_proj, pv, pu], dim=1)  # (E, 2*d + d_e + 2*p)\n",
    "            z = self.leaky_relu(torch.matmul(cat, self.a[h])) # (E,)\n",
    "            alpha = self.segment_softmax(dst, z)              # (E,)\n",
    "            alpha = self.dropout(alpha)\n",
    "\n",
    "            # Aggregate to nodes: sum_u alpha_{vu} * x_proj[u]\n",
    "            out_h = torch.zeros((N, self.out_dim), device=x.device, dtype=x.dtype)\n",
    "            out_h.index_add_(0, dst, (alpha.unsqueeze(1) * xu))\n",
    "            head_outs.append(out_h)\n",
    "        H = torch.cat(head_outs, dim=1)  # (N, heads*out_dim)\n",
    "        return self.norm(H)\n",
    "\n",
    "class EdgeAwareGAT(nn.Module):\n",
    "    def __init__(self, in_dim, edge_dim, pos_dim, hidden_dim=64, heads=4, num_layers=3, dropout=0.2, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.res_proj = nn.ModuleList()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.act = nn.ELU()\n",
    "        # First layer\n",
    "        self.layers.append(EdgeAwareGATLayer(in_dim, edge_dim, pos_dim, hidden_dim, heads=heads, dropout=dropout))\n",
    "        self.res_proj.append(nn.Linear(in_dim, hidden_dim*heads, bias=False))\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers-1):\n",
    "            self.layers.append(EdgeAwareGATLayer(hidden_dim*heads, edge_dim, pos_dim, hidden_dim, heads=heads, dropout=dropout))\n",
    "            self.res_proj.append(nn.Identity())  # same dim residual\n",
    "        self.final_norm = nn.LayerNorm(hidden_dim*heads)\n",
    "        self.out_lin = nn.Linear(hidden_dim*heads, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, pos):\n",
    "        h = x\n",
    "        for i, gat in enumerate(self.layers):\n",
    "            z = gat(h, edge_index, edge_attr, pos)\n",
    "            # Residual + dropout + activation\n",
    "            res = self.res_proj[i](h)\n",
    "            h = self.final_norm(res + self.drop(self.act(z)))\n",
    "        logits = self.out_lin(h)\n",
    "        return logits\n",
    "\n",
    "# --------------------\n",
    "# Training utilities\n",
    "# --------------------\n",
    "def compute_class_weights(y_list: List[np.ndarray]):\n",
    "    y = np.concatenate(y_list, axis=0)\n",
    "    counts = np.bincount(y, minlength=2).astype(np.float32)\n",
    "    total = counts.sum()\n",
    "    weights = total / (2.0 * np.maximum(counts, 1.0))\n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "def train_epoch(model, optimizer, criterion, graphs, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for g in graphs:\n",
    "        x = torch.tensor(g[\"x\"], dtype=torch.float32, device=device)\n",
    "        pos = torch.tensor(g[\"pos\"], dtype=torch.float32, device=device)\n",
    "        ei = torch.tensor(g[\"edge_index\"], dtype=torch.long, device=device)\n",
    "        ea = torch.tensor(g[\"edge_attr\"], dtype=torch.float32, device=device)\n",
    "        y = torch.tensor(g[\"y\"], dtype=torch.long, device=device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x, ei, ea, pos)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / max(len(graphs),1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_graph(model, g, device):\n",
    "    model.eval()\n",
    "    x = torch.tensor(g[\"x\"], dtype=torch.float32, device=device)\n",
    "    pos = torch.tensor(g[\"pos\"], dtype=torch.float32, device=device)\n",
    "    ei = torch.tensor(g[\"edge_index\"], dtype=torch.long, device=device)\n",
    "    ea = torch.tensor(g[\"edge_attr\"], dtype=torch.float32, device=device)\n",
    "    logits = model(x, ei, ea, pos)\n",
    "    probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "    preds = probs.argmax(axis=1)\n",
    "    return preds, probs\n",
    "\n",
    "def plot_confusion(y_true, y_pred, title, savepath=None):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    plt.figure(figsize=(4.2,3.6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=[\"neg\",\"pos\"], yticklabels=[\"neg\",\"pos\"])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    if savepath:\n",
    "        plt.savefig(savepath, dpi=150)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# --------------------\n",
    "# Main script\n",
    "# --------------------\n",
    "def main(args):\n",
    "    set_seed(args.seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.cpu else \"cpu\")\n",
    "\n",
    "    nodes, edges = load_graph_tables(args.nodes, args.edges)\n",
    "\n",
    "    # Build graph keys and encoders\n",
    "    graph_keys = build_graph_index(nodes)\n",
    "    gate_to_idx, gate_other_idx, meta = fit_feature_encoders(nodes, top_k_gates=args.top_k_gates)\n",
    "    gate_feat_dim = len(meta[\"gate_vocab\"]) + 1  # +other\n",
    "\n",
    "    # Split by graphs to avoid leakage\n",
    "    # Keep proportion across fault types\n",
    "    keys_by_type = defaultdict(list)\n",
    "    for (nf, ft) in graph_keys:\n",
    "        keys_by_type[ft].append((nf, ft))\n",
    "\n",
    "    train_keys, val_keys, test_keys = [], [], []\n",
    "    rng = np.random.RandomState(args.seed)\n",
    "    for ft, keys in keys_by_type.items():\n",
    "        keys = keys.copy()\n",
    "        rng.shuffle(keys)\n",
    "        n = len(keys)\n",
    "        n_train = int(n * args.train_ratio)\n",
    "        n_val = int(n * args.val_ratio)\n",
    "        train_keys += keys[:n_train]\n",
    "        val_keys += keys[n_train:n_train+n_val]\n",
    "        test_keys += keys[n_train+n_val:]\n",
    "\n",
    "    # Fit numeric scalers on TRAIN graphs only\n",
    "    train_numeric = []\n",
    "    for nf, ft in train_keys:\n",
    "        n_g = nodes[(nodes[\"netlist_file\"]==nf) & (nodes[\"fault_type\"]==ft)]\n",
    "        train_numeric.append(n_g[NUMERIC_NODE_FEATS].values.astype(np.float32))\n",
    "    train_numeric = np.concatenate(train_numeric, axis=0) if len(train_numeric)>0 else np.zeros((1,len(NUMERIC_NODE_FEATS)), dtype=np.float32)\n",
    "    num_mean = train_numeric.mean(axis=0)\n",
    "    num_std = train_numeric.std(axis=0)\n",
    "    num_std[num_std < 1e-6] = 1.0\n",
    "\n",
    "    # Build datasets\n",
    "    def build_set(keys):\n",
    "        gs = []\n",
    "        for gk in keys:\n",
    "            gs.append(collate_graph(nodes, edges, gk,\n",
    "                                    gate_to_idx, gate_other_idx, gate_feat_dim,\n",
    "                                    numeric_scaler=(num_mean, num_std)))\n",
    "        return gs\n",
    "\n",
    "    train_graphs = build_set(train_keys)\n",
    "    val_graphs   = build_set(val_keys)\n",
    "    test_graphs  = build_set(test_keys)\n",
    "\n",
    "    # Model dims\n",
    "    in_dim = len(NUMERIC_NODE_FEATS) + gate_feat_dim\n",
    "    edge_dim = 1\n",
    "    pos_dim = 3\n",
    "\n",
    "    model = EdgeAwareGAT(in_dim=in_dim,\n",
    "                         edge_dim=edge_dim,\n",
    "                         pos_dim=pos_dim,\n",
    "                         hidden_dim=args.hidden_dim,\n",
    "                         heads=args.heads,\n",
    "                         num_layers=args.layers,\n",
    "                         dropout=args.dropout,\n",
    "                         num_classes=2).to(device)\n",
    "\n",
    "    # Class weights for imbalance (computed from TRAIN nodes)\n",
    "    train_labels = [g[\"y\"] for g in train_graphs]\n",
    "    class_weights = compute_class_weights(train_labels).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "    # Training loop\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = None\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        tr_loss = train_epoch(model, optimizer, criterion, train_graphs, device)\n",
    "        # Simple validation loss\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            for g in val_graphs:\n",
    "                x = torch.tensor(g[\"x\"], dtype=torch.float32, device=device)\n",
    "                pos = torch.tensor(g[\"pos\"], dtype=torch.float32, device=device)\n",
    "                ei = torch.tensor(g[\"edge_index\"], dtype=torch.long, device=device)\n",
    "                ea = torch.tensor(g[\"edge_attr\"], dtype=torch.float32, device=device)\n",
    "                y = torch.tensor(g[\"y\"], dtype=torch.long, device=device)\n",
    "                logits = model(x, ei, ea, pos)\n",
    "                loss = criterion(logits, y)\n",
    "                val_loss += loss.item()\n",
    "            val_loss = val_loss / max(len(val_graphs),1)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_state = {k:v.cpu().clone() for k,v in model.state_dict().items()}\n",
    "        if epoch % max(1, args.log_every) == 0:\n",
    "            print(f\"Epoch {epoch:03d} | train_loss={tr_loss:.4f} | val_loss={val_loss:.4f}\")\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # Evaluation on test graphs\n",
    "    all_true, all_pred = [], []\n",
    "    all_ft_true, all_ft_pred = defaultdict(list), defaultdict(list)\n",
    "\n",
    "    for g in test_graphs:\n",
    "        preds, _ = predict_graph(model, g, device)\n",
    "        y = g[\"y\"]\n",
    "        all_true.append(y)\n",
    "        all_pred.append(preds)\n",
    "        ft = g[\"fault_type\"]\n",
    "        # Only collect per-fault stats for target fault types (exclude clean)\n",
    "        if ft in [\"stuck_at\",\"bridging\",\"glitch\"]:\n",
    "            all_ft_true[ft].append(y)\n",
    "            all_ft_pred[ft].append(preds)\n",
    "\n",
    "    y_true = np.concatenate(all_true, axis=0) if len(all_true)>0 else np.array([], dtype=np.int64)\n",
    "    y_pred = np.concatenate(all_pred, axis=0) if len(all_pred)>0 else np.array([], dtype=np.int64)\n",
    "\n",
    "    # Overall metrics\n",
    "    print(\"\\n=== Overall evaluation (all test graphs) ===\")\n",
    "    if y_true.size > 0:\n",
    "        print(classification_report(y_true, y_pred, digits=4))\n",
    "        print(f\"Overall accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "        plot_confusion(y_true, y_pred, \"Confusion Matrix - Overall\", savepath=\"cm_overall.png\")\n",
    "    else:\n",
    "        print(\"No test data available.\")\n",
    "\n",
    "    # Per-fault-type metrics\n",
    "    for ft in [\"stuck_at\",\"bridging\",\"glitch\"]:\n",
    "        print(f\"\\n=== Evaluation for fault_type={ft} ===\")\n",
    "        if ft in all_ft_true and len(all_ft_true[ft])>0:\n",
    "            y_t = np.concatenate(all_ft_true[ft], axis=0)\n",
    "            y_p = np.concatenate(all_ft_pred[ft], axis=0)\n",
    "            print(classification_report(y_t, y_p, digits=4))\n",
    "            print(f\"Accuracy ({ft}): {accuracy_score(y_t, y_p):.4f}\")\n",
    "            plot_confusion(y_t, y_p, f\"Confusion Matrix - {ft}\", savepath=f\"cm_{ft}.png\")\n",
    "        else:\n",
    "            print(f\"No test graphs for {ft}.\")\n",
    "\n",
    "    # Save training metadata\n",
    "    meta = {\n",
    "        \"in_dim\": in_dim,\n",
    "        \"edge_dim\": edge_dim,\n",
    "        \"pos_dim\": pos_dim,\n",
    "        \"gate_vocab\": list(gate_to_idx.keys()),\n",
    "        \"train_graphs\": len(train_graphs),\n",
    "        \"val_graphs\": len(val_graphs),\n",
    "        \"test_graphs\": len(test_graphs),\n",
    "        \"class_weights\": class_weights.cpu().numpy().tolist()\n",
    "    }\n",
    "    with open(\"training_meta.json\",\"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "    torch.save(model.state_dict(), \"edgeaware_gat.pt\")\n",
    "    print(\"\\nSaved model to edgeaware_gat.pt and metadata to training_meta.json\")\n",
    "    print(\"Confusion matrices saved as cm_overall.png, cm_stuck_at.png, cm_bridging.png, cm_glitch.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # =========================\n",
    "    # JUPYTER NOTEBOOK FRIENDLY VERSION\n",
    "    # =========================\n",
    "\n",
    "    # 1) Parameter setup (edit these directly)\n",
    "    nodes_path      = \"nodes.csv\"\n",
    "    edges_path      = \"edges.csv\"\n",
    "    epochs          = 50\n",
    "    lr              = 2e-3\n",
    "    weight_decay    = 1e-5\n",
    "    hidden_dim      = 64\n",
    "    heads           = 4\n",
    "    layers          = 3\n",
    "    dropout         = 0.2\n",
    "    train_ratio     = 0.7\n",
    "    val_ratio       = 0.1\n",
    "    top_k_gates     = 64\n",
    "    seed            = 42\n",
    "    use_cpu         = False\n",
    "    log_every       = 5\n",
    "\n",
    "    # 2) Paste here the ENTIRE script body from the version I gave you earlier,\n",
    "    #    but remove ONLY the argparse parts at the end, and replace the\n",
    "    #    `main(args)` call with this:\n",
    "\n",
    "    class Args: pass\n",
    "    args = Args()\n",
    "    args.nodes = nodes_path\n",
    "    args.edges = edges_path\n",
    "    args.epochs = epochs\n",
    "    args.lr = lr\n",
    "    args.weight_decay = weight_decay\n",
    "    args.hidden_dim = hidden_dim\n",
    "    args.heads = heads\n",
    "    args.layers = layers\n",
    "    args.dropout = dropout\n",
    "    args.train_ratio = train_ratio\n",
    "    args.val_ratio = val_ratio\n",
    "    args.top_k_gates = top_k_gates\n",
    "    args.seed = seed\n",
    "    args.cpu = use_cpu\n",
    "    args.log_every = log_every\n",
    "    main(args)  # call the function from the pasted code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe79481b",
   "metadata": {},
   "source": [
    "#### Trying Simple Architecture: GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "832aef87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global positives: 9281, negatives: 1737631, pos_weight=187.225\n",
      "[Epoch 1] loss=6.6194\n",
      "[Epoch 2] loss=4.1068\n",
      "[Epoch 3] loss=3.6614\n",
      "[Epoch 4] loss=3.3925\n",
      "[Epoch 5] loss=3.3549\n",
      "[Epoch 6] loss=3.2031\n",
      "[Epoch 7] loss=3.1964\n",
      "[Epoch 8] loss=3.1170\n",
      "[Epoch 9] loss=3.1024\n",
      "[Epoch 10] loss=3.1056\n",
      "\n",
      "=== Overall Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9964    0.0551    0.1044   1737631\n",
      "           1     0.0054    0.9625    0.0108      9281\n",
      "\n",
      "    accuracy                         0.0599   1746912\n",
      "   macro avg     0.5009    0.5088    0.0576   1746912\n",
      "weighted avg     0.9911    0.0599    0.1039   1746912\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  95673 1641958]\n",
      " [    348    8933]]\n",
      "Overall accuracy: 5.99%\n",
      "\n",
      "--- Fault Type: clean ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9964    0.0551    0.1044   1737631\n",
      "           1     0.0054    0.9625    0.0108      9281\n",
      "\n",
      "    accuracy                         0.0599   1746912\n",
      "   macro avg     0.5009    0.5088    0.0576   1746912\n",
      "weighted avg     0.9911    0.0599    0.1039   1746912\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  95673 1641958]\n",
      " [    348    8933]]\n",
      "Accuracy (fault type=clean): 5.99%\n"
     ]
    }
   ],
   "source": [
    "# graphsage_baseline.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import SAGEConv\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from dgl import add_self_loop\n",
    "\n",
    "# -------------------------------\n",
    "# Load and preprocess data\n",
    "# -------------------------------\n",
    "nodes_df = pd.read_csv(\"nodes.csv\")\n",
    "edges_df = pd.read_csv(\"edges.csv\")\n",
    "\n",
    "# numeric feature columns used in your pipeline\n",
    "num_feats = [\"total_degree\", \"path_depth\", \"CC0\", \"CC1\", \"CO\"]\n",
    "\n",
    "# normalize numeric columns (same as your GAT code)\n",
    "for c in num_feats:\n",
    "    mean, std = nodes_df[c].mean(), nodes_df[c].std()\n",
    "    nodes_df[c] = (nodes_df[c] - mean) / (std + 1e-6)\n",
    "\n",
    "# gate type one-hot mapping\n",
    "gate_types = sorted(nodes_df.gate_type.unique().tolist())\n",
    "gate2idx = {g: i for i, g in enumerate(gate_types)}\n",
    "NUM_GATES = len(gate_types)\n",
    "\n",
    "def build_dgl_graph(nodes, edges, netfile):\n",
    "    nd = nodes[nodes.netlist_file == netfile].copy()\n",
    "    ed = edges[edges.netlist_file == netfile].copy()\n",
    "    if nd.empty:\n",
    "        return None\n",
    "    node_ids = nd.node_id.values\n",
    "    idx_map = {nid: i for i, nid in enumerate(node_ids)}\n",
    "    src = [idx_map[u] for u in ed.src_node if u in idx_map]\n",
    "    dst = [idx_map[v] for v in ed.dst_node if v in idx_map]\n",
    "    g = dgl.graph((src, dst), num_nodes=len(node_ids))\n",
    "    # add self-loop to avoid 0-in-degree issues\n",
    "    g = add_self_loop(g)\n",
    "    onehot = np.zeros((len(nd), NUM_GATES), dtype=np.float32)\n",
    "    for i, gt in enumerate(nd.gate_type):\n",
    "        onehot[i, gate2idx[gt]] = 1.0\n",
    "    nums = nd[num_feats].values.astype(np.float32)\n",
    "    x = np.concatenate([onehot, nums], axis=1)\n",
    "    g.ndata[\"x\"] = torch.from_numpy(x)\n",
    "    g.ndata[\"y\"] = torch.from_numpy(nd.label.values.astype(np.float32))\n",
    "    # store the fault_type at construction time (returned alongside graph)\n",
    "    ft = nd.fault_type.iloc[0] if \"fault_type\" in nd.columns else \"unknown\"\n",
    "    return g, ft\n",
    "\n",
    "# build list of (graph, fault_type)\n",
    "netlist_files = nodes_df.netlist_file.unique().tolist()\n",
    "graphs_with_type = []\n",
    "for nf in netlist_files:\n",
    "    pair = build_dgl_graph(nodes_df, edges_df, nf)\n",
    "    if pair is not None:\n",
    "        graphs_with_type.append(pair)  # (graph, fault_type)\n",
    "\n",
    "if len(graphs_with_type) == 0:\n",
    "    raise SystemExit(\"No graphs found. Check nodes.csv / edges.csv\")\n",
    "\n",
    "# -------------------------------\n",
    "# compute global pos_weight for BCEWithLogitsLoss\n",
    "# -------------------------------\n",
    "total_pos = int(nodes_df.label.sum())\n",
    "total_neg = int(len(nodes_df) - total_pos)\n",
    "if total_pos == 0:\n",
    "    pos_weight_val = 1.0\n",
    "else:\n",
    "    pos_weight_val = float(total_neg) / float(max(total_pos, 1))\n",
    "# clip extreme ratios to avoid numeric instability\n",
    "pos_weight_val = float(min(pos_weight_val, 500.0))\n",
    "\n",
    "print(f\"Global positives: {total_pos}, negatives: {total_neg}, pos_weight={pos_weight_val:.3f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# GraphSAGE model\n",
    "# -------------------------------\n",
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats=64, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_dim = in_feats if i == 0 else hidden_feats\n",
    "            self.layers.append(SAGEConv(in_dim, hidden_feats, aggregator_type='mean'))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.pred = nn.Linear(hidden_feats, 1)\n",
    "\n",
    "    def forward(self, g):\n",
    "        h = g.ndata[\"x\"]\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "            h = F.relu(h)\n",
    "            h = self.dropout(h)\n",
    "        return self.pred(h).squeeze(-1)\n",
    "\n",
    "# -------------------------------\n",
    "# training loop\n",
    "# -------------------------------\n",
    "def train(graphs_with_type, device=\"cpu\", epochs=10, lr=1e-3):\n",
    "    in_dim = graphs_with_type[0][0].ndata[\"x\"].shape[1]\n",
    "    model = GraphSAGEModel(in_dim).to(device)\n",
    "    pos_weight = torch.tensor(pos_weight_val, device=device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for g, _ in graphs_with_type:\n",
    "            g = g.to(device)\n",
    "            logits = model(g)\n",
    "            y = g.ndata[\"y\"].to(device)\n",
    "            loss = criterion(logits, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss.item())\n",
    "        avg_loss = total_loss / len(graphs_with_type)\n",
    "        print(f\"[Epoch {ep}] loss={avg_loss:.4f}\")\n",
    "    return model\n",
    "\n",
    "# -------------------------------\n",
    "# evaluation (overall + per-fault-type)\n",
    "# -------------------------------\n",
    "@torch.no_grad()\n",
    "def evaluate(model, graphs_with_type, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    y_true_all = []\n",
    "    y_pred_all = []\n",
    "    ftypes_all = []\n",
    "\n",
    "    for g, ftype in graphs_with_type:\n",
    "        g = g.to(device)\n",
    "        logits = model(g)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        preds = (probs > 0.5).astype(int).tolist()\n",
    "        labels = g.ndata[\"y\"].cpu().numpy().astype(int).tolist()\n",
    "        y_true_all.extend(labels)\n",
    "        y_pred_all.extend(preds)\n",
    "        ftypes_all.extend([ftype] * len(labels))\n",
    "\n",
    "    # overall\n",
    "    acc = 100.0 * accuracy_score(y_true_all, y_pred_all)\n",
    "    print(\"\\n=== Overall Classification Report ===\")\n",
    "    print(classification_report(y_true_all, y_pred_all, digits=4, zero_division=0))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true_all, y_pred_all))\n",
    "    print(f\"Overall accuracy: {acc:.2f}%\")\n",
    "\n",
    "    # per fault type\n",
    "    uniq_types = sorted(set(ftypes_all))\n",
    "    for ft in uniq_types:\n",
    "        idx = [i for i, f in enumerate(ftypes_all) if f == ft]\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        yt = [y_true_all[i] for i in idx]\n",
    "        yp = [y_pred_all[i] for i in idx]\n",
    "        acc_ft = 100.0 * accuracy_score(yt, yp)\n",
    "        print(f\"\\n--- Fault Type: {ft} ---\")\n",
    "        print(classification_report(yt, yp, digits=4, zero_division=0))\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(yt, yp))\n",
    "        print(f\"Accuracy (fault type={ft}): {acc_ft:.2f}%\")\n",
    "\n",
    "# -------------------------------\n",
    "# run\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = train(graphs_with_type, device=device, epochs=10, lr=1e-3)\n",
    "    evaluate(model, graphs_with_type, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eed9dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global positives: 9281, negatives: 1737631, pos_weight=187.225\n",
      "[Epoch 1] loss=6.7355\n",
      "[Epoch 2] loss=5.0567\n",
      "[Epoch 3] loss=4.1255\n",
      "[Epoch 4] loss=3.8169\n",
      "[Epoch 5] loss=3.6201\n",
      "[Epoch 6] loss=3.5248\n",
      "[Epoch 7] loss=3.4590\n",
      "[Epoch 8] loss=3.4095\n",
      "[Epoch 9] loss=3.3706\n",
      "[Epoch 10] loss=3.3440\n",
      "\n",
      "=== Overall Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9967    0.0661    0.1240   1737631\n",
      "           1     0.0055    0.9587    0.0108      9281\n",
      "\n",
      "    accuracy                         0.0708   1746912\n",
      "   macro avg     0.5011    0.5124    0.0674   1746912\n",
      "weighted avg     0.9914    0.0708    0.1234   1746912\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 114856 1622775]\n",
      " [    383    8898]]\n",
      "Overall accuracy: 7.08%\n",
      "\n",
      "--- Fault Type: clean ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9967    0.0661    0.1240   1737631\n",
      "           1     0.0055    0.9587    0.0108      9281\n",
      "\n",
      "    accuracy                         0.0708   1746912\n",
      "   macro avg     0.5011    0.5124    0.0674   1746912\n",
      "weighted avg     0.9914    0.0708    0.1234   1746912\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 114856 1622775]\n",
      " [    383    8898]]\n",
      "Accuracy (fault type=clean): 7.08%\n"
     ]
    }
   ],
   "source": [
    "# gcn_baseline.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GraphConv\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from dgl import add_self_loop\n",
    "\n",
    "# -------------------------------\n",
    "# Load and preprocess data\n",
    "# -------------------------------\n",
    "nodes_df = pd.read_csv(\"nodes.csv\")\n",
    "edges_df = pd.read_csv(\"edges.csv\")\n",
    "\n",
    "num_feats = [\"total_degree\", \"path_depth\", \"CC0\", \"CC1\", \"CO\"]\n",
    "for c in num_feats:\n",
    "    mean, std = nodes_df[c].mean(), nodes_df[c].std()\n",
    "    nodes_df[c] = (nodes_df[c] - mean) / (std + 1e-6)\n",
    "\n",
    "gate_types = sorted(nodes_df.gate_type.unique().tolist())\n",
    "gate2idx = {g: i for i, g in enumerate(gate_types)}\n",
    "NUM_GATES = len(gate_types)\n",
    "\n",
    "def build_dgl_graph(nodes, edges, netfile):\n",
    "    nd = nodes[nodes.netlist_file == netfile].copy()\n",
    "    ed = edges[edges.netlist_file == netfile].copy()\n",
    "    if nd.empty:\n",
    "        return None\n",
    "    node_ids = nd.node_id.values\n",
    "    idx_map = {nid: i for i, nid in enumerate(node_ids)}\n",
    "    src = [idx_map[u] for u in ed.src_node if u in idx_map]\n",
    "    dst = [idx_map[v] for v in ed.dst_node if v in idx_map]\n",
    "    g = dgl.graph((src, dst), num_nodes=len(node_ids))\n",
    "    g = add_self_loop(g)\n",
    "    onehot = np.zeros((len(nd), NUM_GATES), dtype=np.float32)\n",
    "    for i, gt in enumerate(nd.gate_type):\n",
    "        onehot[i, gate2idx[gt]] = 1.0\n",
    "    nums = nd[num_feats].values.astype(np.float32)\n",
    "    x = np.concatenate([onehot, nums], axis=1)\n",
    "    g.ndata[\"x\"] = torch.from_numpy(x)\n",
    "    g.ndata[\"y\"] = torch.from_numpy(nd.label.values.astype(np.float32))\n",
    "    ft = nd.fault_type.iloc[0] if \"fault_type\" in nd.columns else \"unknown\"\n",
    "    return g, ft\n",
    "\n",
    "netlist_files = nodes_df.netlist_file.unique().tolist()\n",
    "graphs_with_type = []\n",
    "for nf in netlist_files:\n",
    "    pair = build_dgl_graph(nodes_df, edges_df, nf)\n",
    "    if pair is not None:\n",
    "        graphs_with_type.append(pair)\n",
    "\n",
    "if len(graphs_with_type) == 0:\n",
    "    raise SystemExit(\"No graphs found. Check nodes.csv / edges.csv\")\n",
    "\n",
    "# pos_weight\n",
    "total_pos = int(nodes_df.label.sum())\n",
    "total_neg = int(len(nodes_df) - total_pos)\n",
    "pos_weight_val = float(total_neg) / float(max(total_pos, 1)) if total_pos > 0 else 1.0\n",
    "pos_weight_val = float(min(pos_weight_val, 500.0))\n",
    "print(f\"Global positives: {total_pos}, negatives: {total_neg}, pos_weight={pos_weight_val:.3f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# GCN model\n",
    "# -------------------------------\n",
    "class SimpleGCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats=64, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_dim = in_feats if i == 0 else hidden_feats\n",
    "            self.layers.append(GraphConv(in_dim, hidden_feats))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.pred = nn.Linear(hidden_feats, 1)\n",
    "\n",
    "    def forward(self, g):\n",
    "        h = g.ndata[\"x\"]\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "            h = F.relu(h)\n",
    "            h = self.dropout(h)\n",
    "        return self.pred(h).squeeze(-1)\n",
    "\n",
    "# -------------------------------\n",
    "# training & evaluation (same pattern)\n",
    "# -------------------------------\n",
    "def train(graphs_with_type, device=\"cpu\", epochs=10, lr=1e-3):\n",
    "    in_dim = graphs_with_type[0][0].ndata[\"x\"].shape[1]\n",
    "    model = SimpleGCN(in_dim).to(device)\n",
    "    pos_weight = torch.tensor(pos_weight_val, device=device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for g, _ in graphs_with_type:\n",
    "            g = g.to(device)\n",
    "            logits = model(g)\n",
    "            y = g.ndata[\"y\"].to(device)\n",
    "            loss = criterion(logits, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss.item())\n",
    "        avg_loss = total_loss / len(graphs_with_type)\n",
    "        print(f\"[Epoch {ep}] loss={avg_loss:.4f}\")\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, graphs_with_type, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    y_true_all = []\n",
    "    y_pred_all = []\n",
    "    ftypes_all = []\n",
    "    for g, ftype in graphs_with_type:\n",
    "        g = g.to(device)\n",
    "        logits = model(g)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        preds = (probs > 0.5).astype(int).tolist()\n",
    "        labels = g.ndata[\"y\"].cpu().numpy().astype(int).tolist()\n",
    "        y_true_all.extend(labels)\n",
    "        y_pred_all.extend(preds)\n",
    "        ftypes_all.extend([ftype] * len(labels))\n",
    "\n",
    "    acc = 100.0 * accuracy_score(y_true_all, y_pred_all)\n",
    "    print(\"\\n=== Overall Classification Report ===\")\n",
    "    print(classification_report(y_true_all, y_pred_all, digits=4, zero_division=0))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true_all, y_pred_all))\n",
    "    print(f\"Overall accuracy: {acc:.2f}%\")\n",
    "\n",
    "    for ft in sorted(set(ftypes_all)):\n",
    "        idx = [i for i, f in enumerate(ftypes_all) if f == ft]\n",
    "        yt = [y_true_all[i] for i in idx]\n",
    "        yp = [y_pred_all[i] for i in idx]\n",
    "        acc_ft = 100.0 * accuracy_score(yt, yp)\n",
    "        print(f\"\\n--- Fault Type: {ft} ---\")\n",
    "        print(classification_report(yt, yp, digits=4, zero_division=0))\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(yt, yp))\n",
    "        print(f\"Accuracy (fault type={ft}): {acc_ft:.2f}%\")\n",
    "\n",
    "# -------------------------------\n",
    "# run\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = train(graphs_with_type, device=device, epochs=10, lr=1e-3)\n",
    "    evaluate(model, graphs_with_type, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dbc2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
